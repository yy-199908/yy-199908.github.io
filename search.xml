<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>FuzzyLogicReport2</title>
      <link href="/2022/04/17/FuzzyLogicReport2/"/>
      <url>/2022/04/17/FuzzyLogicReport2/</url>
      
        <content type="html"><![CDATA[<div class="row">    <embed src="https://yangyin.cool/pdf/FuzzyLogic/test.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>模型的保存与加载</title>
      <link href="/2022/04/17/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/"/>
      <url>/2022/04/17/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/</url>
      
        <content type="html"><![CDATA[<h1>模型的保存与加载</h1><h2 id="保存与加载整个模型的结构信息和参数信息">保存与加载整个模型的结构信息和参数信息</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.save(model,&#x27;./model.pth&#x27;)</span><br><span class="line">load_model=torch.load(&#x27;model.pth&#x27;)</span><br></pre></td></tr></table></figure><h2 id="保存与加载整个模型的参数信息">保存与加载整个模型的参数信息</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.save(model.state_dict(),&#x27;./model_state.pth&#x27;)</span><br><span class="line">load_model=model.load_state_dic(torch.load(&#x27;model_state.pth&#x27;))</span><br></pre></td></tr></table></figure><p><mark>load_model加载前需要先实例化</mark></p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPU加速与可视化</title>
      <link href="/2022/04/17/GPU%E5%8A%A0%E9%80%9F%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
      <url>/2022/04/17/GPU%E5%8A%A0%E9%80%9F%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1>GPU与可视化</h1><h2 id="Gpu加速">Gpu加速</h2><p>GPU加速</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device=torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">net=MLP.to(device)</span><br><span class="line">对应数据也需要加入.to(device)或者使用data.cuda（）</span><br></pre></td></tr></table></figure><p><code>.item()</code>：取tensor中的值</p><h2 id="Visdom可视化">Visdom可视化</h2><h3 id="tensorboardX">tensorboardX</h3><p><code>pip install tensorboardX</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer=SummaryWriter()</span><br><span class="line">writer.add_scalar(<span class="string">&#x27;data/scalar_group&#x27;</span>,&#123;<span class="string">&#x27;xsinx&#x27;</span>:n_iter*np.sin(n_iter),<span class="string">&#x27;xcos&#x27;</span>:n_iter*np.cos(n_iter),n_iter&#125;)</span><br><span class="line">writer.add_image(<span class="string">&#x27;Image&#x27;</span>,x,n_iter)</span><br><span class="line">writer.add_text(<span class="string">&#x27;Text&#x27;</span>,<span class="string">&#x27;text logged at step:&#x27;</span>+<span class="built_in">str</span>(n_iter),n_iter)</span><br><span class="line"><span class="keyword">for</span> name,param <span class="keyword">in</span> resnet18.named_parameters():</span><br><span class="line">    writer.add_histogram(name,param.clone().cpu().adta.numpy(),n_iter)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h3 id="Visdom">Visdom</h3><p>运行效率更高</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install visdom</span><br><span class="line">python -m visdom.server</span><br><span class="line">如果运行报错，则重新下载visdom后通过进入文件目录后</span><br><span class="line">pip install -e 安装</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#一条线</span></span><br><span class="line"><span class="keyword">from</span> visdom <span class="keyword">import</span> Visdom</span><br><span class="line">viz=Visdom()</span><br><span class="line">viz.line([<span class="number">0.</span>],[<span class="number">0.</span>],win=<span class="string">&#x27;train_loss&#x27;</span>,opts=<span class="built_in">dict</span>(title=<span class="string">&#x27;train loss&#x27;</span>))<span class="comment">#创建一条直线（y,x,ID,opt:属性配置）</span></span><br><span class="line">viz.line([loss.item()],[global_step],win=<span class="string">&#x27;train_loss&#x27;</span>,update=<span class="string">&#x27;append&#x27;</span>)<span class="comment">#将数据添加到直线中（update操作）</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#多条线</span></span><br><span class="line"><span class="keyword">from</span> visdom <span class="keyword">import</span> Visdom</span><br><span class="line">viz=Visdom()</span><br><span class="line">viz.line([<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">0.0</span>,<span class="number">0.0</span>],win=<span class="string">&#x27;test&#x27;</span>,opts=<span class="built_in">dict</span>(title=<span class="string">&#x27;test loss&amp;acc&#x27;</span>，legend=[<span class="string">&#x27;loss&#x27;</span>,<span class="string">&#x27;acc&#x27;</span>]))<span class="comment">#创建两条直线（[y1,y2],x,ID,opt:属性配置）</span></span><br><span class="line">viz.line([[test_loss,cprrect/<span class="built_in">len</span>(test_loader.dataset)]],[global_step],win=<span class="string">&#x27;test&#x27;</span>,update=<span class="string">&#x27;append&#x27;</span>)<span class="comment">#将数据添加到直线中（update操作）</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#可视化image与pred</span><br><span class="line">from visdom import Visdom</span><br><span class="line">viz=Visdom()</span><br><span class="line">viz.images(data.reshape(-1,1,28,28),win=&#x27;x&#x27;)</span><br><span class="line">viz.text(str(pred.detach().cpu().numpy),win=&#x27;pred&#x27;,opts=dict(title=&#x27;pred&#x27;))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>过拟合与欠拟合</title>
      <link href="/2022/04/17/%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88/"/>
      <url>/2022/04/17/%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88/</url>
      
        <content type="html"><![CDATA[<h2 id="过拟合与欠拟合">过拟合与欠拟合</h2><p>欠拟合：训练集和测试集的acc都很差 可以适当增加模型复杂度再测试</p><p>过拟合：使用模型复杂度高于实际数据模型复杂度 训练多次后training acc很好，val acc效果不好 泛化能力变差</p><h3 id="交叉验证">交叉验证</h3><p>每训练多少次做一次test，只保存test acc 最好的模型</p><p>overfitting之后的模型都不会保存</p><h4 id="K-fold-cross-validation">K-fold cross-validation</h4><p>每次训练完以后重新划分Train Set 与Val Set</p><p>把最开始的Trainning set划分为K份，每次取K-1份做为Trainning Set 另外一份作为Val Set</p><h2 id="减少过拟合">减少过拟合</h2><p>增大数据集</p><p>减少模型复杂度</p><p>Dropout</p><p>Data argume</p><h3 id="Regularization">Regularization</h3><p>$$<br>J(\theta)=-\frac{1}{m}\sum_{i=1}^m[y_i\ln\hat y_i+(1-y_i)\ln(1-\bar y^i)]+\lambda \vert\theta_i\vert<br>$$</p><p>式子前半部分为交叉熵，后半部分为$\lambda$乘上网络参数的1或2范数，因为矩阵的1，2范数大于等于0，当我们对$J(\theta)$做最优化使其最小时，也相当于最优化$\vert\theta_i\vert$减小接近为0，达到简化网络的目的：高维参数接近为0，低维参数保持。</p><p>常用L2-Regularization：<br>$$<br>J(W;X,y)+\frac{1}{2}\Vert w\Vert^2<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer=optim.SGD(net.parameters(),lr=learn_rate,weight_decay=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>使用时只需在.optim时给定参数weight_decay=$\lambda$</p><p>L1-Regularization：pytorch 没有api支持，需要人为编写</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">regularization_loss=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">regularization_loss+=torch.<span class="built_in">sum</span>(torch.<span class="built_in">abs</span>(param))</span><br><span class="line"></span><br><span class="line">classify_loss=criteon(logits,target)</span><br><span class="line">loss=classify_loss+<span class="number">0.01</span>*regularzation_loss</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><h3 id="动量与学习率衰减">动量与学习率衰减</h3><h4 id="Momentum：用于减少停止在局部最优点的情况">Momentum：用于减少停止在局部最优点的情况</h4><p>原梯度更新公式：<br>$$<br>w^{k+1}=w^k-\alpha\nabla f(w^k)<br>$$<br>Momentum梯度更新公式：<br>$$<br>z^{k+1}=\beta z^k+\nabla f(w^k)<br>$$</p><p>$$<br>w^{k+1}=w^k-\alpha z^{k+1}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer=optim.SGD(net.parameters(),lr=learn_rate,momentum=<span class="number">0.78</span>,weight_decay=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>pytorch中调用只需在.optim中设置momentum=$\beta$即可</p><h4 id="Learning-rate-tunning：减少收敛点附近震荡">Learning rate tunning：减少收敛点附近震荡</h4><p>lr随着迭代的进行不断减小</p><p>Scheme1：<code>ReduceLROnPlateau(optimizer,'min')</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer=optim.SGD(net.parameters(),lr=learn_rate,momentum=<span class="number">0.78</span>,weight_decay=<span class="number">0.01</span>)</span><br><span class="line">scheduler=ReduceLROnPlateau(optimizer,<span class="string">&#x27;min&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(args.start_epoch,args.epochs):</span><br><span class="line">    train(train_loader,model,criterion,optimizer,epoch)</span><br><span class="line">    result_avg,loss_val=validate(val_loader,model,criterion,epoch)</span><br><span class="line">    scheduler.step(loss_val)</span><br></pre></td></tr></table></figure><p><code>scheduler.step(loss_val)</code>：loss函数连续一定不减小的话，就衰减lr</p><p>Scheme2:设置每30个epoch，lr=0.1*lr一次</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scheduler=StepLR(optimizer,step_size=30,gamma=0.1)</span><br><span class="line">for epoch in range(100):</span><br><span class="line">scheduler.step()</span><br><span class="line">train()</span><br><span class="line">validate()</span><br></pre></td></tr></table></figure><h3 id="Early-stopping：">Early stopping：</h3><p>train performance 还在上升，validation performance已经保持不变或者下降，则 <strong>early stoppping</strong>，保存Validation performance最大的模型</p><h3 id="Dropout：">Dropout：</h3><p>train前向传播时，每个connection有一定概率的输出为0，即令$w_ix_i=0$</p><p>可在任意连接层中添加<code>torch.nn.Dropout(dropout_prob)</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net_dropped=torch.nn.Sequential(</span><br><span class="line">torch.nn.Linear(<span class="number">784</span>,<span class="number">200</span>),</span><br><span class="line">torch.nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">torch.nn.ReLU(),</span><br><span class="line">torch.nn.Linear(<span class="number">200</span>,<span class="number">200</span>),</span><br><span class="line">torch.nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">torch.nn.ReLU(),</span><br><span class="line">torch.nn.Linear(<span class="number">200</span>,<span class="number">10</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><mark>validation 时不用Dropouot</mark></p><p>SGD: Stochastic Gradient Descent</p><p>对于大数据集，由于显存有限，训练时不可能对所有数据求LOSS后进行梯度更新，所以每次随机选择一个batch的数据进行训练。</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性模型</title>
      <link href="/2022/04/17/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/2022/04/17/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1>线性模型</h1><h2 id="问题描述">问题描述</h2><p><img src="https://s1.328888.xyz/2022/04/17/reHbW.png" alt="线性回归"></p><p>通俗点讲，就是给一堆点，找到一条直线使所有点到直线的距离之和最小。数学描述是给定由d个属性描述的示例 ${\bf x}=(x_1,x_2,…,x_d)$,其中$x_i$表示第i个属性上的取值，线性模型试图学得一个通过属性线性组合来进行预测的函数：<br>$$<br>f(x)=w_1x_1+w_2x_2+…+w_dx_d+b<br>$$<br>写成向量形式：<br>$$<br>f(x)={\bf w^Tx}+b<br>$$<br>其中${\bf w}=(w_1,…,w_d)$与$b$ 需要由学习所得</p><h3 id="广义线性回归：">广义线性回归：</h3><p>考虑可微单调函数<code>g(·)</code><br>$$<br>y=g^{-1}(w^Tx)+b<br>$$</p><h2 id="一维线性回归">一维线性回归</h2><p>给定数据集$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$,要使的$f(x_i)=w_i+b$能够尽量与$y_i$接近</p><p>loss：<br>$$<br>Loss=\sum_{i=1}^m(f(x_i)-y_i)^2<br>$$<br>我们需要找到:<br>$$<br>\begin{aligned}<br>(w^<em>,b^</em>)&amp;=arg\min_{w,b}\sum_{i=1}^m(f(x_i)-y_i)^2\\<br>&amp;=arg\min_{w,b}\sum_{i=1}^m(y_i-wx_i-b)^2<br>\end{aligned}<br>$$</p><p>令其偏导等于0：<br>$$<br>\frac{\partial Loss_{(w,b)}}{\partial w}=2(w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i)=0<br>$$</p><p>$$<br>\frac{\partial Loss_{(w,b)}}{\partial b}=2(mb-\sum_{i=1}^m(y_i-wx_i))=0<br>$$</p><p>可得：<br>$$<br>w=\frac{\sum_{i=1}^my_i(x_i-\bar x)}{\sum_{i=1}^mx_i^2-\frac{1}{m}(\sum_{i=1}^mx_i)^2}<br>$$</p><p>$$<br>b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i)<br>$$</p><h2 id="多维线性回归">多维线性回归</h2><p>$$<br>f(x)={\bf w^Tx}+b<br>$$</p><p>将数据集表示为$m\times (d+1)$ 的矩阵形式：<br>$$<br>X=<br>\begin{pmatrix}<br>{x_{11}}&amp;{x_{12}}&amp;{\cdots}&amp;{x_{1d}}&amp;1\\<br>{x_{21}}&amp;{x_{22}}&amp;{\cdots}&amp;{x_{2d}}&amp;1\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}&amp;{\vdots}\\<br>{x_{m1}}&amp;{x_{m2}}&amp;{\cdots}&amp;{x_{md}}&amp;1<br>\end{pmatrix}<br>=\begin{pmatrix}<br>{x_1^T}&amp;1\\<br>{x_2^T}&amp;1\\<br>{\vdots}&amp;{\vdots}\\<br>{x_m^T}&amp;1<br>\end{pmatrix}<br>$$<br>目标y也写成向量形式$y=(y_1,y_2,…,y_m)$,则：<br>$$<br>w^*=(w_1,w_2,…,w_d,b)<br>$$</p><p>$$<br>w^*=arg\min_w(y-Xw)^T(y-Xw)<br>$$<br>矩阵求导可参考<a href="https://zhuanlan.zhihu.com/p/273729929">矩阵求导公式的数学推导（矩阵求导——基础篇） - 知乎 (zhihu.com)</a></p><p>对其求导：<br>$$<br>\frac{\partial Loss_{(w,b)}}{\partial w}=2X^T(Xw-y)=0<br>$$<br>若$X^TX$为满秩矩阵，则：<br>$$<br>w^*=(X^TX)^{-1}X^Ty<br>$$<br>此时：<br>$$<br>f(\hat x_i)=\hat x_i^T(X^TX)^{-1}X^Ty<br>$$</p><p>然而实际情况是，$X^TX$往往不可逆，此时可以解出多个$\hat w$, 他们都能使均方误差最小化，选择哪一个作为输出将由算法的归纳偏好决定，常见做法是引入正则化。</p><h2 id="对数几率回归">对数几率回归</h2><p>即sigmoid函数应用于广义线性回归问题：<br>$$<br>y=\frac{1}{1+e^{(wx^T+b)}}<br>$$<br>若将y视为样本x为正例的可能性，则1-y为样本为反例的可能性，用概率更改上述公式为：<br>$$<br>\ln\frac{p(y=1\vert x)}{p(y=1\vert x)}=w^Tx+b<br>$$<br>显然：<br>$$<br>p(y=1\vert x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}<br>$$</p><p>$$<br>p(y=0\vert x)=\frac{1}{1+e^{w^Tx+b}}<br>$$</p><p>给定数据集${(x_i,y_i)}_{i=1}^m$,对于每一对数据，预测正确的概率为：<br>$$<br>p(y_i\vert x_i;w,b)=y_ip(\hat y_i=1\vert x;w,b)+(1-y_i)p(\hat y_i=0\vert x;w,b)<br>$$<br>其中 $y_i$分为0，1两类.</p><p>则使用极大似然法则，即最大化似然对数：<br>$$<br>l(w,b)=\sum_{i=1}^m\ln p(y_i\vert x_i;w,b)<br>$$<br>令$\beta=(w,b),\ \hat x=(x,1)$,则上式等价于：<br>$$<br>\begin{aligned}<br>-l(\beta)&amp;=\sum_{i=1}^m-p(y_i\vert \hat x_i;\beta)\\<br>&amp;=\sum_{i=1}^m-\ln \frac{y_i(1+e^{\beta^T \hat x_i})+(1-y_i)}{1+e^{\beta^T \hat x_i}}\\<br>&amp;=\sum_{i=1}^m-[ln(y_ie^{\beta^T \hat x_i}+(1-y_i))-\ln(1+e^{\beta^T \hat x_i})]\\<br>&amp;=\sum_{i=1}^m(-y_i\beta ^T\hat x_i+\ln (1+e^{\beta^T\hat x_i} ))<br>\end{aligned}<br>$$</p><p>上式为关于$\beta$的高阶可到连续凸函数最小化该式可由梯度下降法、牛顿法等求解。</p><h2 id="pytorch一维线性回归代码实现">pytorch一维线性回归代码实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> turtle <span class="keyword">import</span> forward</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备</span></span><br><span class="line">x_train=np.array([[<span class="number">3.3</span>],[<span class="number">4.4</span>],[<span class="number">5.5</span>],[<span class="number">6.71</span>],[<span class="number">6.93</span>],[<span class="number">4.168</span>],[<span class="number">9.779</span>],[<span class="number">6.182</span>],[<span class="number">7.59</span>],[<span class="number">2.167</span>],[<span class="number">7.042</span>],[<span class="number">10.791</span>],[<span class="number">5.313</span>],[<span class="number">7.997</span>],[<span class="number">3.1</span>]],dtype=np.float32)</span><br><span class="line">y_train=np.array([[<span class="number">1.7</span>],[<span class="number">2.76</span>],[<span class="number">2.09</span>],[<span class="number">3.19</span>],[<span class="number">1.694</span>],[<span class="number">1.573</span>],[<span class="number">3.366</span>],[<span class="number">2.596</span>],[<span class="number">2.53</span>],[<span class="number">1.221</span>],[<span class="number">2.827</span>],[<span class="number">3.465</span>],[<span class="number">1.65</span>],[<span class="number">2.904</span>],[<span class="number">1.3</span>]],dtype=np.float32)</span><br><span class="line">point=plt.scatter(x_train,y_train)</span><br><span class="line">x_train=torch.from_numpy(x_train)</span><br><span class="line">y_train=torch.from_numpy(y_train)</span><br><span class="line"><span class="comment"># 模型定义y=wx+b</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegreession</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(LinearRegreession,self).__init__() <span class="comment">#调用父类的初始化方法</span></span><br><span class="line">        self.linear=nn.Linear(<span class="number">1</span>,<span class="number">1</span>)<span class="comment">#输入输出都为1维</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        out=self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model=LinearRegreession().cuda() <span class="comment">#如果有GPU,则加载到GPU上运行</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model=LinearRegreession()</span><br><span class="line"></span><br><span class="line"><span class="comment">#LOSS函数</span></span><br><span class="line">criterion=nn.MSELoss()</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=<span class="number">1e-3</span>)<span class="comment">#梯度下降算法</span></span><br><span class="line"></span><br><span class="line">epochs=<span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">#判断是否使用GPU</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        inputs=x_train.cuda()</span><br><span class="line">        target=y_train.cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inputs=x_train</span><br><span class="line">        target=y_train</span><br><span class="line">    <span class="comment">#forward</span></span><br><span class="line">    out=model(inputs)</span><br><span class="line">    loss=criterion(out,target)</span><br><span class="line">    <span class="comment">#backward</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    loss.backward()<span class="comment">#求导</span></span><br><span class="line">    optimizer.step()<span class="comment">#反向更新w与b</span></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>)%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch[&#123;&#125;/&#123;&#125;],loss:&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>,epochs,loss))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">predict=model(x_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(predict))</span><br><span class="line">predict=predict.detach().numpy()</span><br><span class="line">plt.plot(x_train.numpy(),y_train.numpy(),<span class="string">&#x27;ro&#x27;</span>,label=<span class="string">&#x27;Original data&#x27;</span>)</span><br><span class="line">plt.plot(x_train.numpy(),predict,label=<span class="string">&#x27;fitting line&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/17/ryT8g.png" alt="结果.png"></p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 线性模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>激活函数与loss函数</title>
      <link href="/2022/04/13/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8ELoss%E5%87%BD%E6%95%B0/"/>
      <url>/2022/04/13/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8ELoss%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1>激活函数</h1><p><img src="https://s1.328888.xyz/2022/04/13/fx5H4.png" alt="fx5H4.png"><br>$$<br>y=f(\sum_{i=1}^nw_ix_i+b)<br>$$<br>此f()为激活函数，输入为前一层所有输出的加权和再加上一个偏执b</p><h2 id="sigmoid">sigmoid</h2><p>$$<br>f(x)=\sigma(x)=\frac{1}{1+ e^x}\<br>$$</p><p>$$<br>\frac{\partial f(x)}{\partial x}=f(x)(1-f(x))<br>$$</p><p>优点：求导简单，数据可压缩到（0，1）</p><p>缺点：梯度离散，输入很大时输出太平缓</p><p>调用指令<code>torch.sigmoid()</code></p><h2 id="Tanh">Tanh</h2><p>$$<br>f(x)=tanh(x)=\frac{(e^x-e^{-x})}{(e^x+e^{-x})}<br>=2sigmoid(2x)-1<br>$$</p><p>$$<br>\frac{\partial f(x)}{\partial x}=1-tanh^2(x)<br>$$</p><p>调用指令<code>torch.tanh()</code></p><h2 id="ReLU">ReLU</h2><p>$$<br>f(x)=\begin{cases}0\ for\ x&lt;0 \\<br>x\ for\ x\geq0\end{cases}<br>$$</p><p>调用指令<code>torch.relu()</code></p><h1>Loss函数</h1><h2 id="MSE">MSE</h2><p>$$<br>loss_i=\sum(y_i-\bar y_i)^2=norm((y_i-\hat y_i),2)^2<br>$$</p><p>调用指令 torch.nn.functional.mse_loss($y,\hat y$')</p><p>自动求导:</p><p>​<strong>方法一</strong><br>1. 首先对需要求导的参数使用<code>requires_grad_()</code>方法标明需要求导<br>2. 计算mse：torch.nn.functional.mse_loss($y$,$\hat y$)<br>3. 使用<code>torch.autograd.grad(mse,[w])</code>对其进行求导</p><p>​<strong>方法二</strong></p><ol><li><p>首先对需要求导的参数使用<code>requires_grad_()</code>方法标明需要求导</p></li><li><p>计算mse：torch.nn.functional.mse_loss($y，\hat y$)</p></li><li><p>调用<code>mse.backward</code>该指令会计算mse对所有已设置需要求导变量的梯度</p></li><li><p>调用<code>w.grad</code>显示梯度</p><p><mark>backward设置(retain_graph=True)才可以再一次调用，不设置则会报错</mark></p></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">x=torch.ones(<span class="number">1</span>)</span><br><span class="line">w=torch.Tensor([<span class="number">2</span>])</span><br><span class="line">w.requires_grad_()</span><br><span class="line">mse=F.mse_loss(torch.ones(<span class="number">1</span>),x*w)</span><br><span class="line"><span class="comment">#torch.autograd.grad(mse,[w])</span></span><br><span class="line">mse.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([2.])</span></span><br></pre></td></tr></table></figure><h2 id="Softmax">Softmax</h2><p>$$<br>softmax(y_i)=\frac{e^{y_i}}{\sum_ je^{y_j}} \<br>$$</p><p>$$<br>\frac{\partial softmax(y_i)}{\partial y_j}=\begin{cases}softmax(y_i)(1-softmax(y_j))\ \ if\  i=j\\<br>-softmax(y_i)softmax(y_j)\ \ \ \ \ \ \ \ \ if\ i\neq j\end{cases}<br>$$</p><p>优点：将输出label的probability压缩到（0，1），且所有probability之和为1，原数据间隔拉大</p><p>调用指令 <code>torch.nn.functional.mse_loss(y,dim=x)</code></p><h2 id="交叉熵">交叉熵</h2><p>$$<br>H(P,Q)=-\sum_{i=1}^nP(i)logQ(i)<br>$$</p><p>通常用于分类问题</p><p>调用指令 <code>torch.nn.functional.cross_entropy(logits,y)</code></p><p>或者</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred=torch.nn.functional.softmax(logits,dim=<span class="number">1</span>)</span><br><span class="line">pred_log=torch.nn.functional.log(pred)</span><br><span class="line">torch.nn.functional.nll_loss(pred_log,y)</span><br></pre></td></tr></table></figure><p><mark>cross_entropy=softmax→log→nll_loss</mark></p><h2 id="一个二元二次函数梯度下降求极值算法">#一个二元二次函数梯度下降求极值算法</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#一个二元二次函数梯度下降求极值算法</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">himmelblau</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span>(x[<span class="number">0</span>]**<span class="number">2</span>+x[<span class="number">1</span>]-<span class="number">11</span>)**<span class="number">2</span>+(x[<span class="number">0</span>]+x[<span class="number">1</span>]**<span class="number">2</span>-<span class="number">7</span>)**<span class="number">2</span></span><br><span class="line">x=np.arange(-<span class="number">6</span>,<span class="number">6</span>,<span class="number">0.1</span>)</span><br><span class="line">y=np.arange(-<span class="number">6</span>,<span class="number">6</span>,<span class="number">0.1</span>)</span><br><span class="line">X,Y=np.meshgrid(x,y)</span><br><span class="line">Z=himmelblau([X,Y])</span><br><span class="line"></span><br><span class="line">fig=plt.figure(<span class="string">&quot;himelblau&quot;</span>)</span><br><span class="line">ax=fig.gca(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(X,Y,Z)</span><br><span class="line">ax.view_init(<span class="number">60</span>,-<span class="number">30</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.show()<span class="comment">#画出图像</span></span><br><span class="line">x=torch.tensor([<span class="number">4.</span>,<span class="number">0.</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line">optimizer=torch.optim.Adam([x],lr=<span class="number">1e-3</span>)<span class="comment">#lr:learning rate 实例化反向传播算法优化器</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line">    pred=himmelblau(x)</span><br><span class="line">    optimizer.zero_grad()<span class="comment">#参数梯度置为0</span></span><br><span class="line">    pred.backward()</span><br><span class="line">    optimizer.step()<span class="comment">#执行梯度下降</span></span><br><span class="line">    <span class="keyword">if</span> step%<span class="number">2000</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;step&#123;&#125;:x=&#123;&#125;,f(x)=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(step,x.tolist(),pred.item()))</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/13/fP7Oi.png" alt="fP7Oi.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#运算结果</span><br><span class="line">step0:x=[3.999000072479248, -0.0009999999310821295],f(x)=34.0</span><br><span class="line">step2000:x=[3.5741987228393555, -1.764183521270752],f(x)=0.09904692322015762</span><br><span class="line">step4000:x=[3.5844225883483887, -1.8481197357177734],f(x)=2.1100277081131935e-09</span><br><span class="line">step6000:x=[3.5844264030456543, -1.8481241464614868],f(x)=2.41016095969826e-10</span><br><span class="line">step8000:x=[3.58442759513855, -1.848125696182251],f(x)=2.9103830456733704e-11</span><br><span class="line">step10000:x=[3.584428310394287, -1.8481262922286987],f(x)=9.094947017729282e-13</span><br><span class="line">step12000:x=[3.584428310394287, -1.8481265306472778],f(x)=0.0</span><br><span class="line">step14000:x=[3.584428310394287, -1.8481265306472778],f(x)=0.0</span><br><span class="line">step16000:x=[3.584428310394287, -1.8481265306472778],f(x)=0.0</span><br><span class="line">step18000:x=[3.584428310394287, -1.8481265306472778],f(x)=0.0</span><br></pre></td></tr></table></figure><h2 id="一个交叉熵多分类问题">##一个交叉熵多分类问题</h2><p><img src="https://s1.328888.xyz/2022/04/13/fxuiq.png" alt="网络结构"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">##一个交叉熵多分类问题</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensor基本运算</title>
      <link href="/2022/04/13/%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/"/>
      <url>/2022/04/13/%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<h2 id="Tensor基本运算">Tensor基本运算</h2><h3 id="矩阵相乘">矩阵相乘</h3><p><a href="http://torch.mm">torch.mm</a>：只适合矩阵 dim=2情形</p><p>torch.matmul：适用任何形式</p><p>@：简便写法</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a=torch.rand(<span class="number">4</span>,<span class="number">784</span>)</span><br><span class="line">&gt;&gt;&gt;x=torch.rand(<span class="number">512</span>,<span class="number">784</span>)</span><br><span class="line">&gt;&gt;&gt;(a@x.t()).shape</span><br><span class="line"><span class="comment">#torch.Size([4, 512])</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a=torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">64</span>)</span><br><span class="line">&gt;&gt;&gt;b=torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">&gt;&gt;&gt;torch.matmul(a,b).shape</span><br><span class="line"><span class="comment">#torch.Size([4, 3, 28, 32])</span></span><br></pre></td></tr></table></figure><h3 id="乘方">乘方</h3><p>power</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a=torch.full([<span class="number">2</span>,<span class="number">2</span>],<span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt;a.<span class="built_in">pow</span>(<span class="number">2</span>)</span><br><span class="line"><span class="comment">#tensor([[9, 9],</span></span><br><span class="line"><span class="comment">#        [9, 9]])</span></span><br></pre></td></tr></table></figure><h3 id="取整">取整</h3><p>.floor()：向下取整</p><p>.ceil()：向上取整</p><p>.trunc()：取小数</p><p>.frac()：取整数</p><p>.round()：四舍五入</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.tensor(<span class="number">3.14</span>)</span><br><span class="line">a.floor(),a.ceil(),a.trunc(),a.frac(),a.<span class="built_in">round</span>()</span><br><span class="line"><span class="comment">#(tensor(3.), tensor(4.), tensor(3.), tensor(0.1400), tensor(3.))</span></span><br></pre></td></tr></table></figure><h3 id="裁剪">裁剪</h3><p>.clamp()：输入参数<code>min</code> ：将小于min的数都置为min</p><p>​ 输入参数<code>(min,max)</code>：将小于min的数都置为min，大于max的数都置为max</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">3</span>)*<span class="number">15</span></span><br><span class="line">a.clamp(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line"><span class="comment">#tensor([[10.0000, 10.0000,  2.5097],</span></span><br><span class="line"><span class="comment">#         [10.0000,  1.2573,  8.4877]])</span></span><br></pre></td></tr></table></figure><h3 id="自动求导">自动求导:</h3><p><strong>方法一</strong><br>1. 首先对需要求导的参数使用<code>requires_grad_()</code>方法标明需要求导<br>2. 计算mse：torch.nn.functional.mse_loss($y$,$\hat y$)<br>3. 使用<code>torch.autograd.grad(mse,[w])</code>对其进行求导</p><p><strong>方法二</strong></p><ol><li><p>首先对需要求导的参数使用<code>requires_grad_()</code>方法标明需要求导</p></li><li><p>计算mse：torch.nn.functional.mse_loss($y，\hat y$)</p></li><li><p>调用<code>mse.backward</code>该指令会计算mse对所有已设置需要求导变量的梯度</p></li><li><p>调用<code>w.grad</code>显示梯度</p><p><mark>backward设置(retain_graph=True)才可以再一次调用，不设置则会报错</mark></p></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">x=torch.ones(<span class="number">1</span>)</span><br><span class="line">w=torch.Tensor([<span class="number">2</span>])</span><br><span class="line">w.requires_grad_()</span><br><span class="line">mse=F.mse_loss(torch.ones(<span class="number">1</span>),x*w)</span><br><span class="line"><span class="comment">#torch.autograd.grad(mse,[w])</span></span><br><span class="line">mse.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([2.])</span></span><br></pre></td></tr></table></figure><h2 id="Tensor统计属性">Tensor统计属性</h2><h3 id="范数">范数</h3><p><img src="https://s1.328888.xyz/2022/04/13/fNR54.png" alt="范数"></p><p>.norm§：求矩阵的 <strong>p</strong> 范数</p><p>.norm(p,dim=x):在 <strong>x</strong> 维度上做p范数，输出shape为除了原维度去掉x维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">b = a.reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">b.norm(<span class="number">1</span>,dim=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#tensor([0.3336, 0.0033, 0.5679, 0.7974, 0.1241, 0.4108, 0.2766, 0.8038])</span></span><br><span class="line"><span class="comment">#tensor([[[0.3336, 0.0033],</span></span><br><span class="line"><span class="comment">#         [0.5679, 0.7974]],</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#        [[0.1241, 0.4108],</span></span><br><span class="line"><span class="comment">#         [0.2766, 0.8038]]])</span></span><br><span class="line"><span class="comment">#tensor([[0.4577, 0.4141],</span></span><br><span class="line"><span class="comment">#        [0.8445, 1.6012]])</span></span><br></pre></td></tr></table></figure><h3 id="统计属性">统计属性</h3><p>.prod()：累乘</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.prod()</span><br><span class="line"><span class="comment">#tensor(0.0008)</span></span><br></pre></td></tr></table></figure><p>.argmax（）：返回最大元素的索引，该索引是tensor打平为1维的索引</p><p>.argmin（）：返回最小元素的索引，该索引是tensor打平为1维的索引</p><p>.argmax（dim=x）：返回最大元素的索引，该索引是 <strong>x维度上</strong> 的索引</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.argmax()</span><br><span class="line"><span class="comment"># tensor([[[0.2517, 0.9526, 0.5908],</span></span><br><span class="line"><span class="comment">#          [0.1431, 0.3951, 0.5676]],</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [[0.7481, 0.8191, 0.4051],</span></span><br><span class="line"><span class="comment">#          [0.7140, 0.4541, 0.5540]]])</span></span><br><span class="line"><span class="comment"># tensor(1)</span></span><br><span class="line">a = torch.rand([<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.argmax(dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># tensor([[[0.0630, 0.4025, 0.8124],</span></span><br><span class="line"><span class="comment">#          [0.2175, 0.4514, 0.5231]],</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [[0.8366, 0.4124, 0.6334],</span></span><br><span class="line"><span class="comment">#          [0.3470, 0.0701, 0.2093]]])</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 0],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0]])</span></span><br></pre></td></tr></table></figure><p>keepdim=True :返回的tensor与原tensor维度一样</p><h3 id="TOPK与K-TH">TOPK与K-TH</h3><p>.topk(k,dim=x,largest=true): largest=true返回x维度上最大的k个值，largest=false返回x维度上最小的k个值，输出第一个参数为其值，第二个参数维其索引</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.topk(<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.2393, 0.7239, 0.3985, 0.5578],</span></span><br><span class="line"><span class="comment">#         [0.8645, 0.0815, 0.7446, 0.3979],</span></span><br><span class="line"><span class="comment">#         [0.6933, 0.7192, 0.4393, 0.2296],</span></span><br><span class="line"><span class="comment">#         [0.1022, 0.7430, 0.6715, 0.9983]])</span></span><br><span class="line"><span class="comment"># torch.return_types.topk(</span></span><br><span class="line"><span class="comment"># values=tensor([[0.7239, 0.5578, 0.3985],</span></span><br><span class="line"><span class="comment">#         [0.8645, 0.7446, 0.3979],</span></span><br><span class="line"><span class="comment">#         [0.7192, 0.6933, 0.4393],</span></span><br><span class="line"><span class="comment">#         [0.9983, 0.7430, 0.6715]]),</span></span><br><span class="line"><span class="comment"># indices=tensor([[1, 3, 2],</span></span><br><span class="line"><span class="comment">#         [0, 2, 3],</span></span><br><span class="line"><span class="comment">#         [1, 0, 2],</span></span><br><span class="line"><span class="comment">#         [3, 1, 2]]))</span></span><br></pre></td></tr></table></figure><p>.kthvalue(k,dim=x)：返回由小到大第k个值及其索引</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.kthvalue(<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.4287, 0.7747, 0.8699, 0.7784],</span></span><br><span class="line"><span class="comment">#         [0.1043, 0.4982, 0.5863, 0.3341],</span></span><br><span class="line"><span class="comment">#         [0.1408, 0.0510, 0.4056, 0.9592],</span></span><br><span class="line"><span class="comment">#         [0.3366, 0.1080, 0.8596, 0.3885]])</span></span><br><span class="line"><span class="comment"># torch.return_types.kthvalue(</span></span><br><span class="line"><span class="comment"># values=tensor([0.7784, 0.4982, 0.4056, 0.3885]),</span></span><br><span class="line"><span class="comment"># indices=tensor([3, 1, 2, 3]))</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="高阶操作">高阶操作</h2><h3 id="torch-where">torch.where</h3><p>torch.where(condition,x,y)→Tensor</p><p>$$<br>out_i=\begin{cases}x_i\ \ if\ condition_i\\y_i\ \ otherwise\end{cases}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.zeros([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">b=torch.ones([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">condition=torch.rand([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(condition)</span><br><span class="line">torch.where(condition&gt;<span class="number">0.5</span>,a,b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.5633, 0.7544, 0.6521, 0.6338],</span></span><br><span class="line"><span class="comment">#         [0.5439, 0.5644, 0.6126, 0.1168],</span></span><br><span class="line"><span class="comment">#         [0.6247, 0.4382, 0.4246, 0.2221],</span></span><br><span class="line"><span class="comment">#         [0.0017, 0.7347, 0.6782, 0.9357]])</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0., 1.],</span></span><br><span class="line"><span class="comment">#         [0., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 0., 0., 0.]])</span></span><br></pre></td></tr></table></figure><h3 id="torch-gather">torch.gather</h3><p>torch.gather(input,dim,index)→Tensor：根据将index的第dim维作为索引查取input中对应元素并生成Tensor输出<br>$$<br>input=\begin{bmatrix}cat\\dog\\fish\end{bmatrix}\ \ dim=0\ \ index=\begin{bmatrix}1\\2\\0\end{bmatrix}\Rightarrow\ \ out=\begin{bmatrix}dog\\fish\\cat\end{bmatrix}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.rand(<span class="number">4</span>,<span class="number">10</span>)</span><br><span class="line">a1=a.topk(<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line">i=a1[<span class="number">1</span>]</span><br><span class="line">b=torch.arange(<span class="number">10</span>)+<span class="number">100</span></span><br><span class="line">torch.gather(b.expand(<span class="number">4</span>,<span class="number">10</span>),dim=<span class="number">1</span>,index=i)</span><br><span class="line"><span class="comment"># tensor([[100, 105, 101],</span></span><br><span class="line"><span class="comment">#         [101, 105, 108],</span></span><br><span class="line"><span class="comment">#         [107, 104, 100],</span></span><br><span class="line"><span class="comment">#         [101, 107, 106]])</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensor数据类型</title>
      <link href="/2022/04/12/Tensor%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>/2022/04/12/Tensor%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1>Pytorch</h1><p><font size=4>本学习笔记基于<a href="https://www.bilibili.com/video/BV1J44y1i734?spm_id_from=333.337.search-card.all.click">【深度学习Pytorch入门】5天从Pytorch入门到实战！PyTorch深度学习快速入门教程 150全集 绝对通俗易懂（深度学习框架/神经网络）_哔哩哔哩_bilibili</a></font></p><p>Tensorflow：静态图优先</p><p>Pytorch：动态图优先</p><h2 id="Tensor数据类型">Tensor数据类型</h2><h3 id="数据类型">数据类型</h3><table><thead><tr><th>类型</th><th>类型</th></tr></thead><tbody><tr><td>32位浮点型</td><td>(默认)torch.FloatTensor</td></tr><tr><td>64位浮点型</td><td>torch.DoubleTensor</td></tr><tr><td>16位整型</td><td>torch.shortTensor</td></tr><tr><td>32位整型</td><td>torch.IntTensor</td></tr><tr><td>64位整型</td><td>torch.LongTensor</td></tr></tbody></table><h3 id="维度DIM">维度DIM</h3><p>Tensor：张量，可以理解为任意维度的矩阵</p><p><img src="https://s1.328888.xyz/2022/04/12/fREG1.png" alt="fREG1.png"></p><p><mark>Pytorch 没有string类型，其句子用编码one-bot or enbeding 向量表示</mark></p><p>Dim0(标量)：<code>torch.tensor(1.3)</code> 即生成了一个值为1.3的变量 注意 ：<strong>1.3为0维标量 [1.3]为1维矢量</strong></p><p>​通常应用于loss计算</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.tensor(<span class="number">1.2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([])<span class="comment">#空的矩阵，即0维矢量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(a.shape)</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.size()</span><br><span class="line">torch.Size([])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure><p>Dim1(向量)：通常应用于节点输入bias 或者是Linear Input</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.1</span>])</span><br><span class="line">tensor([<span class="number">1.1000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.1</span>,<span class="number">2.1</span>])</span><br><span class="line">tensor([<span class="number">1.1000</span>, <span class="number">2.1000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.FloatTensor(<span class="number">1</span>)</span><br><span class="line">tensor([-<span class="number">1.0842e-19</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.FloatTensor(<span class="number">2</span>)</span><br><span class="line">tensor([<span class="number">0.0000</span>, <span class="number">0.0078</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones(<span class="number">2</span>)</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure><p>Dim2：通常用于多张图片的 <strong>Linear Input Batch</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.randn(2,3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[-0.4689, -1.2038, -1.6282],</span><br><span class="line">        [-0.8379, -1.1376, -1.9624]])</span><br><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([2, 3])</span><br><span class="line">&gt;&gt;&gt; a.size(0)</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; a.size(1)</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; a.shape[0]</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; a.shape[1]</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>Dim3: 用于 <strong>RNN Input Batch</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.rand(1,2,3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[[0.4257, 0.1625, 0.1817],</span><br><span class="line">         [0.3695, 0.8208, 0.5442]]])</span><br><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([1, 2, 3])</span><br><span class="line">&gt;&gt;&gt; a[0]</span><br><span class="line">tensor([[0.4257, 0.1625, 0.1817],</span><br><span class="line">        [0.3695, 0.8208, 0.5442]])</span><br><span class="line">&gt;&gt;&gt; a[0][1]</span><br><span class="line">tensor([0.3695, 0.8208, 0.5442])</span><br><span class="line">&gt;&gt;&gt; a[0][1][1]</span><br><span class="line">tensor(0.8208)</span><br><span class="line">&gt;&gt;&gt; </span><br></pre></td></tr></table></figure><p>$$<br>\begin{bmatrix}\begin{bmatrix}0.4257&amp;0.1625&amp;0.1817\end{bmatrix}\\\begin{bmatrix}0.3695&amp;0.8208&amp;0.5442\end{bmatrix}\end{bmatrix}<br>$$<br>Dim4：适用于 <strong>图片</strong> [batch,channel,height,width]</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.numel()</span><br><span class="line"><span class="number">4704</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.dim()</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure><h3 id="创建Tensor">创建Tensor</h3><p>从np.array创建</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=np.array([<span class="number">2</span>,<span class="number">3.3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.from_numpy(a)</span><br><span class="line">tensor([<span class="number">2.0000</span>, <span class="number">3.3000</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=np.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.from_numpy(a)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p><code>numpy_a=a.numpy()</code>可将Tensor转换位numpy数据类型</p><p>从list创建：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.tensor([[1,2,3,4,5],[3,4,5,6,7]])</span><br><span class="line">tensor([[1, 2, 3, 4, 5],</span><br><span class="line">        [3, 4, 5, 6, 7]])</span><br></pre></td></tr></table></figure><p><font size =5><mark>注意：tensor()输入参数为初始化数据 Tensor()输入参数为shape或list</mark></font></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.Tensor(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor([[<span class="number">2.3063e-31</span>, <span class="number">8.6740e-43</span>, <span class="number">8.4078e-45</span>],</span><br><span class="line">        [<span class="number">0.0000e+00</span>, <span class="number">2.3073e-31</span>, <span class="number">8.6740e-43</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.Tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tensor([<span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">2</span>,<span class="number">3</span>) </span><br><span class="line">报错</span><br></pre></td></tr></table></figure><p>不初始化：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.FloatTensor(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor([[[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.IntTensor(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]]], dtype=torch.int32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.empty(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">tensor([[[ <span class="number">6.4011e+23</span>,  <span class="number">1.7866e+25</span>],</span><br><span class="line">         [-<span class="number">2.2864e-31</span>,  <span class="number">7.7961e+34</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.1093e+27</span>,  <span class="number">4.1709e-08</span>],</span><br><span class="line">         [ <span class="number">3.7392e-38</span>, -<span class="number">1.2803e-26</span>]]])</span><br></pre></td></tr></table></figure><p>随机初始化：<code>rand</code>:[0,1]间均匀分布</p><p><code>rand_like(a)</code>相当于<code>rand(a.shape)</code></p><p><code>rand_int(min,max,shape)</code></p><p><code>randn(shape)</code>(0,1)正态分布</p><p><code>normal(mean=torch.full([shape],mean),std=torch.full([shape],std))</code>:自定义正态分布，均值mean 方差 std</p><p>torch.full：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.full([2,3],1)</span><br><span class="line">tensor([[1, 1, 1],</span><br><span class="line">        [1, 1, 1]])</span><br></pre></td></tr></table></figure><p>torch.arrange:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.arange(1,10,2)</span><br><span class="line">tensor([1, 3, 5, 7, 9])</span><br></pre></td></tr></table></figure><p>torch.linspace:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">4</span>)</span><br><span class="line">tensor([ <span class="number">0.0000</span>,  <span class="number">3.3333</span>,  <span class="number">6.6667</span>, <span class="number">10.0000</span>])</span><br><span class="line">等分为<span class="number">4</span>个数据</span><br></pre></td></tr></table></figure><p>torch.eye：单位矩阵</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.eye(3,3)</span><br><span class="line">tensor([[1., 0., 0.],</span><br><span class="line">        [0., 1., 0.],</span><br><span class="line">        [0., 0., 1.]])</span><br><span class="line">&gt;&gt;&gt; torch.eye(3,5)</span><br><span class="line">tensor([[1., 0., 0., 0., 0.],</span><br><span class="line">        [0., 1., 0., 0., 0.],</span><br><span class="line">        [0., 0., 1., 0., 0.]])</span><br></pre></td></tr></table></figure><p>randperm: 随机打散 可设置为种子每次相同打散方法</p><h3 id="索引与切片">索引与切片</h3><p>与<code>python</code>一样</p><p>间隔切片</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.Tensor(4,3,28,28)</span><br><span class="line">&gt;&gt;&gt; a[:,:,0:28:2,0:28:2].shape</span><br><span class="line">torch.Size([4, 3, 14, 14])</span><br></pre></td></tr></table></figure><p><strong>第二个<code>:</code>后为步长</strong></p><p><code>index_select(维度，torch.tensor[所选index])</code>：<mark>第二个参数必须是tensor </mark></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.Tensor(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.index_select(<span class="number">0</span>,torch.tensor([<span class="number">0</span>,<span class="number">2</span>])).shape</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure><p><code>...</code>:所有的维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>,...].shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[...,:<span class="number">2</span>].shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><p>masked_select():</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0.2088</span>, -<span class="number">0.1852</span>,  <span class="number">0.6233</span>,  <span class="number">0.5107</span>],</span><br><span class="line">        [ <span class="number">1.6500</span>,  <span class="number">0.3151</span>,  <span class="number">1.1227</span>,  <span class="number">1.7956</span>],</span><br><span class="line">        [-<span class="number">1.1915</span>,  <span class="number">0.8243</span>, -<span class="number">0.0114</span>,  <span class="number">0.7303</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask=a.ge(<span class="number">0.5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask</span><br><span class="line">tensor([[<span class="literal">False</span>, <span class="literal">False</span>,  <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>, <span class="literal">False</span>,  <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>,  <span class="literal">True</span>, <span class="literal">False</span>,  <span class="literal">True</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.masked_select(mask)</span><br><span class="line">tensor([<span class="number">0.6233</span>, <span class="number">0.5107</span>, <span class="number">1.6500</span>, <span class="number">1.1227</span>, <span class="number">1.7956</span>, <span class="number">0.8243</span>, <span class="number">0.7303</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.masked_select(mask).shape</span><br><span class="line">torch.Size([<span class="number">7</span>])</span><br></pre></td></tr></table></figure><h3 id="维度变换">维度变换</h3><h4 id="reshape：">reshape：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.rand(<span class="number">4</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">a.reshape(<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">784</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">784</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">4</span>*<span class="number">28</span>,<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">112</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">4</span>*<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure><h4 id="squeeze-unsqueeze">squeeze/unsqueeze:</h4><p>unsqueeze():参数取值范围 <strong>[-dim-1,dim+1)  正索引</strong> 是在当前索引 <strong>之后</strong> 插入，<strong>负索引</strong>是在当前索引 <strong>之前</strong> 插入</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.unsqueeze(3).shape</span><br><span class="line">torch.Size([4, 1, 28, 1, 28])</span><br><span class="line">&gt;&gt;&gt; a.unsqueeze(-3).shape</span><br><span class="line">torch.Size([4, 1, 1, 28, 28])</span><br></pre></td></tr></table></figure><p>squee(idx): 删除当前维度</p><h4 id="transpose-t-permute：">transpose/t/permute：</h4><p>transpose：维度交换</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.transpose(1,3).shape</span><br><span class="line">torch.Size([4, 28, 28, 1])</span><br></pre></td></tr></table></figure><p>t：只能用于矩阵 二维</p><p>permute：重构 输入参数为维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><h4 id="expand-repeat：">expand/repeat：</h4><p>expand: 需要时才复制数据，输入参数为扩张后的大小，只有为1的才能扩张，-1表示该维度保持不限</p><p>repeat：一开始就复制数据,输入参数为复制次数</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.expand(-1,4,28,28).shape</span><br><span class="line">torch.Size([4, 4, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.repeat(4,1,1,1).shape</span><br><span class="line">torch.Size([16, 1, 28, 28])</span><br></pre></td></tr></table></figure><h3 id="合并与分割">合并与分割</h3><h4 id="cat">cat</h4><p><strong>除了需要合并的dim以外，其他的dim大小应该相同</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.rand(<span class="number">4</span>,<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b=torch.rand(<span class="number">5</span>,<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat([a,b],dim=<span class="number">0</span>).shape</span><br><span class="line">torch.Size([<span class="number">9</span>, <span class="number">32</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><h4 id="stack">stack</h4><p><strong>重新在合并的dim维度之前添加一个维度，该维度不同取值显示合并前不同内容</strong></p><p><strong>要求所有dim的大小一样</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack([a,b],dim=<span class="number">1</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">2</span>, <span class="number">32</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><h4 id="split">split</h4><p><strong>通过长度拆分</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([3, 4, 32])</span><br><span class="line">&gt;&gt;&gt; b.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; c.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; d.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; a1,a2=a.split([2,1],dim=0)</span><br><span class="line">&gt;&gt;&gt; a1.shape</span><br><span class="line">torch.Size([2, 4, 32])</span><br><span class="line">&gt;&gt;&gt; a2.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; a1,a2=a.split(2,dim=0)</span><br><span class="line">&gt;&gt;&gt; a1.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br><span class="line">&gt;&gt;&gt; a2.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br></pre></td></tr></table></figure><h4 id="chunk">chunk</h4><p><strong>通过数量拆分</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a1,a2=a.chunk(2,dim=0)</span><br><span class="line">&gt;&gt;&gt; a1.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br><span class="line">&gt;&gt;&gt; a2.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br></pre></td></tr></table></figure><h3 id="Broadcasting">Broadcasting</h3><p><mark>大维度缺失可自动添加，且每个维度可以自动扩张</mark></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.rand(4,4,32,32)</span><br><span class="line">&gt;&gt;&gt; b=torch.rand(4,1,1)</span><br><span class="line">&gt;&gt;&gt; c=a+b</span><br><span class="line">&gt;&gt;&gt; c.shape</span><br><span class="line">torch.Size([4, 4, 32, 32])</span><br></pre></td></tr></table></figure><p>b:[4,1,1]→[1,4,1,1]→[4,4,32,32]</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.rand(1,3)</span><br><span class="line">&gt;&gt;&gt; b=torch.rand(3,1)</span><br><span class="line">&gt;&gt;&gt; c=a+b</span><br><span class="line">&gt;&gt;&gt; c.shape</span><br><span class="line">torch.Size([3, 3])</span><br></pre></td></tr></table></figure><p>a:[1,3]→[3,3]</p><p>b:[3,1]→[3,3]</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mathjax基本语法</title>
      <link href="/2022/04/12/mathjax%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"/>
      <url>/2022/04/12/mathjax%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1>mathjax基本语法</h1><h2 id="1-基本语法">1.基本语法</h2><h3 id="1-1显示公式">1.1显示公式</h3><p><font size =4>行内公式：<code>$公式$</code></p><p>文内公式：单独一行</p><p><code>$$公式$$</code></p><h3 id="1-2-特殊字符">1.2 特殊字符</h3><h4 id="1-2-1-希腊字符">1.2.1 希腊字符</h4><table><thead><tr><th>显示</th><th>命令</th><th>显示</th><th>命令</th><th>显示</th><th>命令</th></tr></thead><tbody><tr><td>α</td><td>\alpha</td><td>β</td><td>\beta</td><td>υ</td><td>\upsilon</td></tr><tr><td>γ</td><td>\gamma</td><td>δ</td><td>\delta</td><td>ϕ</td><td>\phi</td></tr><tr><td>ϵ</td><td>\epsilon</td><td>ζ</td><td>\zeta</td><td>χ</td><td>\chi</td></tr><tr><td>ι</td><td>\iota</td><td>θ</td><td>\theta</td><td>ψ</td><td>\psi</td></tr><tr><td>η</td><td>\eta</td><td>κ</td><td>\kappa</td><td>ω</td><td>\omega</td></tr><tr><td>λ</td><td>\lambda</td><td>μ</td><td>\mu</td><td></td><td></td></tr><tr><td>ν</td><td>\nu</td><td>ξ</td><td>\xi</td><td></td><td></td></tr><tr><td>π</td><td>\pi</td><td>ρ</td><td>\rho</td><td></td><td></td></tr><tr><td>σ</td><td>\sigma</td><td>τ</td><td>\tau</td><td></td><td></td></tr></tbody></table><p><img src="https://appwk.baidu.com/naapi/doc/view?ih=621&amp;o=jpg_6_0_______&amp;iw=636&amp;ix=0&amp;iy=0&amp;aimw=636&amp;rn=1&amp;doc_id=84a3254a67ec102de3bd897b&amp;pn=1&amp;sign=00ee614654f2c18484257f890bd40217&amp;type=1&amp;app_ver=2.9.8.2&amp;ua=bd_800_800_IncredibleS_2.9.8.2_2.3.7&amp;bid=1&amp;app_ua=IncredibleS&amp;uid=&amp;cuid=&amp;fr=3&amp;Bdi_bear=WIFI&amp;from=3_10000&amp;bduss=&amp;pid=1&amp;screen=800_800&amp;sys_ver=2.3.7" alt="希腊字母大小写对照表"></p><p><mark>需要大写字母则将第一个字母大写</mark></p><p><code>\Gamma</code> :$\Gamma$</p><p><mark>需要斜体大写字母则在前加var</mark></p><p><code>\varGamma</code>: $\varGamma$</p><h4 id="1-2-2-特殊格式">1.2.2 特殊格式</h4><ul><li><p>上下标</p><p>上标：<code>^</code> $x^2$</p><p>下标：<code>_</code>$x_2$</p></li><li><p>向量</p><p>短箭头：<code>\vec a</code> $\vec a$ <code>\vec &#123;ab&#125;</code> $\vec {ab}$</p><p>长箭头：<code>\overrightarrow a</code> $\overrightarrow a$ <code>\overrightarrow &#123;ab&#125;</code> $\overrightarrow {ab}$</p></li><li><p>bar</p><p>上箭头：<code>\hat a</code> $\hat a$</p><p>横线：<code>\overline a</code> $\overline a$</p><p>下划线 <code>\underline a</code> $\underline a$</p></li><li><p>字体</p><p>空心字：<code>\mathbb &#123;a&#125;</code> $\mathbb {ABCDEFG}$</p></li><li><p><mark>空格</mark></p><p>空格需要转义字符\ ：<code>a\ b</code> $a\ b$</p></li></ul><h4 id="1-2-3括号与分组">1.2.3括号与分组</h4><ul><li><p>同一级用{}处理：<code>x_i^2</code> $x_i^2$ <code>x_&#123;i^2&#125;</code> $x_{i^2}$</p></li><li><p>小括号中括号可直接使用，大括号需要专业字符: <code>\&#123;...\&#125;</code></p></li><li><p>尖括号 <code>\langle...\rangle</code> $\langle ab\rangle$</p></li><li><p>绝对值 <code>\vert ... \vert</code> $\vert ab \vert$</p></li><li><p>双竖线 <code>\Vert ...\Vert</code> $\Vert ab \Vert$</p></li><li><p>使用<code>\left</code>和<code>\right</code>)使符号大小与邻近的公式相适应,该语句适用于所有括号类型</p></li><li><p><code>\left\&#123;\frac&#123;(x+y)&#125;&#123;[\alpha+\beta]&#125;\right\&#125;</code>显示为$\left\{\frac{(x+y)}{[\alpha+\beta]}\right\}$</p><p><font size=6><mark>hexo 解析时<code>\left\&#123;</code>和<code>\right\&#125;</code>会分别解析为<code>\left&#123;</code>与<code>\right&#125;</code>,想要正确解析需要更改为<code>\\&#123;  \\&#125;</code></mark></font></p></li></ul><h4 id="1-2-4-运算符">1.2.4 运算符</h4><table><thead><tr><th>运算符</th><th>演示</th><th>运算符</th><th>演示</th></tr></thead><tbody><tr><td>\times</td><td>$x \times y$</td><td>\cdot</td><td>$x \cdot y$</td></tr><tr><td>\ast</td><td>$x \ast y$</td><td>\div</td><td>$x \div y$</td></tr><tr><td>\pm</td><td>$x \pm y$</td><td>\mp</td><td>$x \mp y$</td></tr><tr><td>\leq</td><td>$x \leq y$</td><td>\geq</td><td>$x \geq y$</td></tr><tr><td>\approx</td><td>$x \approx y$</td><td>\equiv</td><td>$x \equiv y$</td></tr><tr><td>\bigodot</td><td>$x \bigodot y$</td><td>\bigtimes</td><td>$x \bigtimes y$</td></tr></tbody></table><table><thead><tr><th>运算符</th><th>演示</th><th>运算符</th><th>演示</th></tr></thead><tbody><tr><td>\in</td><td>$x \in y$</td><td>\subset</td><td>$x \subset y$</td></tr><tr><td>\subseteq</td><td>$x \subseteq y$</td><td>\varnothing</td><td>$\varnothing$</td></tr><tr><td>\cup</td><td>$x \cup y$</td><td>\cap</td><td>$x \cap y$</td></tr></tbody></table><h4 id="1-2-5-特殊符号">1.2.5 特殊符号</h4><table><thead><tr><th>代码</th><th>演示</th><th>命令</th></tr></thead><tbody><tr><td>\overbrace</td><td>$\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}$</td><td>\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}</td></tr><tr><td>\underbrace</td><td>$\underbrace{b+c}_{1.0}$</td><td>\underbrace{b+c}_{1.0}</td></tr><tr><td>\partial</td><td>$\frac{\partial z}{\partial x}$</td><td>\frac{\partial z}{\partial x}</td></tr><tr><td>\idots</td><td>$1,2,\ldots,n$</td><td>1,2,\ldots,n</td></tr><tr><td>\cdots</td><td>$1,2,\cdots,n$</td><td>1,2,\cdots,n$</td></tr><tr><td>\infty</td><td>$\infty$</td><td>–</td></tr><tr><td>\nabla</td><td>$\nabla$</td><td>–</td></tr><tr><td>\forall</td><td>$\forall$</td><td>–</td></tr><tr><td>\exists</td><td>$\exists$</td><td>–</td></tr><tr><td>\triangle</td><td>$\triangle$</td><td>–</td></tr><tr><td>\lnot</td><td>$\lnot$</td><td></td></tr></tbody></table><table><thead><tr><th>\uparrow</th><th>$\uparrow$</th><th>\Uparrow</th><th>$\Uparrow$</th></tr></thead><tbody><tr><td>\downarrow</td><td>$\downarrow$</td><td>\Downarrow</td><td>$\Downarrow$</td></tr><tr><td>\leftarrow</td><td>$\leftarrow$</td><td>\Leftarrow</td><td>$\Leftarrow$</td></tr><tr><td>\rightarrow</td><td>$\rightarrow$</td><td>\Rightarrow</td><td>$\Rightarrow$</td></tr></tbody></table><ul><li><p>求和 符号与积分</p><table><thead><tr><th>\sum</th><th>$\sum$</th><th>\sum_{i=0}^n</th><th>$\sum_{i=0}^n$</th><th>\displaystyle\sum_{i=0}^n</th><th>$\displaystyle\sum_{i=0}^n$</th></tr></thead><tbody><tr><td>\lim</td><td>$\lim$</td><td>\lim_{x\to\infty}</td><td>$\lim_{x\to\infty}$</td><td>\displaystyle\lim_{x\to\infty}</td><td>$\displaystyle\lim_{x\to\infty}$</td></tr><tr><td>\int</td><td>$\int$</td><td>\iint</td><td>$\iint$</td><td>\iiint</td><td>$\iiint$</td></tr><tr><td>\oint</td><td>$\oint$</td><td>\int_0^\infty{fxdx}</td><td>$\int_0^\infty{fxdx}$</td><td>\prod</td><td>$\prod$</td></tr></tbody></table></li></ul><h3 id="1-3-分式与根式">1.3 分式与根式</h3><ul><li>分式 <code>\frac&#123;分子&#125;&#123;分母&#125;</code> $\frac{x}{y}$</li><li>根式 <code>\sqrt[x]&#123;y&#125;</code>$\sqrt[x]{y}$</li></ul><h3 id="1-4-特殊函数">1.4 特殊函数</h3><p><code>\函数名</code> $\sin{x}$ $\log_2{x}$</p><h2 id="2-矩阵">2.矩阵</h2><h3 id="2-1矩阵生成">2.1矩阵生成</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;matrix&#125; </span><br><span class="line">1&amp;0&amp;1\\</span><br><span class="line">0&amp;1&amp;0\\</span><br><span class="line">1&amp;1&amp;0\\</span><br><span class="line">\end&#123;matrix&#125;</span><br></pre></td></tr></table></figure><ul><li>开始和结束需要输入 <code>\begin&#123;matrix&#125; \end&#123;matrix&#125;</code></li><li>同一行元素之间用 <strong>&amp;</strong> 符号连接</li><li><font size=6>换行 <strong>\\\\</strong> <mark>在hexo中时，<strong>\\\\<strong>会解析为一个</strong>\</strong>,故换行时需要输入 <strong>\\\\\\\\</strong> 且该方法只有在 <strong>矩阵、列表、方程组</strong> 等有<code>\beng&#123;&#125;\end&#123;&#125;</code>包围的公式中有效，若想写两行普通公式，还请插入 <strong>两个</strong> 公式块 </mark></font></li></ul><p>$$\begin{matrix}<br>1&amp;0&amp;1\\<br>0&amp;1&amp;0\\<br>1&amp;1&amp;0\\<br>\end{matrix} $$</p><h3 id="2-2矩阵边框">2.2矩阵边框</h3><p>在起始、结束位置替换matrix</p><table><thead><tr><th>pmatrix</th><th>$\begin{pmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{pmatrix}$</th><th>bmatrix</th><th>$\begin{bmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{bmatrix}$</th></tr></thead><tbody><tr><td>Bmatrix</td><td>$\begin{Bmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{Bmatrix}$</td><td>vmatrix</td><td>$\begin{vmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{vmatrix}$</td></tr><tr><td>Vmatrix</td><td>$\begin{Vmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{Vmatrix}$</td><td></td><td></td></tr></tbody></table><h3 id="2-3-高维矩阵的表示">2.3 高维矩阵的表示</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;bmatrix&#125;</span><br><span class="line">&#123;a_&#123;11&#125;&#125;&amp;&#123;a_&#123;12&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;1n&#125;&#125;\\</span><br><span class="line">&#123;a_&#123;21&#125;&#125;&amp;&#123;a_&#123;22&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;2n&#125;&#125;\\</span><br><span class="line">&#123;\vdots&#125;&amp;&#123;\vdots&#125;&amp;&#123;\ddots&#125;&amp;&#123;\vdots&#125;\\</span><br><span class="line">&#123;a_&#123;n1&#125;&#125;&amp;&#123;a_&#123;n2&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;nn&#125;&#125;\\</span><br><span class="line">\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure><p>$$\begin{bmatrix}<br>{a_{11}}&amp;{a_{12}}&amp;{\cdots}&amp;{a_{1n}}\\<br>{a_{21}}&amp;{a_{22}}&amp;{\cdots}&amp;{a_{2n}}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{a_{n1}}&amp;{a_{n2}}&amp;{\cdots}&amp;{a_{nn}}\\<br>\end{bmatrix}$$</p><ul><li><p>横省略号：<code>\cdots</code></p></li><li><p>竖省略号： <code>\vdots</code></p></li><li><p>斜省略号：<code>\ddots</code></p><h2 id="3-列表">3.列表</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;array&#125;&#123;c|lll&#125;</span><br><span class="line">&#123;\downarrow&#125;&amp;&#123;name&#125;&amp;&#123;age&#125;&amp;&#123;ID&#125;\\</span><br><span class="line">\hine</span><br><span class="line">&#123;num1&#125;&amp;&#123;yy&#125;&amp;&#123;22&#125;&amp;&#123;1320&#125;\\</span><br><span class="line">&#123;num2&#125;&amp;&#123;lw&#125;&amp;&#123;22&#125;&amp;&#123;1111&#125;\\</span><br><span class="line">\end&#123;array&#125;</span><br></pre></td></tr></table></figure><p>$$\begin{array}{c|lll}<br>{\downarrow}&amp;{name}&amp;{age}&amp;{ID}\\<br>\hline<br>{num1}&amp;{yy}&amp;{22}&amp;{1320}\\<br>{num2}&amp;{lw}&amp;{22}&amp;{1111}\\<br>\end{array}$$</p></li><li><p>起始、结束处以 <strong>{array}</strong> 声明</p></li><li><p>对齐方式:在{array}后以{}逐列统一声明</p></li><li><p>左对齐:l；居中：c；右对齐：r</p></li><li><p>竖直线:在声明对齐方式时，插入 <strong>|</strong> 建立竖直线</p></li><li><p>插入水平线:<strong>\hline</strong></p></li></ul><h2 id="3-方程组">3.方程组</h2><ul><li><p>起始结束为 <strong>{cases}</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;cases&#125;</span><br><span class="line">a_1x+b_1y+c_1z=d_1\\</span><br><span class="line">a_2x+b_2y+c_2z=d_2\\</span><br><span class="line">a_3x+b_3y+c_3z=d_3\\</span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure><p>$$\begin{cases}<br>a_1x+b_1y+c_1z=d_1\\<br>a_2x+b_2y+c_2z=d_2\\<br>a_3x+b_3y+c_3z=d_3\\<br>\end{cases}$$</p></li></ul><h2 id="长公式换行">长公式换行</h2><p>用<code>&amp;</code>表示对齐位置，<code>\\</code>换行</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line">\frac&#123;\partial w^l&#125;&#123;\partial m_&#123;F_i^l&#125;&#125;&amp;=\frac&#123;\partial&#125;&#123;\partial m_&#123;F_i^l&#125;&#125;\prod_&#123;j=1&#125;^pexp\left\&#123;-\frac&#123;1&#125;&#123;2&#125;[(x_j^&#123;(t)&#125;-m_&#123;F_j^l&#125;)/\sigma_&#123;F_j^l&#125;]^2 \right\&#125;\\&amp;=\frac&#123;\partial &#125;&#123;\partial m_&#123;F_i^l&#125;&#125;exp\left\&#123;  -\frac&#123;1&#125;&#123;2&#125;[(x_i^&#123;(t)&#125;-m_&#123;F_i^l&#125;)/\sigma_&#123;F_i^l&#125;]^2 \right\&#125;\times \prod_&#123;j=1\\j\neq i&#125;^pexp\left\&#123;  -\frac&#123;1&#125;&#123;2&#125;[(x_j^&#123;(t)&#125;-m_&#123;F_j^l&#125;)/\sigma_&#123;F_j^l&#125;]^2\right\&#125;\\&amp;=\prod_&#123;j=1&#125;^pexp\left\&#123;  -\frac&#123;1&#125;&#123;2&#125;[(x_j^&#123;(t)&#125;-m_&#123;F_j^l&#125;)/\sigma_&#123;F_j^l&#125;]^2 \right\&#125;\times \frac&#123;x_i^&#123;(i)&#125;-m_&#123;F_i^l&#125;&#125;&#123;\sigma_&#123;F_i^l&#125;^2&#125;\\&amp;=\frac&#123;x_i^&#123;(i)&#125;-m_&#123;F_i^l&#125;&#125;&#123;\sigma_&#123;F_i^l&#125;^2&#125;\times w^l</span><br><span class="line">\end&#123;split&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>\begin{split}{}<br>\frac{\partial w^l}{\partial m_{F_i^l}}&amp;=\frac{\partial}{\partial m_{F_i^l}}\prod_{j=1}^pexp\left{-\frac{1}{2}[(x_j^{(t)}-m_{F_j^l})/\sigma_{F_j^l}]^2 \right}\<br>&amp;=\frac{\partial }{\partial m_{F_i^l}}exp\left{  -\frac{1}{2}[(x_i^{(t)}-m_{F_i^l})/\sigma_{F_i^l}]^2 \right}\times \prod_{j=1\j\neq i}^pexp\left{  -\frac{1}{2}[(x_j^{(t)}-m_{F_j^l})/\sigma_{F_j^l}]^2\right}\<br>&amp;=\prod_{j=1}^pexp\left{  -\frac{1}{2}[(x_j^{(t)}-m_{F_j^l})/\sigma_{F_j^l}]^2 \right}\times \frac{x_i^{(i)}-m_{F_i^l}}{\sigma_{F_i^l}^2}\<br>&amp;=\frac{x_i^{(i)}-m_{F_i^l}}{\sigma_{F_i^l}^2}\times w^l<br>\end{split}<br>$$</p><blockquote><p>参考</p><p><a href="https://blog.csdn.net/ajacker/article/details/80301378?spm=1001.2014.3001.5502">Mathjax语法总结_ajacker的博客-CSDN博客_mathjax语法</a></p><p><a href="https://blog.csdn.net/ethmery/article/details/50670297">基本数学公式语法(of MathJax)_PUMC芋圆四号的博客-CSDN博客_mathjax语法</a></font></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 杂文 </category>
          
          <category> mathjax </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathjax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown与Typora</title>
      <link href="/2022/04/11/Markdown%E4%B8%8ETypora/"/>
      <url>/2022/04/11/Markdown%E4%B8%8ETypora/</url>
      
        <content type="html"><![CDATA[<h1>markdown基本语法与Typora</h1><p><font size=5>本文既是对markdown语法及Typora快捷键的记录 也是练习</font></p><p><mark>注意在行内插入加粗、斜体等最好在*符号与左右文本之间最好加上一个半角空格，不然hexo可能解析错误</mark></p><p>比如应<code>你好 **yy** 很高兴</code>而不是<code>你好**yy**很高兴</code></p><ol><li><p>加粗 <code>ctrl+B</code></p><p><code> **文字**</code><br><strong>文字</strong></p></li><li><p>倾斜 <code>Ctrl+I</code></p><p><code>*斜体字*</code><br><em>斜体字</em></p></li><li><p>下划线 <code>ctrl+U</code></p><p><code>&lt;u&gt; 下划线&lt;/u&gt;</code><br><u>下划线</u></p></li><li><p>多级标题 <code>Ctrl+1~6</code></p><p><code># 一级标题</code></p><h1>一级标题</h1><p><code>## 二级标题</code></p><h2 id="二级标题">二级标题</h2><p>以此类推</p></li><li><p>有序列表 <code>Ctrl+Shift+[</code></p><p><code>1. 文字</code></p><ol><li><p>一</p></li><li><p>二</p></li></ol></li><li><p>无序列表 <code>Ctrl+Shift+]</code></p><p><code>- 无序列表</code></p><ul><li>无序列表</li></ul></li><li><p>降级 <code>Tab</code></p></li><li><p>升级 <code>Shift+Tab</code></p></li><li><p>插入链接 <code>Ctrl+K</code></p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">文字链接:：[链接名称]（http://链接网址） </span><br><span class="line">网址链接：&lt;http://...&gt;</span><br></pre></td></tr></table></figure><p><code>&lt;网址&gt;</code> <a href="http://baidu.com">http://baidu.com</a></p><p><code>[百度](http://)</code> <a href="http://www.baidu.com">百度</a></p></li><li><p>插入公式 <code>Ctrl+Shift+M</code><br>使用时需要在front-matter中加上mathjax: true</p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">数学公式</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$\lim_{x\to\infty}\exp(-x)=0$$</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">内联公式 $\lim_&#123;x\to\infty&#125;\exp(-x)=0$</span><br></pre></td></tr></table></figure><p>内联公式: $\lim_{x\to\infty}\exp(-x)=0$<br><font size=5>注意使用内联公式时可能与非内联公式样式不同，比如上示lim下标  </font></p></li><li><p>行内代码 <code>Ctrl+shift+k</code></p><p>````代码` ```</p></li><li><p>插入图片 <code>Ctrl+Shift+I</code></p><p><code>![图片名称](http://)</code></p><p><img src="https://s1.328888.xyz/2022/04/11/fXIeF.png" alt="图片1"></p></li><li><p>创建表格 <code>Ctrl+T</code></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| 姓名 | 性别 |</span><br><span class="line">| :--- | ---：|</span><br><span class="line">| 张三 | 男 |</span><br><span class="line">| 李四 | 女 |</span><br></pre></td></tr></table></figure><table><thead><tr><th>姓名</th><th>性别</th></tr></thead><tbody><tr><td>张三</td><td>男</td></tr><tr><td>李四</td><td>女</td></tr></tbody></table></li><li><p>删除线 <code>ALT+Shift+5</code></p><p><code>~~删除线~~</code></p><p><s>删除线</s></p></li><li><p>引用 <code>Ctrl+Shift+Q</code></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; 这是一个引用</span><br><span class="line">&gt;&gt;这是一个嵌套引用</span><br></pre></td></tr></table></figure><blockquote><p>这是一个引用</p><blockquote><p>这是一个引用嵌套</p></blockquote></blockquote></li><li><p>上标</p><p><code> X&lt;sup&gt;2&lt;sup&gt;</code></p><p>X<sup>2</sup></p></li><li><p>下标</p><p><code>H&lt;sub&gt;2&lt;/suB&gt;O</code></p><p>H<sub>2</sub>O</p></li><li><p>分割线 <code>*** 或者 ___</code></p><hr><hr></li><li><p>自动产生目录 <code>[TOC]+Enter</code></p><p><code>[TOC]</code></p></li><li><p>改变字体大小</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span>=<span class="string">1</span>&gt;</span>字体大小size=1<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span>=<span class="string">3</span>&gt;</span>字体大小size=3<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span>=<span class="string">5</span>&gt;</span>字体大小size=5<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font size=1>字体大小size=1</font></p><p><font size=3>字体大小size=3</font></p><p><font size=5>字体大小size=5</font></p></li><li><p>改变字体颜色</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">red</span>&gt;</span>红色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">&quot;blue&quot;</span>&gt;</span>蓝色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">Yellow</span>&gt;</span>黄色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">YellowGreen</span>&gt;</span>黄绿色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font color=red>红色</font><br><font color="blue">蓝色</font><br><font color=Yellow>黄色</font><br><font color=YellowGreen>黄绿色</font></p></li><li><p>改变字体类型</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;黑体&quot;</span>&gt;</span>黑体<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;宋体&quot;</span>&gt;</span>宋体<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;仿宋&quot;</span>&gt;</span>仿宋<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;幼圆&quot;</span>&gt;</span>幼圆<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;楷书&quot;</span>&gt;</span>楷书<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文行楷&quot;</span>&gt;</span>华文行楷<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文隶书&quot;</span>&gt;</span>华文隶书<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文新魏&quot;</span>&gt;</span>华文新魏<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文彩云&quot;</span>&gt;</span>华文彩云<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文琥珀&quot;</span>&gt;</span>华文琥珀<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font face="黑体">黑体</font><br><font face="宋体">宋体</font><br><font face="仿宋">仿宋</font><br><font face="幼圆">幼圆</font><br><font face="楷书">楷书</font><br><font face="华文行楷">华文行楷</font><br><font face="华文隶书">华文隶书</font><br><font face="华文新魏">华文新魏</font><br><font face="华文彩云">华文彩云</font><br><font face="华文琥珀">华文琥珀</font></p></li><li><p>文本高亮</p><p><code>&lt;mark&gt;highlight 2&lt;/mark&gt;</code></p><p><mark>highlight 2</mark></p></li></ol><p>​</p><p>​</p>]]></content>
      
      
      <categories>
          
          <category> 杂文 </category>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> Typora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>序言</title>
      <link href="/2022/04/11/%E5%BA%8F%E8%A8%80/"/>
      <url>/2022/04/11/%E5%BA%8F%E8%A8%80/</url>
      
        <content type="html"><![CDATA[<h1>剑谱</h1><h2 id="序言">序言</h2><p><img src="https://s1.328888.xyz/2022/04/11/fXIeF.png" alt="fXIeF.png"></p><p><font size=4>   小生于壬寅年元月痛失所爱，数月以来郁郁寡欢，再不得昨日之愉，然偶见各路大神奉为顶上珍宝的格言：</p><blockquote><p>剑谱第一页 无爱既是神</p></blockquote><p>  日思夜想之下竟真悟出了几番道理，称其为道理确些许有失偏颇，然实有些许感悟，故在此立此blog，欲决心 <strong><s>发奋学习</s></strong> 练剑，将些许 <strong><s>学习笔记</s></strong> 心得写于此剑谱之中，望暮年之日回首往事，仍有迹可循。</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 序言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 骚话 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
