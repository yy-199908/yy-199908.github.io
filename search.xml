<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>生成对抗网络</title>
      <link href="/2022/04/27/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
      <url>/2022/04/27/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="生成对抗网络-GANs"><a href="#生成对抗网络-GANs" class="headerlink" title="生成对抗网络(GANs)"></a>生成对抗网络(GANs)</h1><p>顾名思义，生成对抗网络由两部分组成，一是生成模型，就像之前介绍的自动编码器的解码部分。二是对抗模型：严格来说是一个判断真假图片的判别器。</p><p>简单来说，生成对抗网络就是希望两个网络相互竞争，通过生成网络生成假的数据，对抗网络判别真伪，最后希望生成网络生成的数据能够以假乱真。</p><h2 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h2><p>此处生成模型不再是将图片输入编码器再通过解编码生成假图片，而是随机初始化一个隐含向量，生成图片，具体步骤如下：首先给出一个简单的高维的正态分布的噪声向量，然后这个时候我们可以通过仿射变换，也就是 xw+b 将其映射到一个更高的维度，然后将他重新排列成一个矩形，这样看着更像一张图片，接着进行一些卷积、转置卷积、池化、激活函数等进行处理，最后得到了一个与我们输入图片大小一模一样的噪音矩阵，这就是我们所说的假的图片。</p><p>这个时候我们如何去训练这个生成器呢？这就需要通过对抗学习，增大判别器判别这个结果为真的概率，通过这个步骤不断调整生成器的参数，希望生成的图片越来越像真的，而在这一步中我们不会更新判别器的参数，因为如果判别器不断被优化，可能生成器无论生成什么样的图片都无法骗过判别器。</p><h2 id="对抗模型"><a href="#对抗模型" class="headerlink" title="对抗模型"></a>对抗模型</h2><p>对抗模型简单来说是一个真假图片的判别器，所以其实就是一个二分类问题。对于假的图片我们期望它输出为0，真的图片我希望它输出为1，这个输出与原图片label没关系，不管原图片有多少总分类，对抗模型都只是一个判断图片为真或为假 的二分类问题。</p><p>在训练时，我们先对对抗模型进行训练，希望其能正确区分真的或假的图片。然后再训练生成模型，希望其生成的图片能够骗过对抗模型。</p><p>对于此二分类问题，可以用我们前面讲过的很多方法去处理，比如 logistic 回归，深层网络，卷积神经网络，循环神经网络都可以。</p><h2 id="生成对抗网络的数学推导"><a href="#生成对抗网络的数学推导" class="headerlink" title="生成对抗网络的数学推导"></a>生成对抗网络的数学推导</h2><p>引入KL divergence：</p><script type="math/tex; mode=display">DKL(P\Vert Q)=\int_{-\infty}^{\infty}p(x)\log\frac{p(x)}{q(x)}dx</script><script type="math/tex; mode=display">DKL(P\Vert Q)=\sum_{-\infty}^{\infty}p(i)\log\frac{p(i)}{q(i)}</script><p>其可用于衡量两个分布的差异程度，KL divergence越小，两种概率分布越接近。</p><p>生成对抗网络想要将一个随机高斯噪声z通过生成网络G得到一个和真实数据分布$p_{data}(x)$差不多的分布$p_G(x;\theta)$,其中$\theta$为网络参数。</p><p>从真实数据分布$p_{data}(x)$中采样m个点${x^1,x^2,…,x^m}$,那么生成数据中与这m个点相同的似然为：</p><script type="math/tex; mode=display">L=\prod_{i=1}^mp_G(x^i;\theta)</script><p>那么我们所需网络参数$ \theta $可由最大化以上似然函数求得：</p><script type="math/tex; mode=display">\begin{aligned}\theta^*&=\arg_\theta max\prod p_G(x^i;\theta)\\\\\log \theta^*&=\arg_\theta max\log\prod p_G(x^i;\theta)\\\\&=\arg_\theta max \sum \log(p_G(x^i;\theta))\\\\&\approx\arg_\theta maxE_{x\sim p_{data}}\log(p_G(x;\theta))\\\\&\Leftrightarrow \arg_\theta max \int_xp_{data}(x)\log\frac{p_{G}(x;\theta)}{p_{data}(x)}\\\\&=\arg_\theta\min DKL(p_{data}(x)\Vert p_G(x;\theta))\end{aligned}</script><p>但是我们如何求得$p_G(x;\theta)$呢？我们只知道生成器的先验分布，这里很难通过极大化似然估计求得$\theta$</p><p>有没有办法来取代极大似然估计呢？答案是有的，首先定义如下函数：</p><script type="math/tex; mode=display">V(G,D)=E_{x\sim P_{data}}[\log D(X)]+E_{x\sim P_{G}}[\log (1-D(X))]</script><p>其中D为对抗模型参数，先给出我们求G的公式：</p><script type="math/tex; mode=display">G^*=\arg \min_G\max_DV(G,D)</script><p>怎么得到这个式子的呢？请往下看：</p><script type="math/tex; mode=display">\begin{aligned}V(G,D)&=E_{x\sim P_{data}}[\log D(X)]+E_{x\sim P_{G}}[\log (1-D(X))]\\\\&=\int_x P_{data}(x)\log D(x)dx+\int_x p_G(x)\log(1-D(x))dx\\\\&=\int_x[P_{data}(x)\log D(x)]+[p_G(x)\log(1-D(x))]dx\end{aligned}</script><p>首先我们固定G，求一个最优的D使得上式最大,看看其代表什么含义吧：</p><script type="math/tex; mode=display">\begin{aligned}f(D)&=[P_{data}(x)\log D(x)]+[p_G(x)\log(1-D(x))]\\\\&=a\log(D)+b\log(1-D)\\\\\frac{df(D)}{dD}&=a\times\frac{1}{D}-b\times \frac{1}{1-D}=0\\\\D^*&=\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}\end{aligned}</script><p>那么将$D^*$带入原式：</p><script type="math/tex; mode=display">\begin{aligned}\max V(G,D)&=V(G,D^*)\\\\&=\int_x P_{data}(x)\log\frac{P_{data}(x)}{P_{data}(x)+P_G(x)} dx+\int_x p_G(x)\log\frac{P_{G}(x)}{P_{data}(x)+P_G(x)}dx\\\\&=\int_x P_{data}(x)\log\frac{\frac{1}{2}P_{data}(x)}{\frac{P_{data}(x)+P_G(x)}{2}} dx+\int_x p_G(x)\log\frac{\frac{1}{2}P_{G}(x)}{\frac{P_{data}(x)+P_G(x)}{2}}dx\\\\&=-2\log2+2JSD(P_{data}(x)\Vert P_G(x))\end{aligned}</script><p>其中：</p><script type="math/tex; mode=display">JSD(P\Vert Q)=\frac{1}{2}DKL(P\Vert M)+\frac{1}{2}DKL(Q\Vert M),M=\frac{1}{2}(P+Q)</script><p>由<script type="math/tex">D^*</script>所获的<script type="math/tex">maxV(G,D^*)</script>包含两个KL divergence，那我们所求<script type="math/tex">G^*</script>就得使其最小</p><p>：</p><script type="math/tex; mode=display">G^*=\arg \min_G\max_DV(G,D)</script><h2 id="生成对抗网络的代码实现"><a href="#生成对抗网络的代码实现" class="headerlink" title="生成对抗网络的代码实现"></a>生成对抗网络的代码实现</h2><p>下面我们来进行一下代码实现：</p><p>简单的全连接神经网络作为生成和对抗模型：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> imag, nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> tfs</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, sampler</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">10.0</span>, <span class="number">8.0</span>) <span class="comment"># 设置画图的尺寸</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.interpolation&#x27;</span>] = <span class="string">&#x27;nearest&#x27;</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.cmap&#x27;</span>] = <span class="string">&#x27;gray&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_img</span>(<span class="params">x</span>):</span><br><span class="line">    x = tfs.ToTensor()(x)</span><br><span class="line">    <span class="keyword">return</span> (x - <span class="number">0.5</span>) / <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">deprocess_img</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> (x + <span class="number">1.0</span>) / <span class="number">2.0</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChunkSampler</span>(sampler.Sampler): <span class="comment"># 定义一个取样的函数</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Samples elements sequentially from some offset. </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        num_samples: # of desired datapoints</span></span><br><span class="line"><span class="string">        start: offset where we should start selecting from</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_samples, start=<span class="number">0</span></span>):</span><br><span class="line">        self.num_samples = num_samples</span><br><span class="line">        self.start = start</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):<span class="comment">#迭代器</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(<span class="built_in">range</span>(self.start, self.start + self.num_samples))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.num_samples</span><br><span class="line"></span><br><span class="line">parse1=argparse.ArgumentParser()</span><br><span class="line">parse1.add_argument(<span class="string">&#x27;-num_train&#x27;</span>,default=<span class="number">50000</span>,<span class="built_in">help</span>=<span class="string">&#x27;用于训练的图片数目&#x27;</span>)</span><br><span class="line">parse1.add_argument(<span class="string">&#x27;-num_val&#x27;</span>,default=<span class="number">5000</span>,<span class="built_in">help</span>=<span class="string">&#x27;val_set图片数目&#x27;</span>)</span><br><span class="line">parse1.add_argument(<span class="string">&#x27;-noise_dim&#x27;</span>,default=<span class="number">96</span>)</span><br><span class="line">parse1.add_argument(<span class="string">&#x27;-batch_size&#x27;</span>,default=<span class="number">128</span>)</span><br><span class="line">args=parse1.parse_args()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_set = MNIST(<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=preprocess_img)</span><br><span class="line"></span><br><span class="line">train_data = DataLoader(train_set, batch_size=args.batch_size, sampler=ChunkSampler(args.num_train, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">val_set = MNIST(<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=preprocess_img)</span><br><span class="line"></span><br><span class="line">val_data = DataLoader(val_set, batch_size=args.batch_size, sampler=ChunkSampler(args.num_val, args.num_train))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">discriminator</span>():<span class="comment">#对抗模型</span></span><br><span class="line">    net = nn.Sequential(        </span><br><span class="line">            nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),<span class="comment">#f(x) = max($\alpha$ x, x)</span></span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generator</span>(<span class="params">noise_dim=args.noise_dim</span>):<span class="comment">#生成模型</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Linear(noise_dim, <span class="number">1024</span>),</span><br><span class="line">        nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">        nn.Linear(<span class="number">1024</span>, <span class="number">1024</span>),</span><br><span class="line">        nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">        nn.Linear(<span class="number">1024</span>, <span class="number">784</span>),</span><br><span class="line">        nn.Tanh()</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line">bce_loss = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">discriminator_loss</span>(<span class="params">logits_real, logits_fake</span>): <span class="comment"># 判别器的 loss由两部分组成：真实图片的loss，生成器生成的假图片的loss</span></span><br><span class="line">    size = logits_real.shape[<span class="number">0</span>]</span><br><span class="line">    true_labels = Variable(torch.ones(size, <span class="number">1</span>)).<span class="built_in">float</span>().cuda()</span><br><span class="line">    false_labels = Variable(torch.zeros(size, <span class="number">1</span>)).<span class="built_in">float</span>().cuda()</span><br><span class="line">    loss = bce_loss(logits_real, true_labels) + bce_loss(logits_fake, false_labels)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generator_loss</span>(<span class="params">logits_fake</span>): <span class="comment"># 生成器的 loss  </span></span><br><span class="line">    size = logits_fake.shape[<span class="number">0</span>]</span><br><span class="line">    true_labels = Variable(torch.ones(size, <span class="number">1</span>)).<span class="built_in">float</span>().cuda()</span><br><span class="line">    loss = bce_loss(logits_fake, true_labels)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"><span class="comment"># 使用 adam 来进行训练，学习率是 3e-4, beta1 是 0.5, beta2 是 0.999</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_optimizer</span>(<span class="params">net</span>):</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">3e-4</span>, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">    <span class="keyword">return</span> optimizer</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_a_gan</span>(<span class="params">D_net, G_net, D_optimizer, G_optimizer, discriminator_loss, generator_loss, show_every=<span class="number">500</span>, </span></span><br><span class="line"><span class="params">                noise_size=<span class="number">96</span>, num_epochs=<span class="number">10</span></span>):</span><br><span class="line">    iter_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> x, _ <span class="keyword">in</span> train_data:</span><br><span class="line">            bs = x.shape[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 判别网络</span></span><br><span class="line">            real_data = Variable(x).view(bs, -<span class="number">1</span>).cuda() <span class="comment"># 真实数据</span></span><br><span class="line">            logits_real = D_net(real_data) <span class="comment"># 判别网络得分</span></span><br><span class="line">            </span><br><span class="line">            sample_noise = (torch.rand(bs, noise_size) - <span class="number">0.5</span>) / <span class="number">0.5</span> <span class="comment"># -1 ~ 1 的均匀分布</span></span><br><span class="line">            g_fake_seed = Variable(sample_noise).cuda()</span><br><span class="line">            fake_images = G_net(g_fake_seed) <span class="comment"># 生成的假的数据</span></span><br><span class="line">            logits_fake = D_net(fake_images) <span class="comment"># 判别网络得分</span></span><br><span class="line"></span><br><span class="line">            d_total_error = discriminator_loss(logits_real, logits_fake) <span class="comment"># 判别器的 loss</span></span><br><span class="line">            D_optimizer.zero_grad()</span><br><span class="line">            d_total_error.backward()</span><br><span class="line">            D_optimizer.step() <span class="comment"># 优化判别网络</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 生成网络</span></span><br><span class="line">            g_fake_seed = Variable(sample_noise).cuda()</span><br><span class="line">            fake_images = G_net(g_fake_seed) <span class="comment"># 生成的假的数据</span></span><br><span class="line"></span><br><span class="line">            gen_logits_fake = D_net(fake_images)</span><br><span class="line">            g_error = generator_loss(gen_logits_fake) <span class="comment"># 生成网络的 loss</span></span><br><span class="line">            G_optimizer.zero_grad()</span><br><span class="line">            g_error.backward()</span><br><span class="line">            G_optimizer.step() <span class="comment"># 优化生成网络</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (iter_count % show_every == <span class="number">0</span>):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Iter: &#123;&#125;, D: &#123;:.4&#125;, G:&#123;:.4&#125;&#x27;</span>.<span class="built_in">format</span>(iter_count, d_total_error.item(), g_error.item()))</span><br><span class="line">                imgs_numpy = deprocess_img(fake_images.data.cpu())</span><br><span class="line">                <span class="built_in">print</span>(imgs_numpy.shape)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;./gan_img&#x27;</span>):</span><br><span class="line">                    os.mkdir(<span class="string">&#x27;./gan_img&#x27;</span>)</span><br><span class="line">                save_image(imgs_numpy.reshape(<span class="number">128</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)[<span class="number">0</span>:<span class="number">16</span>,:,:],<span class="string">&#x27;./gan_img/image_&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(iter_count + <span class="number">1</span>))</span><br><span class="line">            iter_count += <span class="number">1</span></span><br><span class="line">D = discriminator().cuda()</span><br><span class="line">G = generator().cuda()</span><br><span class="line"></span><br><span class="line">D_optim = get_optimizer(D)</span><br><span class="line">G_optim = get_optimizer(G)</span><br><span class="line"></span><br><span class="line">train_a_gan(D, G, D_optim, G_optim, discriminator_loss, generator_loss)</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/27/8jYfX.png" alt="8jYfX.png"></p><p><img src="https://s1.328888.xyz/2022/04/27/8jcFC.png" alt="8jcFC.png"></p><p><img src="https://s1.328888.xyz/2022/04/27/8jwGg.png" alt="8jwGg.png"></p><p><img src="https://s1.328888.xyz/2022/04/27/8j3l1.png" alt="8j3l1.png"></p><p><img src="https://s1.328888.xyz/2022/04/27/8jKOt.png" alt="8jKOt.png"></p><p><img src="https://s1.328888.xyz/2022/04/27/8jOxe.png" alt="8jOxe.png"></p><p>上述为该网络经过训练生成的图片，可以看到已经接近手写数字，但效果还不是很好。都说CNN是处理图片最好的网络，那么我们能不能使用CNN作为生成和对抗模型呢？答案当然是可以的，这样的网络被称为 <strong>DC GANs</strong>，那效果又如何呢？</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> imag, nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> tfs</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, sampler</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">10.0</span>, <span class="number">8.0</span>) <span class="comment"># 设置画图的尺寸</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.interpolation&#x27;</span>] = <span class="string">&#x27;nearest&#x27;</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.cmap&#x27;</span>] = <span class="string">&#x27;gray&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_img</span>(<span class="params">x</span>):</span><br><span class="line">    x = tfs.ToTensor()(x)</span><br><span class="line">    <span class="keyword">return</span> (x - <span class="number">0.5</span>) / <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">deprocess_img</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> (x + <span class="number">1.0</span>) / <span class="number">2.0</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChunkSampler</span>(sampler.Sampler): <span class="comment"># 定义一个取样的函数</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Samples elements sequentially from some offset. </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        num_samples: # of desired datapoints</span></span><br><span class="line"><span class="string">        start: offset where we should start selecting from</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_samples, start=<span class="number">0</span></span>):</span><br><span class="line">        self.num_samples = num_samples</span><br><span class="line">        self.start = start</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):<span class="comment">#迭代器</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(<span class="built_in">range</span>(self.start, self.start + self.num_samples))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.num_samples</span><br><span class="line"></span><br><span class="line">parse1=argparse.ArgumentParser()</span><br><span class="line">parse1.add_argument(<span class="string">&#x27;-num_train&#x27;</span>,default=<span class="number">50000</span>,<span class="built_in">help</span>=<span class="string">&#x27;用于训练的图片数目&#x27;</span>)</span><br><span class="line">parse1.add_argument(<span class="string">&#x27;-num_val&#x27;</span>,default=<span class="number">5000</span>,<span class="built_in">help</span>=<span class="string">&#x27;val_set图片数目&#x27;</span>)</span><br><span class="line">parse1.add_argument(<span class="string">&#x27;-noise_dim&#x27;</span>,default=<span class="number">96</span>)</span><br><span class="line">parse1.add_argument(<span class="string">&#x27;-batch_size&#x27;</span>,default=<span class="number">128</span>)</span><br><span class="line">args=parse1.parse_args()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_set = MNIST(<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=preprocess_img)</span><br><span class="line"></span><br><span class="line">train_data = DataLoader(train_set, batch_size=args.batch_size, sampler=ChunkSampler(args.num_train, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">val_set = MNIST(<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=preprocess_img)</span><br><span class="line"></span><br><span class="line">val_data = DataLoader(val_set, batch_size=args.batch_size, sampler=ChunkSampler(args.num_val, args.num_train))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">build_dc_classifier</span>(nn.Module):<span class="comment">#对抗模型</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(build_dc_classifier, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.01</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.01</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.01</span>),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">build_dc_generator</span>(nn.Module): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, noise_dim=args.noise_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(build_dc_generator, self).__init__()</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(noise_dim, <span class="number">1024</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">1024</span>),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">7</span> * <span class="number">7</span> * <span class="number">128</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>, <span class="number">64</span>, <span class="number">4</span>, <span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">64</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>], <span class="number">128</span>, <span class="number">7</span>, <span class="number">7</span>) <span class="comment"># reshape 通道是 128，大小是 7x7</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">bce_loss = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">discriminator_loss</span>(<span class="params">logits_real, logits_fake</span>): <span class="comment"># 判别器的 loss由两部分组成：真实图片的loss，生成器生成的假图片的loss</span></span><br><span class="line">    size = logits_real.shape[<span class="number">0</span>]</span><br><span class="line">    true_labels = Variable(torch.ones(size, <span class="number">1</span>)).<span class="built_in">float</span>().cuda()</span><br><span class="line">    false_labels = Variable(torch.zeros(size, <span class="number">1</span>)).<span class="built_in">float</span>().cuda()</span><br><span class="line">    loss = bce_loss(logits_real, true_labels) + bce_loss(logits_fake, false_labels)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generator_loss</span>(<span class="params">logits_fake</span>): <span class="comment"># 生成器的 loss  </span></span><br><span class="line">    size = logits_fake.shape[<span class="number">0</span>]</span><br><span class="line">    true_labels = Variable(torch.ones(size, <span class="number">1</span>)).<span class="built_in">float</span>().cuda()</span><br><span class="line">    loss = bce_loss(logits_fake, true_labels)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"><span class="comment"># 使用 adam 来进行训练，学习率是 3e-4, beta1 是 0.5, beta2 是 0.999</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_optimizer</span>(<span class="params">net</span>):</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">3e-4</span>, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">    <span class="keyword">return</span> optimizer</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_a_gan</span>(<span class="params">D_net, G_net, D_optimizer, G_optimizer, discriminator_loss, generator_loss, show_every=<span class="number">500</span>, </span></span><br><span class="line"><span class="params">                noise_size=<span class="number">96</span>, num_epochs=<span class="number">10</span></span>):</span><br><span class="line">    iter_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> x, _ <span class="keyword">in</span> train_data:</span><br><span class="line">            bs = x.shape[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 判别网络</span></span><br><span class="line">            real_data = Variable(x).cuda() <span class="comment"># 真实数据</span></span><br><span class="line">            logits_real = D_net(real_data) <span class="comment"># 判别网络得分</span></span><br><span class="line">            </span><br><span class="line">            sample_noise = (torch.rand(bs, noise_size) - <span class="number">0.5</span>) / <span class="number">0.5</span> <span class="comment"># -1 ~ 1 的均匀分布</span></span><br><span class="line">            g_fake_seed = Variable(sample_noise).cuda()</span><br><span class="line">            fake_images = G_net(g_fake_seed) <span class="comment"># 生成的假的数据</span></span><br><span class="line">            logits_fake = D_net(fake_images) <span class="comment"># 判别网络得分</span></span><br><span class="line"></span><br><span class="line">            d_total_error = discriminator_loss(logits_real, logits_fake) <span class="comment"># 判别器的 loss</span></span><br><span class="line">            D_optimizer.zero_grad()</span><br><span class="line">            d_total_error.backward()</span><br><span class="line">            D_optimizer.step() <span class="comment"># 优化判别网络</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 生成网络</span></span><br><span class="line">            g_fake_seed = Variable(sample_noise).cuda()</span><br><span class="line">            fake_images = G_net(g_fake_seed) <span class="comment"># 生成的假的数据</span></span><br><span class="line"></span><br><span class="line">            gen_logits_fake = D_net(fake_images)</span><br><span class="line">            g_error = generator_loss(gen_logits_fake) <span class="comment"># 生成网络的 loss</span></span><br><span class="line">            G_optimizer.zero_grad()</span><br><span class="line">            g_error.backward()</span><br><span class="line">            G_optimizer.step() <span class="comment"># 优化生成网络</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (iter_count % show_every == <span class="number">0</span>):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Iter: &#123;&#125;, D: &#123;:.4&#125;, G:&#123;:.4&#125;&#x27;</span>.<span class="built_in">format</span>(iter_count, d_total_error.item(), g_error.item()))</span><br><span class="line">                imgs_numpy = deprocess_img(fake_images.data.cpu())</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;./gan_img&#x27;</span>):</span><br><span class="line">                    os.mkdir(<span class="string">&#x27;./gan_img&#x27;</span>)</span><br><span class="line">                save_image(imgs_numpy.reshape(<span class="number">128</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)[<span class="number">0</span>:<span class="number">16</span>,:,:],<span class="string">&#x27;./gan_img/image_&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(iter_count + <span class="number">1</span>))</span><br><span class="line">            iter_count += <span class="number">1</span></span><br><span class="line">D = build_dc_classifier().cuda()</span><br><span class="line">G = build_dc_generator().cuda()</span><br><span class="line"></span><br><span class="line">D_optim = get_optimizer(D)</span><br><span class="line">G_optim = get_optimizer(G)</span><br><span class="line"></span><br><span class="line">train_a_gan(D, G, D_optim, G_optim, discriminator_loss, generator_loss)</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/27/8jZFS.png" alt="8jZFS.png"></p><p><img src="https://s1.328888.xyz/2022/04/27/8jvGR.png" alt="8jvGR.png"></p><p><img src="https://s1.328888.xyz/2022/04/27/8j6ti.png" alt="8j6ti.png"></p><p><img src="https://s1.328888.xyz/2022/04/27/8jMOv.png" alt="8jMOv.png"></p><p><img src="https://s1.328888.xyz/2022/04/27/8jNx0.png" alt="8jNx0.png"></p><p><img src="https://s1.328888.xyz/2022/04/27/8jo4J.png" alt="8jo4J.png"></p><p>可以看到，生成图像十分清晰，比全连接神经网络效果好很多</p><p><strong>Least Squares GAN</strong></p><p><a href="https://arxiv.org/abs/1611.04076">Least Squares GAN</a> 比最原始的 GANs 的 loss 更加稳定，通过名字我们也能够看出这种 GAN 是通过最小平方误差来进行估计，而不是通过二分类的损失函数，下面我们看看 loss 的计算公式</p><script type="math/tex; mode=display">\ell_G  =  \frac{1}{2}\mathbb{E}_{z \sim p(z)}\left[\left(D(G(z))-1\right)^2\right]</script><script type="math/tex; mode=display">\ell_D = \frac{1}{2}\mathbb{E}_{x \sim p_\text{data}}\left[\left(D(x)-1\right)^2\right] + \frac{1}{2}\mathbb{E}_{z \sim p(z)}\left[ \left(D(G(z))\right)^2\right]</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ls_discriminator_loss</span>(<span class="params">scores_real, scores_fake</span>):</span><br><span class="line">    loss = <span class="number">0.5</span> * ((scores_real - <span class="number">1</span>) ** <span class="number">2</span>).mean() + <span class="number">0.5</span> * (scores_fake ** <span class="number">2</span>).mean()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ls_generator_loss</span>(<span class="params">scores_fake</span>):</span><br><span class="line">    loss = <span class="number">0.5</span> * ((scores_fake - <span class="number">1</span>) ** <span class="number">2</span>).mean()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/27/8jENW.png" alt="8jENW.png"></p><p>清晰度更高，效果有所提升。</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 生成对抗网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> GANs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生成模型</title>
      <link href="/2022/04/27/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
      <url>/2022/04/27/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1>生成模型</h1><p>生成模型的概念属于概率统计和机器学习，是指一系列用于随机生成可观测数据的模型。简而言之，我们就是要生成的样本和实际的样本尽可能的相似，其主要功能有两个：</p><ul><li>学习一个概率分布</li><li>生成数据</li></ul><h2 id="自动编码器">自动编码器</h2><p>自动编码器最开始是一种数据压缩的方法，其特点有：</p><ul><li>跟数据的相关程度很高：这意味着自动编码器只能压缩与训练数据相似的数据</li><li>有损压缩</li></ul><p>现在自动编码器主要应用于：</p><ul><li>数据去噪</li><li>可视化降维</li><li>生成数据</li></ul><p><img src="https://s1.328888.xyz/2022/04/26/8CzFi.png" alt="8CzFi.png"></p><p>如图所示自动编码器结构，其主要由两部分组成：编码器（Encoder）和解码器（Decoder），编码器和解码器都可以是任意的模型，主要使用神经网络模型。输入数据经过神经网络降维到一个编码，接着又通过另外一个神经网络去解码得到一个与输入数据一模一样的生成数据。然后通过比较这两个数据，通过最小化他们之间的差异来训练网络中的参数。</p><p>训练完成后，拿出解码器，随机传入一个编码，就能生成一个和原始数据差不多的生成数据。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> tfs</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"></span><br><span class="line">im_tfs = tfs.Compose([</span><br><span class="line">    tfs.ToTensor(),</span><br><span class="line">    tfs.Normalize(<span class="number">0.5</span>,<span class="number">0.5</span>) <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_set = MNIST(<span class="string">&#x27;./data&#x27;</span>, transform=im_tfs,download=<span class="literal">True</span>)</span><br><span class="line">train_data = DataLoader(train_set, batch_size=<span class="number">128</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">autoencoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(autoencoder, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.encoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">12</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">12</span>, <span class="number">3</span>) <span class="comment"># 输出的 code 是 3 维，便于可视化</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">3</span>, <span class="number">12</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">12</span>, <span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">28</span>*<span class="number">28</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        encode = self.encoder(x)</span><br><span class="line">        decode = self.decoder(encode)</span><br><span class="line">        <span class="keyword">return</span> encode, decode</span><br><span class="line"></span><br><span class="line">net = autoencoder().cuda()</span><br><span class="line">x = Variable(torch.randn(<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)).cuda() <span class="comment"># batch size 是 1</span></span><br><span class="line">code, _ = net(x)</span><br><span class="line"><span class="built_in">print</span>(code.shape)</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">to_img</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    定义一个函数将最后的结果转换回图片</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    x = <span class="number">0.5</span> * (x + <span class="number">1.</span>)</span><br><span class="line">    x = x.clamp(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    x = x.view(x.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 开始训练自动编码器</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_data:</span><br><span class="line">        im,_=batch</span><br><span class="line">        im = im.view(im.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        im = Variable(im).cuda()</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        _, output = net(im)</span><br><span class="line">        loss = criterion(output, im) / im.shape[<span class="number">0</span>] <span class="comment"># 平均</span></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#123;&#125;, Loss: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>, loss.item()))</span><br><span class="line">    <span class="keyword">if</span> (e+<span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>: <span class="comment"># 每 20 次，将生成的图片保存一下</span></span><br><span class="line">        </span><br><span class="line">        pic = to_img(output.cpu().data)</span><br><span class="line">        <span class="built_in">print</span>(pic.shape)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;./simple_autoencoder&#x27;</span>):</span><br><span class="line">            os.mkdir(<span class="string">&#x27;./simple_autoencoder&#x27;</span>)</span><br><span class="line">        save_image(pic, <span class="string">&#x27;./simple_autoencoder/image_&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line">view_data = Variable((train_set.train_data[:<span class="number">200</span>].<span class="built_in">type</span>(torch.FloatTensor).view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>) / <span class="number">255.</span> - <span class="number">0.5</span>) / <span class="number">0.5</span>).cuda()</span><br><span class="line">encode, _ = net(view_data)    <span class="comment"># 提取压缩的特征值</span></span><br><span class="line">encode=encode.cpu()</span><br><span class="line">fig = plt.figure(<span class="number">2</span>)</span><br><span class="line">ax = Axes3D(fig)    <span class="comment"># 3D 图</span></span><br><span class="line"><span class="comment"># x, y, z 的数据值</span></span><br><span class="line">X = encode.data[:, <span class="number">0</span>].numpy()</span><br><span class="line">Y = encode.data[:, <span class="number">1</span>].numpy()</span><br><span class="line">Z = encode.data[:, <span class="number">2</span>].numpy()</span><br><span class="line">values = train_set.train_labels[:<span class="number">200</span>].numpy()  <span class="comment"># 标签值</span></span><br><span class="line"><span class="keyword">for</span> x, y, z, s <span class="keyword">in</span> <span class="built_in">zip</span>(X, Y, Z, values):</span><br><span class="line">    c = cm.rainbow(<span class="built_in">int</span>(<span class="number">255</span>*s/<span class="number">9</span>))    <span class="comment"># 上色</span></span><br><span class="line">    ax.text(x, y, z, s, backgroundcolor=c)  <span class="comment"># 标位子</span></span><br><span class="line">ax.set_xlim(X.<span class="built_in">min</span>(), X.<span class="built_in">max</span>())</span><br><span class="line">ax.set_ylim(Y.<span class="built_in">min</span>(), Y.<span class="built_in">max</span>())</span><br><span class="line">ax.set_zlim(Z.<span class="built_in">min</span>(), Z.<span class="built_in">max</span>())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">code = Variable(torch.FloatTensor([[<span class="number">1.19</span>, -<span class="number">3.36</span>, <span class="number">2.06</span>]])).cuda() <span class="comment"># 给一个 code 是 (1.19, -3.36, 2.06)</span></span><br><span class="line">decode = net.decoder(code).cpu()</span><br><span class="line">decode_img = to_img(decode).squeeze()</span><br><span class="line">decode_img = decode_img.data.numpy() * <span class="number">255</span></span><br><span class="line">plt.figure(<span class="number">3</span>)</span><br><span class="line">plt.imshow(decode_img.astype(<span class="string">&#x27;uint8&#x27;</span>), cmap=<span class="string">&#x27;gray&#x27;</span>) <span class="comment"># 生成图片 3</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># class conv_autoencoder(nn.Module):</span></span><br><span class="line"><span class="comment">#     def __init__(self):</span></span><br><span class="line"><span class="comment">#         super(conv_autoencoder, self).__init__()</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">#         self.encoder = nn.Sequential(</span></span><br><span class="line"><span class="comment">#             nn.Conv2d(1, 16, 3, stride=3, padding=1),  # (b, 16, 10, 10)</span></span><br><span class="line"><span class="comment">#             nn.ReLU(True),</span></span><br><span class="line"><span class="comment">#             nn.MaxPool2d(2, stride=2),  # (b, 16, 5, 5)</span></span><br><span class="line"><span class="comment">#             nn.Conv2d(16, 8, 3, stride=2, padding=1),  # (b, 8, 3, 3)</span></span><br><span class="line"><span class="comment">#             nn.ReLU(True),</span></span><br><span class="line"><span class="comment">#             nn.MaxPool2d(2, stride=1)  # (b, 8, 2, 2)</span></span><br><span class="line"><span class="comment">#         )</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">#         self.decoder = nn.Sequential(</span></span><br><span class="line"><span class="comment">#             nn.ConvTranspose2d(8, 16, 3, stride=2),  # (b, 16, 5, 5)</span></span><br><span class="line"><span class="comment">#             nn.ReLU(True),</span></span><br><span class="line"><span class="comment">#             nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # (b, 8, 15, 15)</span></span><br><span class="line"><span class="comment">#             nn.ReLU(True),</span></span><br><span class="line"><span class="comment">#             nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # (b, 1, 28, 28)</span></span><br><span class="line"><span class="comment">#             nn.Tanh()</span></span><br><span class="line"><span class="comment">#         )</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     def forward(self, x):</span></span><br><span class="line"><span class="comment">#         encode = self.encoder(x)</span></span><br><span class="line"><span class="comment">#         decode = self.decoder(encode)</span></span><br><span class="line"><span class="comment">#         return encode, decode</span></span><br><span class="line"><span class="comment"># conv_net = conv_autoencoder()</span></span><br><span class="line"><span class="comment"># if torch.cuda.is_available():</span></span><br><span class="line"><span class="comment">#     conv_net = conv_net.cuda()</span></span><br><span class="line"><span class="comment"># optimizer = torch.optim.Adam(conv_net.parameters(), lr=1e-3, weight_decay=1e-5)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 开始训练自动编码器</span></span><br><span class="line"><span class="comment"># for e in range(40):</span></span><br><span class="line"><span class="comment">#     for im, _ in train_data:</span></span><br><span class="line"><span class="comment">#         if torch.cuda.is_available():</span></span><br><span class="line"><span class="comment">#             im = im.cuda()</span></span><br><span class="line"><span class="comment">#         im = Variable(im)</span></span><br><span class="line"><span class="comment">#         # 前向传播</span></span><br><span class="line"><span class="comment">#         _, output = conv_net(im)</span></span><br><span class="line"><span class="comment">#         loss = criterion(output, im) / im.shape[0] # 平均</span></span><br><span class="line"><span class="comment">#         # 反向传播</span></span><br><span class="line"><span class="comment">#         optimizer.zero_grad()</span></span><br><span class="line"><span class="comment">#         loss.backward()</span></span><br><span class="line"><span class="comment">#         optimizer.step()</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#     if (e+1) % 20 == 0: # 每 20 次，将生成的图片保存一下</span></span><br><span class="line"><span class="comment">#         print(&#x27;epoch: &#123;&#125;, Loss: &#123;:.4f&#125;&#x27;.format(e+1, loss.data[0]))</span></span><br><span class="line"><span class="comment">#         pic = to_img(output.cpu().data)</span></span><br><span class="line"><span class="comment">#         if not os.path.exists(&#x27;./conv_autoencoder&#x27;):</span></span><br><span class="line"><span class="comment">#             os.mkdir(&#x27;./conv_autoencoder&#x27;)</span></span><br><span class="line"><span class="comment">#         save_image(pic, &#x27;./conv_autoencoder/image_&#123;&#125;.png&#x27;.format(e+1))</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/26/8pM2y.png" alt="8pM2y.png"></p><p><img src="https://s1.328888.xyz/2022/04/26/8pSjd.png" alt="8pSjd.png"></p><p><img src="https://s1.328888.xyz/2022/04/26/8pBJQ.png" alt="8pBJQ.png"></p><p>如图二所示，不同数字通过自动编码器所得编码大概聚集在一起，且通过解编码所得图三大概也能看出是5.</p><h2 id="变分自动编码">变分自动编码</h2><p>变分编码器是自动编码器的升级版本，其结构跟自动编码器是类似的，也由编码器和解码器构成。</p><p>自动编码器有个问题，就是并不能任意生成图片，因为我们没有办法自己去编码向量，需要通过一张图片输入编码我们才知道得到的编码向量是什么，这时我们就可以通过变分自动编码器来解决这个问题。</p><p>其实原理特别简单，只需要在编码过程给它增加一些限制，迫使其生成的编码向量能够粗略的遵循一个标准正态分布，这就是其与一般的自动编码器最大的不同。</p><p>这样我们生成一张新图片就很简单了，我们只需要给它一个标准正态分布的随机隐含向量，这样通过解码器就能够生成我们想要的图片，而不需要给它一张原始图片先编码。</p><p>综上所述，就是让编码都服从一个分布，这样我们就可以任意取服从该分布的编码。</p><p>一般来讲，我们通过 encoder 得到的隐含向量并不是一个标准的正态分布，为了衡量两种分布的相似程度，我们使用 KL divergence，利用其来表示隐含向量与标准正态分布之间差异的 loss，另外一个 loss 仍然使用生成图片与原图片的均方误差来表示。</p><h3 id="KL-divergence-的公式如下">KL divergence 的公式如下</h3><p>$$<br>D{KL} (P || Q) =  \sum_{-\infty}^{\infty} p(i) \log \frac{p(i)}{q(i)} dx<br>$$</p><p>$$<br>D{KL} (P || Q) =  \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx<br>$$</p><h3 id="重参数">重参数</h3><p>很明显，对于变分自动编码器的输出向量做积分或求和是十分麻烦的，我们不如直接生成两个向量：一个表示均值，一个表示标准差，那么我们所需要的编码向量直接用一个标准正态分布先乘上标准差再加上均值就行了。这时候尽量让均值接近0，标准差接近1，也就不需要再去计算KL divergence。<br>$$<br>\begin{aligned}<br>DKL(P\Vert Q)&amp;=\int_{-\infty}^{\infty}p(x)\log\frac{p(x)}{q(x)}dx\\<br>&amp;=\int_{-\infty}^{\infty}p(x)\log p(x)-p(x)\log q(x)dx<br>\end{aligned}<br>$$</p><p>$$<br>\begin{aligned}<br>\int_{-\infty}^{\infty}p(x)\log p(x)dx&amp;=\int_{-\infty}^{\infty}p(x)\log (\frac{1}{\sqrt{2\pi}\sigma_1}\exp(-\frac{(x-\mu_1)^2}{2\sigma_1^2}))dx\\<br>&amp;=-\frac{1}{2}\log(2\pi\sigma_1^2)\int p(x)dx<br>+\int p(x)(-\frac{(x-\mu_1)^2}{2\sigma_1^2})dx\\<br>&amp;=-\frac{1}{2}\log(2\pi\sigma_1^2)-\frac{\int p(x)x^2dx-\int p(x)2x\mu_1dx+\int p(x)\mu_1^2dx}{2\sigma_1^2}\\<br>&amp;=-\frac{1}{2}\log(2\pi\sigma_1^2)-\frac{(\mu_1^2+\sigma_q^2)-(2\mu_1\times \mu_1)+u_1^2}{2\sigma_1^2}\\<br>&amp;=-\frac{1}{2}[1+\log(2\pi \sigma_1^2)]<br>\end{aligned}<br>$$</p><p>$$<br>\begin{aligned}<br>\int_{-\infty}^{\infty}p(x)\log q(x)dx&amp;=\int_{-\infty}^{\infty}p(x)\log (\frac{1}{\sqrt{2\pi}\sigma_2}\exp(-\frac{(x-\mu_2)^2}{2\sigma_2^2}))dx\\<br>&amp;=-\frac{1}{2}\log(2\pi\sigma_2^2)\int p(x)dx<br>+\int p(x)(-\frac{(x-\mu_2)^2}{2\sigma_1^2})dx\\<br>&amp;=-\frac{1}{2}\log(2\pi\sigma_2^2)-\frac{\int p(x)x^2dx-\int p(x)2x\mu_2dx+\int p(x)\mu_2^2dx}{2\sigma_2^2}\\<br>&amp;=-\frac{1}{2}\log(2\pi\sigma_2^2)-\frac{(\mu_1^2+\sigma12^2)-(2\mu_2\times \mu_1)+u_2^2}{2\sigma_2^2}\\<br>&amp;=-\frac{1}{2}\log(2\pi \sigma_2^2)-\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}<br>\end{aligned}<br>$$</p><p>$$<br>\begin{aligned}<br>D{KL} (P || Q)&amp;=-\frac{1}{2}[1+\log(2\pi \sigma_1^2)]-[-\frac{1}{2}\log(2\pi \sigma_2^2)-\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}]\\<br>&amp;=\log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}<br>\end{aligned}<br>$$</p><p><img src="https://s1.328888.xyz/2022/04/26/80uoM.png" alt="80uoM.png"></p><p>那么我们的总体loss就等于$MSE+KLD$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reconstruction_function=nn.MSELoss()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_function</span>(<span class="params">recon_x, x, mu, logvar</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    recon_x: generating images</span></span><br><span class="line"><span class="string">    x: origin images</span></span><br><span class="line"><span class="string">    mu: latent mean</span></span><br><span class="line"><span class="string">    logvar: latent log variance</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    MSE = reconstruction_function(recon_x, x)</span><br><span class="line">    <span class="comment"># kld = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)</span></span><br><span class="line">    KLD_element = mu.<span class="built_in">pow</span>(<span class="number">2</span>).add_(logvar.exp()).mul_(-<span class="number">1</span>).add_(<span class="number">1</span>).add_(logvar)</span><br><span class="line">    KLD = torch.<span class="built_in">sum</span>(KLD_element).mul_(-<span class="number">0.5</span>)</span><br><span class="line">    <span class="comment"># KL divergence</span></span><br><span class="line">    <span class="keyword">return</span> MSE + KLD</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> tfs</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line">im_tfs = tfs.Compose([</span><br><span class="line">    tfs.ToTensor(),</span><br><span class="line">    tfs.Normalize(<span class="number">0.5</span>,<span class="number">0.5</span>) <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_set = MNIST(<span class="string">&#x27;./data&#x27;</span>, transform=im_tfs)</span><br><span class="line">train_data = DataLoader(train_set, batch_size=<span class="number">128</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VAE</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(VAE, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">784</span>, <span class="number">400</span>)</span><br><span class="line">        self.fc21 = nn.Linear(<span class="number">400</span>, <span class="number">20</span>) <span class="comment"># mean</span></span><br><span class="line">        self.fc22 = nn.Linear(<span class="number">400</span>, <span class="number">20</span>) <span class="comment"># var</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">20</span>, <span class="number">400</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">400</span>, <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        h1 = F.relu(self.fc1(x))</span><br><span class="line">        <span class="keyword">return</span> self.fc21(h1), self.fc22(h1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reparametrize</span>(<span class="params">self, mu, logvar</span>):</span><br><span class="line">        std = logvar.mul(<span class="number">0.5</span>).exp_()</span><br><span class="line">        eps = torch.FloatTensor(std.size()).normal_()</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            eps = Variable(eps.cuda())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            eps = Variable(eps)</span><br><span class="line">        <span class="keyword">return</span> eps.mul(std).add_(mu)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, z</span>):</span><br><span class="line">        h3 = F.relu(self.fc3(z))</span><br><span class="line">        <span class="keyword">return</span> F.tanh(self.fc4(h3))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mu, logvar = self.encode(x) <span class="comment"># 编码</span></span><br><span class="line">        z = self.reparametrize(mu, logvar) <span class="comment"># 重新参数化成正态分布</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(z), mu, logvar <span class="comment"># 解码，同时输出均值方差</span></span><br><span class="line">net = VAE() <span class="comment"># 实例化网络</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    net = net.cuda()</span><br><span class="line">x, _ = train_set[<span class="number">0</span>]</span><br><span class="line">x = x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    x = x.cuda()</span><br><span class="line">x = Variable(x)</span><br><span class="line">_, mu, var = net(x)</span><br><span class="line"><span class="built_in">print</span>(mu)</span><br><span class="line"></span><br><span class="line">reconstruction_function = nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_function</span>(<span class="params">recon_x, x, mu, logvar</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    recon_x: generating images</span></span><br><span class="line"><span class="string">    x: origin images</span></span><br><span class="line"><span class="string">    mu: latent mean</span></span><br><span class="line"><span class="string">    logvar: latent log variance</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    MSE = reconstruction_function(recon_x, x)</span><br><span class="line">    <span class="comment"># loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)</span></span><br><span class="line">    KLD_element = mu.<span class="built_in">pow</span>(<span class="number">2</span>).add_(logvar.exp()).mul_(-<span class="number">1</span>).add_(<span class="number">1</span>).add_(logvar)</span><br><span class="line">    KLD = torch.<span class="built_in">sum</span>(KLD_element).mul_(-<span class="number">0.5</span>)</span><br><span class="line">    <span class="comment"># KL divergence</span></span><br><span class="line">    <span class="keyword">return</span> MSE + KLD</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">to_img</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    定义一个函数将最后的结果转换回图片</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    x = <span class="number">0.5</span> * (x + <span class="number">1.</span>)</span><br><span class="line">    x = x.clamp(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    x = x.view(x.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> im, _ <span class="keyword">in</span> train_data:</span><br><span class="line">        im = im.view(im.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        im = Variable(im)</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            im = im.cuda()</span><br><span class="line">        recon_im, mu, logvar = net(im)</span><br><span class="line">        loss = loss_function(recon_im, im, mu, logvar) / im.shape[<span class="number">0</span>] <span class="comment"># 将 loss 平均</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#123;&#125;, Loss: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>, loss.item()))</span><br><span class="line">    <span class="keyword">if</span> (e + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        </span><br><span class="line">        save = to_img(recon_im.cpu().data)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;./vae_img&#x27;</span>):</span><br><span class="line">            os.mkdir(<span class="string">&#x27;./vae_img&#x27;</span>)</span><br><span class="line">        save_image(save, <span class="string">&#x27;./vae_img/image_&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">x, _ = train_set[<span class="number">0</span>]</span><br><span class="line">x = x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    x = x.cuda()</span><br><span class="line">x = Variable(x)</span><br><span class="line">_, mu, _ = net(x)</span><br><span class="line"><span class="built_in">print</span>(mu)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/26/8YQSJ.png" alt="8YQSJ.png"></p><p>可以明显看出变分自动编码的输出图片比自动编码器输出的图片更清晰，但其仍然使用MESLoss为loss函数，所以生成的图片会有模糊现象。</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 生成对抗网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> GANs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN的pytorch实现</title>
      <link href="/2022/04/27/RNN%E7%9A%84pytorch%E5%AE%9E%E7%8E%B0/"/>
      <url>/2022/04/27/RNN%E7%9A%84pytorch%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="标准RNN">标准RNN</h2><p>可直接调用<code>torch.nn.RNN()</code><br>$$<br>h_t=tanh(w_{ih}*x_t+b_{ih}+w_{hh}*h_{h-1}+b_{hh})<br>$$</p><p>参数：</p><p><code>input_size</code>：输入$x_t$的特征维度。</p><p><code>hidden_size</code>：输出$h_t$的特征维度</p><p><code>layers</code>：网络层数</p><p><code>nonlinearity</code>：非线性激活函数，默认为tanh</p><p><code>bias</code>：是否使用偏执，默认为True</p><p><code>batch_first</code>：是否将输入$x_t(batch,seq,feature)$,默认为False，batch是第二维度</p><p><code>dropout</code>：接受0~1之间的值，会在网络中除了最后一层之外的输出层加上dropout</p><p><code>bidirectional</code>：是否双向，默认为False，如果设置为True，则为双向循环神经网络</p><p>网络的输入：</p><p>网络的输入为一个序列$x_t$和记忆输入$h_0$</p><p>$x_t(seq,batch,feature)$</p><p>$h_0(layers*direction,batch,hidden)$</p><p>direction 表示方向，如果双向RNN则<code>direction=2</code></p><p>hidden 表示输出的特征维度</p><p>网络的输出：</p><p>网络输出$output$和$h_n$:</p><p>$output(seq,batch,hidden*direction)$</p><p>$h_n(layers*direction,batch,hidden)$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">basic_rnn=nn.RNN(input_size=<span class="number">20</span>,hidden_size=<span class="number">50</span>,num_layers=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(basic_rnn.weight_ih_l0.shape)<span class="comment">#查看参数weight_ih_l0\weight_hh_l1\bias_ih_l0</span></span><br><span class="line">input_test=torch.randn(<span class="number">100</span>,<span class="number">32</span>,<span class="number">20</span>)</span><br><span class="line">h_0=torch.randn(<span class="number">2</span>,<span class="number">32</span>,<span class="number">50</span>)</span><br><span class="line">output_test,h_n=basic_rnn(input_test,h_0)</span><br><span class="line"><span class="built_in">print</span>(output_test.shape)</span><br><span class="line"><span class="built_in">print</span>(h_n.shape)</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/24/8LBIX.png" alt="8LBIX.png"></p><h2 id="LSTM">LSTM</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lstm=nn.LSTM(input_size=<span class="number">20</span>,hidden_size=<span class="number">50</span>,num_layers=<span class="number">2</span>)</span><br><span class="line">input_test=torch.randn(<span class="number">100</span>,<span class="number">32</span>,<span class="number">20</span>)</span><br><span class="line">lstm_out,(h_n,c_n)=lstm(input_test)</span><br><span class="line"><span class="built_in">print</span>(lstm.weight_ih_l0.shape)</span><br><span class="line"><span class="built_in">print</span>(lstm_out.shape)</span><br><span class="line"><span class="built_in">print</span>(h_n.shape)</span><br><span class="line"><span class="built_in">print</span>(c_n.shape)</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/24/8XUn1.png" alt="8XUn1.png"></p><p>lstm参数是标准RNN的4倍</p><h2 id="GRU">GRU</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gru=nn.GRU(input_size=<span class="number">20</span>,hidden_size=<span class="number">50</span>,num_layers=<span class="number">2</span>)</span><br><span class="line">input_test=torch.randn(<span class="number">100</span>,<span class="number">32</span>,<span class="number">20</span>)</span><br><span class="line">gru_out,(h_n,c_n)=gru(input_test)</span><br><span class="line"><span class="built_in">print</span>(gru.weight_ih_l0.shape)</span><br><span class="line"><span class="built_in">print</span>(gru_out.shape)</span><br><span class="line"><span class="built_in">print</span>(h_n.shape)</span><br><span class="line"><span class="built_in">print</span>(c_n.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/24/8Xuet.png" alt="8Xuet.png"></p><h2 id="LSTM实现MNIST图片识别">LSTM实现MNIST图片识别</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pickletools <span class="keyword">import</span> optimize</span><br><span class="line"><span class="keyword">from</span> turtle <span class="keyword">import</span> forward</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">import</span> os,sys</span><br><span class="line"><span class="keyword">from</span> visdom <span class="keyword">import</span> Visdom</span><br><span class="line">viz=Visdom()</span><br><span class="line">viz.line([<span class="number">0.</span>],[<span class="number">0.</span>],win=<span class="string">&#x27;train_loss&#x27;</span>,opts=<span class="built_in">dict</span>(title=<span class="string">&#x27;train loss&#x27;</span>))<span class="comment">#创建一条直线（y,x,ID,opt:属性配置）</span></span><br><span class="line">os.chdir(sys.path[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyRnn</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_dim,hidden_dim,n_layer,n_class</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(MyRnn,self).__init__()</span><br><span class="line">        self.n_layer=n_layer</span><br><span class="line">        self.hidden_dim=hidden_dim</span><br><span class="line">        self.lstm=nn.LSTM(in_dim,hidden_dim,n_layer,batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.classifer=nn.Linear(hidden_dim,n_class)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        out,(h_n,c_0)=self.lstm(x)</span><br><span class="line">        out=out[:,-<span class="number">1</span>,:]</span><br><span class="line">        out=self.classifer(out)</span><br><span class="line">        <span class="keyword">return</span>  out</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model=MyRnn(<span class="number">784</span>,<span class="number">50</span>,<span class="number">2</span>,<span class="number">10</span>).cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model=MyRnn(<span class="number">784</span>,<span class="number">50</span>,<span class="number">2</span>,<span class="number">10</span>)</span><br><span class="line">criter=nn.CrossEntropyLoss()</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=<span class="number">1e-2</span>)</span><br><span class="line">epochs=<span class="number">30</span></span><br><span class="line"></span><br><span class="line">data_tf=transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.5</span>],[<span class="number">0.5</span>])]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据集</span></span><br><span class="line">train_data=datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,train=<span class="literal">True</span>,transform=data_tf,download=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">test_data=train_data=datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,train=<span class="literal">False</span>,transform=data_tf)</span><br><span class="line">train_loader=DataLoader(train_data,batch_size=<span class="number">32</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader=DataLoader(test_data,batch_size=<span class="number">32</span>,shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">global_step=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        img,label=batch</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            img=img.cuda()</span><br><span class="line">            label=label.cuda()</span><br><span class="line"></span><br><span class="line">        img=img.reshape(img.size(<span class="number">0</span>),img.size(<span class="number">1</span>),<span class="number">784</span>)  </span><br><span class="line">        global_step+=<span class="number">1</span>  </span><br><span class="line">        output=model(img)</span><br><span class="line">        loss=criter(output,label)</span><br><span class="line">        viz.line([loss.item()],[global_step],win=<span class="string">&#x27;train_loss&#x27;</span>,update=<span class="string">&#x27;append&#x27;</span>)<span class="comment">#将数据添加到直线中（update操作）</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epochs:&#123;&#125;,loss:&#123;:.6f&#125;&quot;</span>.<span class="built_in">format</span>(epoch,loss))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">eval_loss=<span class="number">0</span></span><br><span class="line">eval_acc=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> test_loader:</span><br><span class="line">    img,label=batch</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        img=img.cuda()</span><br><span class="line">        label=label.cuda()</span><br><span class="line">    img=img.reshape(img.size(<span class="number">0</span>),img.size(<span class="number">1</span>),<span class="number">784</span>)  </span><br><span class="line">    out=model(img)</span><br><span class="line">    loss=criter(out,label)</span><br><span class="line"> </span><br><span class="line">    eval_loss+=loss.detach()*label.size(<span class="number">0</span>)</span><br><span class="line">    pred=torch.<span class="built_in">max</span>(out,dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">    num_correct=(pred==label).<span class="built_in">sum</span>()</span><br><span class="line">    eval_acc+=num_correct.detach()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test loss:&#123;&#125;,ACC:&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(eval_loss/<span class="built_in">len</span>(test_data),eval_acc/<span class="built_in">len</span>(test_data)))</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/25/8Hwp2.png" alt="8Hwp2.png"></p><p><img src="https://s1.328888.xyz/2022/04/25/8HOy7.png" alt="8HOy7.png"></p><p>RNN是不舍和处理图片数据的，其一是图片中的信息没有很强的序列关系，其二是RNN必须前面一个数据计算结束才能进行后面一个的数据计算，这对于大图片来说无疑是很慢的。</p><h2 id="RNN适用场景-序列预测">RNN适用场景-序列预测</h2><p>​通过<code>s(k-3),s(k-2),s(k-1),s(k)</code>预测<code>s(k+1)</code>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pickletools <span class="keyword">import</span> optimize</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os,sys</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyRnn</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_dim,hidden_dim,n_layer,n_class</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(MyRnn,self).__init__()</span><br><span class="line">        self.n_layer=n_layer</span><br><span class="line">        self.hidden_dim=hidden_dim</span><br><span class="line">        self.lstm=nn.LSTM(in_dim,hidden_dim,n_layer)</span><br><span class="line">        self.classifer=nn.Linear(hidden_dim,n_class)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        out,(h_n,c_0)=self.lstm(x)</span><br><span class="line">        <span class="comment"># out=out.reshape(out.size(0)*out.size(1),-1)</span></span><br><span class="line">        out=self.classifer(out)</span><br><span class="line">        <span class="keyword">return</span>  out</span><br><span class="line">os.chdir(sys.path[<span class="number">0</span>])</span><br><span class="line">data=pd.read_csv(<span class="string">&#x27;author\code-of-learn-deep-learning-with-pytorch\chapter5_RNN\\time-series\\data.csv&#x27;</span>,usecols=[<span class="number">1</span>])</span><br><span class="line">data1=data.dropna()</span><br><span class="line">data=np.array(data1.iloc[:,<span class="number">0</span>])</span><br><span class="line">data.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#归一化为0~1</span></span><br><span class="line">max_data=<span class="built_in">max</span>(data)</span><br><span class="line">min_data=<span class="built_in">min</span>(data)</span><br><span class="line">data=data/(max_data-min_data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#用前4年预测第五年</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataset</span>(<span class="params">dataset, look_back=<span class="number">2</span></span>):</span><br><span class="line">    dataX, dataY = [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset) - look_back):</span><br><span class="line">        a = dataset[i:(i + look_back)]</span><br><span class="line">        dataX.append(a)</span><br><span class="line">        dataY.append(dataset[i + look_back])</span><br><span class="line">    <span class="keyword">return</span> np.array(dataX), np.array(dataY)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataX,dataY=create_dataset(data,look_back=<span class="number">4</span>)</span><br><span class="line">len_train=<span class="built_in">int</span>(<span class="built_in">len</span>(data)*<span class="number">0.7</span>)</span><br><span class="line">train_data=dataX[:len_train]</span><br><span class="line">train_label=dataY[:len_train]</span><br><span class="line">test_data=dataX[len_train:]</span><br><span class="line">test_label=dataY[len_train:]</span><br><span class="line"></span><br><span class="line">dataX=dataX.reshape(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">train_data=train_data.reshape(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">train_label=train_label.reshape(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">test_data=test_data.reshape(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">dataX=torch.from_numpy(dataX).<span class="built_in">float</span>().cuda()</span><br><span class="line">train_data=torch.from_numpy(train_data).<span class="built_in">float</span>().cuda()</span><br><span class="line">train_label=torch.from_numpy(train_label).<span class="built_in">float</span>().cuda()</span><br><span class="line">test_data=torch.from_numpy(test_data).<span class="built_in">float</span>().cuda()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model=MyRnn(<span class="number">4</span>,<span class="number">50</span>,<span class="number">2</span>,<span class="number">1</span>).cuda()</span><br><span class="line">cri=nn.MSELoss()</span><br><span class="line">optimizer=optim.Adam(model.parameters(),lr=<span class="number">1e-3</span>)</span><br><span class="line">epochs=<span class="number">30</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output=model(train_data)  </span><br><span class="line">    loss=cri(output,train_label)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch:&#123;&#125;,loss:&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch,loss)) </span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">pred=model(dataX)</span><br><span class="line">pred=pred.reshape(-<span class="number">1</span>).cpu().detach().numpy()*(max_data-min_data)</span><br><span class="line"><span class="built_in">print</span>(pred)</span><br><span class="line">plt.plot(pred, <span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;prediction&#x27;</span>)</span><br><span class="line">plt.plot(data1, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;real&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/25/8D2qA.png" alt="8D2qA.png"></p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 循环神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> RNN </tag>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN的更多应用</title>
      <link href="/2022/04/27/RNN%E7%9A%84%E6%9B%B4%E5%A4%9A%E5%BA%94%E7%94%A8/"/>
      <url>/2022/04/27/RNN%E7%9A%84%E6%9B%B4%E5%A4%9A%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h1>RNN的更多应用</h1><h2 id="Many-to-one">Many to one</h2><p>RNN不仅可以输入一个序列，输出一个序列，也可以输入一个序列，输出一个向量，通常是取输出序列的最后一个向量，这样的结构称为Many to one。</p><p>常用这种结构的任务有：</p><ul><li>情感分析，讲一句话作为序列输出网络，输出只取最后一个，更具输出判断这句话的态度是积极的还是消极的</li><li>关键字提取，用最后一个输出表示这句话的关键字</li></ul><h2 id="Many-to-Many-shorter">Many to Many(shorter)</h2><p>输入输出都是序列，但输出的序列比输入的序列短。常用于语音识别，因为一段话用语音表示肯定比这段话更长。</p><p>语音识别中需要使用CTC算法解决重复问题，CTC算法可阅读：<a href="https://zhuanlan.zhihu.com/p/161186907">带你看懂CTC算法 - 知乎 (zhihu.com)</a></p><h2 id="Seq2seq">Seq2seq</h2><p>输入为一个序列输出为一个不定长序列，常用于机器翻译，将一句中文翻译成英文，那么这句英文的长度有可能比中文短。</p><p>该模型还常用于聊天机器人和问答系统，比如：</p><p><img src="https://s1.328888.xyz/2022/04/26/8CGK2.png" alt="8CGK2.png"></p><p>模型将前面的输入作为下一次的输入，这样就能输出任意长的序列。</p><p>除此之外还可以引入注意力机制强化模型效果，注意力机制可参考：<a href="https://www.zhihu.com/question/519290359/answer/2403538047"> 注意力机制到底是什么？ - 知乎 (zhihu.com)</a></p><h2 id="CNN-RNN">CNN+RNN</h2><p>通过预训练的CNN提取图片特征，接着通过RNN将特征变为文字描述。</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 循环神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nlp</title>
      <link href="/2022/04/27/nlp/"/>
      <url>/2022/04/27/nlp/</url>
      
        <content type="html"><![CDATA[<h1>自然语言处理(nlp)</h1><h2 id="词嵌入-word-embedding">词嵌入(word embedding)</h2><p>one-hot:图像分类问题主要使用one-hot编码，比如一共有五类，那么属于第二类的话，他的编码为(0,1,0,0,0),但是在nlp中自然是行不通的，假如一篇文章有2000个词，那么任意一个词的编码表示为(0,0,…,0,1,0,…,0),一个长2000的向量中只有一个1，其余全是0。这未免太稀疏了，且也不能体现词的特性，所以一般对nlp词的编码使用word embedding</p><p>embedding：对于每个词，可以使用一个高维向量去表示，这里的高维向量不再是全为0和1的形式，向量的每一位都是一些实数，而这些实数隐含着这个单词的某种属性。两个词之间的 <strong>相似性</strong>就可以通过 <strong>embedding向量之间的夹角表示</strong>。<br>$$<br>\cos \theta=\frac{\vec a·\vec b}{\vert \vec a \vert \vert \vec b\vert}<br>$$<br>我们举一个例子，下面有 4 段话</p><ol><li><p>The cat likes playing wool.</p></li><li><p>The kitty likes playing wool.</p></li><li><p>The dog likes playing ball.</p></li><li><p>The boy does not like playing ball or wool.</p></li></ol><p>这里面有 4 个词，分别是 cat, kitty, dog 和 boy。下面我们使用一个二维的词向量 (a, b) 来表示每一个词，其中 a，b 分别代表着这个词的一种属性，比如 a 代表是否喜欢玩球，b 代表是否喜欢玩毛线，数值越大表示越喜欢，那么我们就能够用数值来定义每一个单词。</p><p>对于 cat，我们可以定义它的词嵌入为 (-1, 4)，因为他不喜欢玩球，喜欢玩毛线，同时可以定义 kitty 为 (-2, 5)，dog 为 (3, 2) 以及 boy 为 (-2, -3)。</p><p>那么问题来了：如何对词进行embedding编码呢？这个问题可以交给神经网络去做，我们只需要定义我们需要的维度。词嵌入的每个元素表示一种属性，维度较低的时候我们能够推断出每一维度的具体含义，然而维度较高之后，我们并不需要指定每一维度代表什么，因为每个维度都是网络自己学习出来的属性。具体做法是skip-garm模型，该模型是<a href="https://arxiv.org/pdf/1301.3781.pdf">Word2Vec</a>这篇论文的网络架构，skip-gram 模型非常简单，我们在一段文本中训练一个简单的网络，这个网络的任务是通过一个词周围的词来预测这个词，然而我们实际上要做的就是训练我们的词嵌入。</p><p>比如我们给定一句话中的一个词，看看它周围的词，然后随机挑选一个，我们希望网络能够输出一个概率值，这个概率值能够告诉我们到底这个词离我们选择的词的远近程度，比如这么一句话 ‘A dog is playing with a ball’，如果我们选的词是 ‘ball’，那么 ‘playing’ 就要比 ‘dog’ 离我们选择的词更近。</p><p>对于一段话，我们可以按照顺序选择不同的词，然后构建训练样本和 label，比如：</p><p><img src="https://s1.328888.xyz/2022/04/26/85sKM.png" alt="85sKM.png"></p><p>对于这个例子，我们依次取一个词以及其周围的词构成一个训练样本，比如第一次选择的词是 ‘the’，那么我们取其前后两个词作为训练样本，这个也可以被称为一个滑动窗口，对于第一个词，其左边没有单词，所以训练集就是三个词，然后我们在这三个词中选择 ‘the’ 作为输入，另外两个词都是他的输出，就构成了两个训练样本，又比如选择 ‘fox’ 这个词，那么加上其左边两个词，右边两个词，一共是 5 个词，然后选择 ‘fox’ 作为输入，那么输出就是其周围的四个词，一共可以构成 4 个训练样本，通过这个办法，我们就能够训练出需要的词嵌入。</p><p>词嵌入的pytorch实现是通过函数<code>nn.Embedding(m,n)</code>实现的，其中m表示所有的单词数目，n表示词嵌入的维度，其返回一个初始化的embdding矩阵，在RNN训练时会对其不断更新：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">word_to_ix=&#123;<span class="string">&#x27;hello&#x27;</span>:<span class="number">0</span>,<span class="string">&#x27;how&#x27;</span>:<span class="number">1</span>,<span class="string">&#x27;are&#x27;</span>:<span class="number">2</span>,<span class="string">&#x27;you&#x27;</span>:<span class="number">3</span>&#125;</span><br><span class="line">embeds=nn.Embedding(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">hello_idx=torch.IntTensor([word_to_ix[<span class="string">&#x27;hello&#x27;</span>]])</span><br><span class="line">hello_embeds=embeds(hello_idx)</span><br><span class="line"><span class="built_in">print</span>(hello_embeds)</span><br></pre></td></tr></table></figure><p>我们也可以使用别人已经训练好的词嵌入向量，比如：</p><blockquote><p><a href="https://github.com/Embedding/Chinese-Word-Vectors">github.com</a></p></blockquote><p>中有已训练好的300维中文embedding。</p><p>但是在训练时需要设置对其导数不跟新：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.embedding.weight.data.copy_(torch.from_numpy(embeding_vector))</span><br><span class="line">self.embedding.weight.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>第一句加载词向量，第二句设置不对其更新</p><p>加载词向量也可以使用：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.embdeeing.weight=nn.parameter(torch.Tensor(embedding_weight))</span><br></pre></td></tr></table></figure><h2 id="N-Gram模型">N Gram模型</h2><p>N-Gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。</p><p>每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。</p><p>N Gram解决一个什么问题呢？就是通过前文对后文出现词进行预测，比如现在需要预测一句：</p><center>’‘I lived in china for 10 years, I can speak ___."</center><p>很明显我们希望预测这个词为chinese，那么N Gram模型是如何实现此功能的呢？</p><p>首先该模型对此句话引入了某种马尔可夫假设，即当前单词仅有前面n个词有关，而不是对于前面所有词有关，那么对于一句由$w_1,w_2,…w_n$n个单词组成的句子T，其概率为：<br>$$<br>P(T)=P(w_1)P(w_2\vert w_1)P(w_3 \vert w_1w_2)…P(w_n \vert w_{n-1}w_{n-1}…w_1)<br>$$<br>对于这里的条件概率，传统的方法是统计语料中每个词出现的频率，根据贝叶斯定理来估计这个条件概率，这里我们就可以用词嵌入对其进行代替，然后使用 RNN 进行条件概率的计算，然后最大化这个条件概率不仅修改词嵌入，同时能够使得模型可以依据计算的条件概率对其中的一个单词进行预测。</p><p>一个N Gram模型例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">context_size=<span class="number">2</span></span><br><span class="line">embedding_size=<span class="number">10</span></span><br><span class="line">txt=<span class="string">&quot;&quot;&quot;天也欢喜，地也欢喜，人也欢喜，</span></span><br><span class="line"><span class="string">    欢喜我遇到了你，你也遇到了我。</span></span><br><span class="line"><span class="string">    当时是你心里有了一个我，</span></span><br><span class="line"><span class="string">    我心里也有了一个你，</span></span><br><span class="line"><span class="string">    从今后是朝朝暮暮在一起。</span></span><br><span class="line"><span class="string">    地久天长，同心比翼，相敬相爱相扶持，</span></span><br><span class="line"><span class="string">    偶然发脾气，也要规劝勉励。</span></span><br><span class="line"><span class="string">    在工作中学习，在服务上努力，</span></span><br><span class="line"><span class="string">    追求真理，抗战到底。</span></span><br><span class="line"><span class="string">    为着大我忘却小己，直等到最后胜利。</span></span><br><span class="line"><span class="string">    再生一两个孩子，一半儿像我，一半儿像你.&quot;&quot;&quot;</span><span class="comment">#这里采用陶行知老先生写给妻子的诗作文语料库</span></span><br><span class="line">word_list=jieba.lcut(txt)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="string">&#x27;，&#x27;</span>,<span class="string">&#x27; &#x27;</span>,<span class="string">&#x27;\n&#x27;</span>,<span class="string">&#x27;。&#x27;</span>]:</span><br><span class="line">    <span class="keyword">while</span> i <span class="keyword">in</span> word_list:</span><br><span class="line">        word_list.remove(i)</span><br><span class="line">trigram = [((word_list[i], word_list[i+<span class="number">1</span>]), word_list[i+<span class="number">2</span>])  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word_list)-<span class="number">2</span>)]</span><br><span class="line">vocb=<span class="built_in">set</span>(word_list)</span><br><span class="line">word_idx=&#123;word: i <span class="keyword">for</span> i,word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocb)&#125;</span><br><span class="line">inx_to_word=&#123;word_idx[word]:word <span class="keyword">for</span> word <span class="keyword">in</span> word_idx&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NgramModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,vocb_size,context_size=context_size,n_dim=embedding_size</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(NgramModel,self).__init__()</span><br><span class="line">        self.n_word=vocb_size</span><br><span class="line">        self.embedding=nn.Embedding(self.n_word,n_dim)</span><br><span class="line">        self.linear1=nn.Sequential(nn.Linear(context_size*n_dim,<span class="number">128</span>),nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        self.linear2=nn.Linear(<span class="number">128</span>,self.n_word)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        emb=self.embedding(x)</span><br><span class="line">        emb=emb.reshape(<span class="number">1</span>,-<span class="number">1</span>) </span><br><span class="line">        out=self.linear1(emb)</span><br><span class="line">        out=self.linear2(out)</span><br><span class="line">       </span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">net = NgramModel(<span class="built_in">len</span>(word_idx)).cuda()</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">1e-2</span>, weight_decay=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word, label <span class="keyword">in</span> trigram: <span class="comment"># 使用前 100 个作为训练集</span></span><br><span class="line">        word = torch.LongTensor([word_idx[i] <span class="keyword">for</span> i <span class="keyword">in</span> word]).cuda() <span class="comment"># 将两个词作为输入</span></span><br><span class="line">        label = torch.LongTensor([word_idx[label]]).cuda()</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        out = net(word)</span><br><span class="line">        loss = criterion(out, label)</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> (e + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#123;&#125;, Loss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>, train_loss / <span class="built_in">len</span>(trigram)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = net.<span class="built_in">eval</span>()</span><br><span class="line">word, label = trigram[<span class="number">30</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;input: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(word))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;label: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(label))</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line">word = torch.LongTensor([word_idx[i] <span class="keyword">for</span> i <span class="keyword">in</span> word]).cuda()</span><br><span class="line">out = net(word)</span><br><span class="line">pred_label_idx = out.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>].item()</span><br><span class="line">predict_word = inx_to_word[pred_label_idx]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;real word is &#123;&#125;, predicted word is &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(label, predict_word))</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/26/857P7.png" alt="857P7.png"></p><p>​可以看到我们的N Gram模型中，只使用单边单词预测当前词汇，而还有一种使用双边单词预测当前词汇的模型：Continuous Bag-of-words model(CBOW),感兴趣的朋友可以查看<a href="https://zhuanlan.zhihu.com/p/55983009">Word2Vec之CBOW - 知乎 (zhihu.com)</a>。</p><h2 id="词性判断">词性判断</h2><p>很明显我们在N Gram中只使用了word embedding和Linear层，并没有使用RNN,接下来我们将使用LSTM进行词性判断.</p><p>原理：其实词性判断说到底还是一个分类问题，输入一句话可以看作一个序列，序列中每个词由高维embedding表示，其输出与该序列等长，每个输出表示对词性的判断。使用LSTM做词性判断很好理解，因为单独对一个词做词性判断显然是不现实 的，我们需要结合上下文。</p><p>字符增强：还可以引入字符增强来实现词性预测，何谓字符增强呢？很简单，比如一个词后缀为ly，那么我们大概率判定其为副词。将这两种方法结合起来，就能更好的做词性判断。</p><p>我们还是使用LSTM，但是这次不再使用句子作为输入序列，二十将每个单词作为输入。每个单词由不同字母组成，我们将其看作一个序列，对每个字母做词向量，然后传入LSTM网络，接着我们把这个单词的输出(我们不需要关心其输出是什么，其只是一种抽象特征，能更好地预测结果)和其前面几个单词构成序列，可以对这些单词构建新的词嵌入，最后输出结果是单词的词性，也就是根据前面几个词的信息对这个词的词性进行分类。</p><p>综上，就是先将一个单词拆分为一个个字母序列，然后输入进入一个LSTM网络，将其输出与句子中该单词的embedding组合为一个更高维向量，然后再输入一个LSTM网络做词性预测。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> unittest <span class="keyword">import</span> result</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">training_data = [(<span class="string">&quot;The dog ate the apple&quot;</span>.split(),</span><br><span class="line">                  [<span class="string">&quot;DET&quot;</span>, <span class="string">&quot;NN&quot;</span>, <span class="string">&quot;V&quot;</span>, <span class="string">&quot;DET&quot;</span>, <span class="string">&quot;NN&quot;</span>]),</span><br><span class="line">                 (<span class="string">&quot;Everybody read that book&quot;</span>.split(), </span><br><span class="line">                  [<span class="string">&quot;NN&quot;</span>, <span class="string">&quot;V&quot;</span>, <span class="string">&quot;DET&quot;</span>, <span class="string">&quot;NN&quot;</span>])]</span><br><span class="line"></span><br><span class="line">word_to_idx = &#123;&#125;</span><br><span class="line">tag_to_idx = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> context, tag <span class="keyword">in</span> training_data:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> context:</span><br><span class="line">        <span class="keyword">if</span> word.lower() <span class="keyword">not</span> <span class="keyword">in</span> word_to_idx:</span><br><span class="line">            word_to_idx[word.lower()] = <span class="built_in">len</span>(word_to_idx)</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> tag:</span><br><span class="line">        <span class="keyword">if</span> label.lower() <span class="keyword">not</span> <span class="keyword">in</span> tag_to_idx:</span><br><span class="line">            tag_to_idx[label.lower()] = <span class="built_in">len</span>(tag_to_idx)</span><br><span class="line">word_to_idx</span><br><span class="line">tag_to_idx</span><br><span class="line"></span><br><span class="line">alphabet = <span class="string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span></span><br><span class="line">char_to_idx = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(alphabet)):</span><br><span class="line">    char_to_idx[alphabet[i]] = i</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_sequence</span>(<span class="params">x, dic</span>): <span class="comment"># 字符编码</span></span><br><span class="line">    idx = [dic[i.lower()] <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">    idx = torch.LongTensor(idx)</span><br><span class="line">    <span class="keyword">return</span> idx</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">char_lstm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_char, char_dim, char_hidden</span>):</span><br><span class="line">        <span class="built_in">super</span>(char_lstm, self).__init__()</span><br><span class="line">        self.char_embed = nn.Embedding(n_char, char_dim)</span><br><span class="line">        self.lstm = nn.LSTM(char_dim, char_hidden)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        </span><br><span class="line">        x = self.char_embed(x)</span><br><span class="line">        out, _ = self.lstm(x)</span><br><span class="line">        <span class="keyword">return</span> out[-<span class="number">1</span>] <span class="comment"># (batch, hidden)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">lstm_tagger</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_word, n_char, char_dim, word_dim, </span></span><br><span class="line"><span class="params">                 char_hidden, word_hidden, n_tag</span>):</span><br><span class="line">        <span class="built_in">super</span>(lstm_tagger, self).__init__()</span><br><span class="line">        self.word_embed = nn.Embedding(n_word, word_dim)</span><br><span class="line">        self.char_lstm = char_lstm(n_char, char_dim, char_hidden)</span><br><span class="line">        self.word_lstm = nn.LSTM(word_dim + char_hidden, word_hidden)</span><br><span class="line">        self.classify = nn.Linear(word_hidden, n_tag)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, word</span>):</span><br><span class="line">        char = []</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> word: <span class="comment"># 对于每个单词做字符的 lstm</span></span><br><span class="line">            char_list = make_sequence(w, char_to_idx)</span><br><span class="line">           </span><br><span class="line">            char_list = char_list.unsqueeze(<span class="number">1</span>) <span class="comment"># (seq, batch, feature) 满足 lstm 输入条件</span></span><br><span class="line">            char_infor = self.char_lstm(Variable(char_list)) <span class="comment"># (batch, char_hidden)</span></span><br><span class="line">            char.append(char_infor)</span><br><span class="line">        char = torch.stack(char, dim=<span class="number">0</span>) <span class="comment"># (seq, batch, feature)  </span></span><br><span class="line">        x = self.word_embed(x) <span class="comment"># (batch, seq, word_dim)</span></span><br><span class="line">        x = x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) <span class="comment"># 改变顺序(seq,batch,word_dim)</span></span><br><span class="line">        x = torch.cat((x, char), dim=<span class="number">2</span>) <span class="comment"># 沿着特征通道将每个词的词嵌入和字符 lstm 输出的结果拼接在一起(seq,batch,word_dim+char_dim)</span></span><br><span class="line">        x, _ = self.word_lstm(x)</span><br><span class="line">        <span class="comment"># s,b,h=x.shape</span></span><br><span class="line">        <span class="comment"># x=x.reshape(-1,h)# 重新 reshape 进行分类线性层</span></span><br><span class="line">        out = self.classify(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">net = lstm_tagger(<span class="built_in">len</span>(word_to_idx), <span class="built_in">len</span>(char_to_idx), <span class="number">10</span>, <span class="number">100</span>, <span class="number">50</span>, <span class="number">128</span>, <span class="built_in">len</span>(tag_to_idx))</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">300</span>):</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word, tag <span class="keyword">in</span> training_data:</span><br><span class="line">        word_list = make_sequence(word, word_to_idx).unsqueeze(<span class="number">0</span>) <span class="comment"># 添加第一维 batch</span></span><br><span class="line">        tag = make_sequence(tag, tag_to_idx)</span><br><span class="line">        word_list = Variable(word_list)</span><br><span class="line">        tag = Variable(tag)</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        out = net(word_list, word)</span><br><span class="line">        out=out.reshape(-<span class="number">1</span>,out.size(<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># print(out.shape,tag.shape)</span></span><br><span class="line">        loss = criterion(out, tag)</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> (e + <span class="number">1</span>) % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125;, Loss: &#123;:.5f&#125;&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>, train_loss / <span class="built_in">len</span>(training_data)))</span><br><span class="line"></span><br><span class="line">net = net.<span class="built_in">eval</span>()</span><br><span class="line">test_sent = <span class="string">&#x27;Everybody ate the apple&#x27;</span></span><br><span class="line">test = make_sequence(test_sent.split(), word_to_idx).unsqueeze(<span class="number">0</span>)</span><br><span class="line">out = net(Variable(test), test_sent.split())</span><br><span class="line">out=out.detach().cpu().numpy()</span><br><span class="line">test_word=test_sent.split()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_key</span>(<span class="params">d,value</span>):</span><br><span class="line">    <span class="keyword">return</span> [k <span class="keyword">for</span> k,v <span class="keyword">in</span> d.items() <span class="keyword">if</span> v==value]</span><br><span class="line">result=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(out.shape[<span class="number">0</span>]):</span><br><span class="line">    max_idx=np.argmax(out[i])</span><br><span class="line">    result[test_word[i]]=get_key(tag_to_idx,max_idx)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/26/8zk8m.png" alt="8zk8m.png"></p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 循环神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN</title>
      <link href="/2022/04/27/RNN/"/>
      <url>/2022/04/27/RNN/</url>
      
        <content type="html"><![CDATA[<h1>RNN</h1><p>对于人类而言，以前见过的十五会在脑海里留下记忆，虽然随后记忆会慢慢消失，但是每当经过体系，人们对往事的记忆又会影响当下的判断。</p><p>而对于卷积神经网络，其相当于人类的视觉，对现在所看到的东西提取特征与分析，去没有记忆能力，所有它只能处理一种特定的视觉任务，没办法根据以前的记忆来处理新的问题。那有没有基于记忆的网络模型呢？<strong>RNN!!!</strong></p><h2 id="问题介绍">问题介绍</h2><p>考虑两个句子：<br>$$<br>[我，喜欢，你]<br>$$</p><p>$$<br>[我，不，喜欢，你]<br>$$</p><p>对一句话表达的是喜欢你的意思，第二句话表达的是不喜欢你的意思。如果采用无记忆的网络模型，第一个句子作为训练集，在测试第二个句子时，模型会判断为喜欢你的意思。因为它无法将前文的 **“不”**和  <strong>喜欢</strong>一起分析，但是如果是模型能记忆 <strong>喜欢</strong>之前的词，就会预测出不同结果。</p><h2 id="RNN基本结构">RNN基本结构</h2><p>RNN的基本结构特别简单，就是将网络的输出保存在一个记忆单元中，这个记忆单元和下一次的输入一起进入神经网络中。</p><p><img src="https://s1.328888.xyz/2022/04/20/rFNlO.png" alt="rFNlO.png"></p><p>如图所示，A为神经网络，$[x_0,x_1,…,x_t]$为输入时间序列，RNN把t=0时刻的输出$h_0$又传入了t=1时刻作为输入，把t=1时刻的输出$h_0$又传入了t=2时刻作为输入即：<br>$$<br>h_t=tanh(x_t@w_{ih}+b_{ih}+h_{t-1}@w_{hh}+b_{hh})<br>$$<br>可以看到网络是单向的，即我们只能知道单侧的信息，比如输入为$x_i$时，除其本身之外，我们只输入$h_{i-1}$，其中只含有$[x_0,x_1,…,x_{i-1}]$的信息，而不知道$[x_{i+1},…,x_n]$的信息。对于文本等信号，常常是需要联系上下文做出解析，这时候就需要能记忆两侧信息的循环神经网络。</p><p>其基本结构如图所示：</p><p><img src="https://s1.328888.xyz/2022/04/20/rVUfR.png" alt="rVUfR.png"></p><h2 id="RNN的问题">RNN的问题</h2><p>人的记忆最大的问题是它具有遗忘性，我们总是更加清楚的记得最近发生过的事而遗忘很久之前发生的事情，RNN有同样的问题。越早之前的信息经过网络的次数也就越多，在权值的作用下其在$h_{t-1}$中的占比也就越小。也就是所谓遗忘。这也是RNN没有广泛应用的原因：长时依赖问题</p><h2 id="RNN的变式">RNN的变式</h2><h3 id="LSTM-长短期记忆网络">LSTM(长短期记忆网络)</h3><p><img src="https://s1.328888.xyz/2022/04/20/rVCsg.png" alt="rVCsg.png"></p><p>LSTM网络一个时刻的计算如图所示。</p><p>首先我们来看所谓记忆单元的运算，$C_{t-1}$为上一步所得到的记忆单元，$C_t$w为这一步所输出的记忆单元，$C_{t-1} \rightarrow C_t$的运算步骤如下：</p><p>第一步将上一层的输出$h_{t-1}$和本层输入$x_t$结合在一起做一个线性运算$W_f[h_{t-1},x_t]+b_f$,然后将其通过sigmoid函数作为长时记忆输入$C_{t-1}$的衰减系数：<br>$$<br>f_t=sigmoid(W_f[h_{t-1},x_t]+b_f)<br>$$<br>可以看到网络具体要保留多少记忆是由前一时刻输出和这一时刻输入共同决定的。</p><p>而对于该时刻学到的记忆，其衰减系数跟上述$C_{t-1}$的衰减系数计算方法一样，而当前学习到的记忆$\hat c_t$是通过线性变换$W_c[h_{t-1},x_t]+b_c$和tanh激活函数得到的。<br>$$<br>i_t=sigmoid(W_i[h_{t-1},x_t]+b_i)<br>$$</p><p>$$<br>\hat c_t=tanh(W_c[h_{t-1},x_t]+b_c)<br>$$</p><p>那么时刻t输出的长时记忆单元为：<br>$$<br>C_t=f_t \times C_{t-1}+i_t \times \hat c_t<br>$$</p><p>时刻t的输出为：<br>$$<br>o_t=sigmoid(W_o[h_{t-1},x_t]+b_o)<br>h_t=o_t \times tang(C_t)<br>$$</p><h3 id="GRU-门控循环单元">GRU(门控循环单元)</h3><p>GRU将长时记忆单元和输入合成了一个更新单元，同时网络不在额外给出记忆状态$C_t$，而是将输出结果$h_t$作为记忆状态不断向后循环传递，网络的输入输出都变得特别简单。</p><p><img src="https://s1.328888.xyz/2022/04/20/rVKsR.png" alt="rVKsR.png"></p><p>其基本单元如图所示，计算如下：<br>$$<br>\begin{aligned}<br>z_t=sigmoid(W_z·[h_{t-1},x_t])\\<br>r_t=sigmoid(W_r·[h_{t-1},x_t])\\<br>h’<em>t=tanh((W·[r_t*h</em>{t-1},x_t])\\<br>h_t=(1-z_t)*h_{t-1}+z_th’_t<br>\end{aligned}<br>$$<br>GRU与LSTM最大的不同就在于记忆状态就是网络的输出结构$h_t$</p><h2 id="收敛性问题">收敛性问题</h2><p>由于RNN权重在网络中循环的结构里会被不断地重复使用，那么梯度微小的变化在经过循环结构之后都被放大。所以设置一个固定的学习率并不能有效收敛，同时梯度变化没有规律，所以设置衰减的学习率也不能满足条件。解决办法之一是梯度裁剪(gradient clipping)</p><h2 id="RNN的pytorch实现">RNN的pytorch实现</h2><p>最简单的标准RNN可以通过<code>nn.RNN()</code>直接调用，其输入，输出参数如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Args:</span><br><span class="line">    input_size: The number of expected features in the input `x`</span><br><span class="line">    hidden_size: The number of features in the hidden state `h`</span><br><span class="line">    num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``</span><br><span class="line">        would mean stacking two RNNs together to form a `stacked RNN`,</span><br><span class="line">        with the second RNN taking in outputs of the first RNN and</span><br><span class="line">        computing the final results. Default: 1</span><br><span class="line">    nonlinearity: The non-linearity to use. Can be either ``&#x27;tanh&#x27;`` or ``&#x27;relu&#x27;``. Default: ``&#x27;tanh&#x27;``</span><br><span class="line">    bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.</span><br><span class="line">        Default: ``True``</span><br><span class="line">    batch_first: If ``True``, then the input and output tensors are provided</span><br><span class="line">        as `(batch, seq, feature)` instead of `(seq, batch, feature)`.</span><br><span class="line">        Note that this does not apply to hidden or cell states. See the</span><br><span class="line">        Inputs/Outputs sections below for details.  Default: ``False``</span><br><span class="line">    dropout: If non-zero, introduces a `Dropout` layer on the outputs of each</span><br><span class="line">        RNN layer except the last layer, with dropout probability equal to</span><br><span class="line">        :attr:`dropout`. Default: 0</span><br><span class="line">    bidirectional: If ``True``, becomes a bidirectional RNN. Default: ``False``</span><br></pre></td></tr></table></figure><p>input_size：输入$x_t$的维度特征</p><p>hidden_size：输出$h_t$的维度特征</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 循环神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据增强</title>
      <link href="/2022/04/20/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/"/>
      <url>/2022/04/20/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/</url>
      
        <content type="html"><![CDATA[<h1>数据增强</h1><p>我们都知道，神经网络能够拟合任意的非线性函数，但是这都有一个前提，就是我们的Train数据集足够的大，能够让模型学习到足够多的信息，但实际情况下，很多时候我们并没有如此大的数据集。那怎么办呢？我们可以通过对原图像做一些变换生成新的图像，从而扩大数据集。</p><center><mark>以下api都来自于torchvision.nn.transforms</mark></center><h2 id="Resize">Resize</h2><p>改变图片大小：</p><p>使用示例：<code>transforms.Resize([32,32])</code></p><h2 id="Flip">Flip</h2><p>翻转操作</p><p><code>transforms.RandomHorizontalFlip()</code>:随机决定是否左右翻转</p><p><code>transforms.RandomVerticalFlip()</code>:随机决定是否上下翻转</p><h2 id="Rotate">Rotate</h2><p>旋转操作</p><p><code>transforms.RandomRotation(15)</code>:随机在-15~15度之间进行旋转.</p><p><code>transforms.RandomRotation([90,180,270])</code>:随机旋转90,180,270度</p><h2 id="Crop-part">Crop part</h2><p>随机裁剪部分：</p><p><code>transforms.RandomCrop([28,28])</code>:随机裁剪为28*28</p><h2 id="Noise">Noise</h2><p>使用numpy随机添加噪声</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 卷积神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN实现MNIST数字分类</title>
      <link href="/2022/04/20/CNN%E5%AE%9E%E7%8E%B0MNIST%E6%95%B0%E5%AD%97%E5%88%86%E7%B1%BB/"/>
      <url>/2022/04/20/CNN%E5%AE%9E%E7%8E%B0MNIST%E6%95%B0%E5%AD%97%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="comment">#参数定义</span></span><br><span class="line">parser=argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>,default=<span class="number">64</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--learning_rate&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">float</span>,default=<span class="number">1e-2</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--num_epochs&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">int</span>,default=<span class="number">20</span>)</span><br><span class="line">args=parser.parse_args()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#模型结构与激活函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">simpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(simpleCNN,self).__init__()</span><br><span class="line">        <span class="comment">#[b,1,28,28]</span></span><br><span class="line">        self.layer1=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>,<span class="number">16</span>,kernel_size=<span class="number">3</span>),nn.BatchNorm2d(<span class="number">16</span>),nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        <span class="comment">#[b,16,26,26]</span></span><br><span class="line">        self.layer2=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">16</span>,<span class="number">32</span>,kernel_size=<span class="number">3</span>),nn.BatchNorm2d(<span class="number">32</span>),nn.ReLU(<span class="literal">True</span>),nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">        <span class="comment">#[b,32,24,24]-&gt;[b,32,12,12]</span></span><br><span class="line">        self.layer3=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,kernel_size=<span class="number">3</span>),nn.BatchNorm2d(<span class="number">64</span>),nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        <span class="comment">#[b,64,10,10]</span></span><br><span class="line">        self.layer4=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">128</span>,kernel_size=<span class="number">3</span>),nn.BatchNorm2d(<span class="number">128</span>),nn.ReLU(<span class="literal">True</span>),nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">        <span class="comment">#[b,128,8,8]-&gt;#[b,128,4,4]</span></span><br><span class="line">        self.fc=nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">128</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">1024</span>)</span><br><span class="line">            ,nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">            ,nn.Linear(<span class="number">1024</span>,<span class="number">128</span>)</span><br><span class="line">            ,nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">            ,nn.Linear(<span class="number">128</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        self.x1=self.layer1(x)</span><br><span class="line">        self.x2=self.layer2(self.x1)</span><br><span class="line">        self.x3=self.layer3(self.x2)</span><br><span class="line">        self.x4=self.layer4(self.x3)</span><br><span class="line">        self.x4=self.x4.reshape(self.x4.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        self.out=self.fc(self.x4)</span><br><span class="line">        <span class="keyword">return</span> self.out</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据预处理</span></span><br><span class="line">data_tf=transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.5</span>],[<span class="number">0.5</span>])]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据集</span></span><br><span class="line">train_data=datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,train=<span class="literal">True</span>,transform=data_tf,download=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">test_data=datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,train=<span class="literal">False</span>,transform=data_tf)</span><br><span class="line">train_loader=DataLoader(train_data,batch_size=args.batch_size,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader=DataLoader(test_data,batch_size=<span class="number">10000</span>,shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model=simpleCNN().cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model=simpleCNN()</span><br><span class="line"></span><br><span class="line">criterion=nn.CrossEntropyLoss()</span><br><span class="line">optimizer=optim.SGD(model.parameters(),lr=args.learning_rate,momentum=<span class="number">0.78</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        img,label=batch</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            img=img.cuda()</span><br><span class="line">            label=label.cuda()</span><br><span class="line">        out=model(img)</span><br><span class="line">      </span><br><span class="line">        loss=criterion(out,label)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epochs:&#123;&#125;,loss:&#123;:.6f&#125;&quot;</span>.<span class="built_in">format</span>(epoch,loss))</span><br><span class="line"></span><br><span class="line"><span class="comment">#测试模型</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">eval_loss=<span class="number">0</span></span><br><span class="line">eval_acc=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> test_loader:</span><br><span class="line">    img,label=batch</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        img=img.cuda()</span><br><span class="line">        label=label.cuda()</span><br><span class="line">    out=model(img)</span><br><span class="line">    loss=criterion(out,label)</span><br><span class="line"> </span><br><span class="line">    eval_loss+=loss.detach()*label.size(<span class="number">0</span>)</span><br><span class="line">    pred=torch.<span class="built_in">max</span>(out,dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">    num_correct=(pred==label).<span class="built_in">sum</span>()</span><br><span class="line">    eval_acc+=num_correct.detach()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test loss:&#123;&#125;,ACC:&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(eval_loss/<span class="built_in">len</span>(test_data),eval_acc/<span class="built_in">len</span>(test_data)))</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/20/raHty.png" alt="raHty.png"></p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 卷积神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典CNN</title>
      <link href="/2022/04/20/%E7%BB%8F%E5%85%B8CNN/"/>
      <url>/2022/04/20/%E7%BB%8F%E5%85%B8CNN/</url>
      
        <content type="html"><![CDATA[<h1>经典CNN</h1><h2 id="LENET-5">LENET-5</h2><p><img src="https://s1.328888.xyz/2022/04/20/rOADZ.png" alt="rOADZ.png"></p><h2 id="AlexNet">AlexNet</h2><p><img src="https://s1.328888.xyz/2022/04/20/rODRt.png" alt="rODRt.png"></p><h2 id="VGGNet">VGGNet</h2><p>计算量非常大</p><h2 id="GoogleNet">GoogleNet</h2><p>同一层用不同尺寸的卷积核：Inception</p><p><img src="https://s1.328888.xyz/2022/04/20/rOW1P.png" alt="rOW1P.png"></p><h2 id="ResNet-深度残差网络">ResNet(深度残差网络)</h2><p>当深度到达22层以后，再堆叠更多的层数，模型性能反而会下降，因为太多层的堆叠经常会使得误差积累导致梯度离散。</p><p>ResNet：卷积层之间加一个shortcut</p><p><img src="https://s1.328888.xyz/2022/04/20/rOpdA.png" alt="rOpdA.png"></p><p>假设传统神经网络输入为x，输出为F(X),则需要学习的就是F(X)这个函数映射。</p><p>而对于ResNet，输入为x，输出为H(X)=F(X)+X，我们需要学习的是F(X=)H(X)-X这样输入与输出差别，即残差。</p><p>常用的ResNet unit：</p><p><img src="https://s1.328888.xyz/2022/04/20/rOwbW.png" alt="rOwbW.png"></p><p>第一层和第三层使用1x1的卷积核能极大的减少参数量。</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 卷积神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> CNN </tag>
            
            <tag> ResNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN原理</title>
      <link href="/2022/04/19/CNN%E5%8E%9F%E7%90%86/"/>
      <url>/2022/04/19/CNN%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1>CNN</h1><p><img src="https://s1.328888.xyz/2022/04/19/rYL5g.png" alt="rYL5g.png"></p><p>卷积神经网络(CNN)的结构:</p><p>卷积层→池化层→全连接层</p><p>那为何不用传统全连接神经网络处理图像呢？其缺点也是很明显的，对于大尺寸图片：</p><ul><li>首先将图片展开为张量会丢失信息</li><li>处理大尺寸图片需要巨量的参数</li><li>参数太多容易导致过拟合</li></ul><h2 id="卷积层">卷积层</h2><p>卷积层是构建卷积神经网络的<strong>核心层</strong>，它产生了网络中大部分的<strong>计算量</strong>。注意是计算量而不是参数量。</p><p><img src="https://s1.328888.xyz/2022/04/19/rI8et.png" alt="rI8et.png"></p><h3 id="卷积">卷积</h3><p>何谓卷积？其实就是乘加运算与位置移动。上图所示，左边第一个矩阵为输入x，第二个矩阵为卷积核，所谓卷积就是他们对应位置相乘相加，最后得到了右图中235的结果。那让卷积核不停在输入x上移动过再乘加，得到的就是卷积层的输出。如下图所示卷积核在输入x上不停移动和乘加即为卷积。</p><p><img src="https://s1.328888.xyz/2022/04/19/rIYvk.gif" alt="rIYvk.gif"></p><h3 id="卷积运算的维度">卷积运算的维度</h3><p>既然引入了上图，那我们就借此说一下卷积运算的维度。都说卷积运算处理高度，宽度以外还有一个深度，何谓深度呢？我们先对以上gif做一个分析，就明白了。</p><p>首先输入为最左边一列三个矩阵，这三个矩阵来自于同一张图片的R,G,B三个通道，高度为5，宽度为5，外面灰色的0为padding(之后我们会提到)，不是原图像数据内容。那么我们输入卷积层的Tensor的size就是：<br>$$<br>x[1,3,5,5]<br>$$<br>第一个维度表示一张图片，第二个维度为3个channel:RGB,也就是输入x的深度，第三、第四个维度为高度和宽度</p><p>图中第二列和第三列为两个卷积核，因为输入深度是3，所以他需要对R,G,B三个矩阵都做卷积，其深度也是3，那么卷积核的<code>Tensor.size()</code>就是：<br>$$<br>K[2,3,3,3]<br>$$<br>第一个维度表示两个不同卷积核，第二个维度表示3个channel,也就是深度，第三、第四个维度为高度和宽度</p><p>那输出参数呢?看图中最后一列：<br>$$<br>out[1,2,3,3]<br>$$<br>因为这是一张图片通过两个卷积核所得的两个3x3输出。</p><h3 id="感受野">感受野</h3><p>如果将上图中每个像素点看作神经节点，对比全连接神经网络，我们发现每个神经节点的输入仅与9个输入节点和权重有关，而不再是全连接中的与所有上一层输入有关。也就是说下一层节点仅与上一层一些神经节点连接，而这些神经节点数量仅决于卷积核的长和宽，这就叫做感受野，即与神经元连接的空间大小。</p><p>如此一来，参数的数量不就打打减小了吗，对于28*28的图片，全连接神经网络第一个隐藏层每个节点需要784个参数，而卷积层中只需要卷积核个数x卷积核大小 这么多个参数。</p><h3 id="空间排列">空间排列</h3><p>稍微动一下脑筋，若卷积核在移动过程中不允许超出输入x的边界范围，那输出矩阵的大小肯定小于原图片大小，所以我们需要引入 <strong>padding</strong>，即在图像最外面加一层或几层全为0的像素，如此已到达输出核输入矩阵大小相同的目的。</p><p>如上图所示，输入为5x5，如果padding=0，卷积核为3x3，那么输出只有2x2，加上一层padding，输出才能达到3x3.</p><p>步长就更好理解了，即卷积核每次移动几个单位。</p><p>那么以上几个参数都确定了的话，输出尺寸为多大呢，公式如下：<br>$$<br>outsize=\frac{W-F+2P}{S}+1<br>$$<br>其中</p><p>W:输入数据大小</p><p>F:卷积核尺寸</p><p>S:步长</p><p>P:padding</p><h3 id="pytorch卷积层模块">pytorch卷积层模块</h3><p><code>nn.Conv2d()</code>:其输入参数如下</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        in_channels: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        out_channels: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        kernel_size: _size_2_t,</span></span><br><span class="line"><span class="params">        stride: _size_2_t = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        padding: <span class="type">Union</span>[<span class="built_in">str</span>, _size_2_t] = <span class="number">0</span>,</span></span><br><span class="line"><span class="params">        dilation: _size_2_t = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        bias: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        padding_mode: <span class="built_in">str</span> = <span class="string">&#x27;zeros&#x27;</span>,  <span class="comment"># <span class="doctag">TODO:</span> refine this type</span></span></span><br><span class="line"><span class="params">        device=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        dtype=<span class="literal">None</span></span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br></pre></td></tr></table></figure><p>in_channels：输入数据的channel数（深度）,为3的化通常为RGB</p><p>out_channels：输出数据的深度，与卷积核的个数相同</p><p>kernel_size：感受野的大小，卷积核的尺寸</p><p>stride：移动步长</p><p>padding：padding=0表示不填充，padding=1表示填充一层</p><p>dilation：卷积杜宇输入数据体的空间间隔，默认为1</p><p>dilation=2时计算如下：</p><p><img src="https://s1.328888.xyz/2022/04/19/rRubR.png" alt="rRubR.png"></p><p>卷积时输入x不在紧邻，而是间隔一个像素点</p><p>groups：输出数据体深度上和输入数据体深度上的联系，默认为1，相关联。如果groups=2，则输入深度被分割成两份，输出数据也被分割成两份，他们之间分别对应起来，要求输入输出深度都能被groups整除。</p><p>bias：是否添加偏执</p><center>卷积层输出也需要激活函数</center><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#example</span></span><br><span class="line">layer1=nn.sequential(</span><br><span class="line">nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">3</span>,<span class="number">1</span>,padding=<span class="number">1</span>),nn.ReLU(<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><p>上述代码添加了一个输入channel为3，卷积核个数32，卷积核尺寸3x3，步长为1，padding=1，采用ReLU激活函数的卷积层。</p><h2 id="池化层">池化层</h2><p>通常会在卷积层之间周期性的插入一个池化层，其作用是逐渐减低数据体的空间尺寸，这样就能减少网络中的参数量，减少计算耗费资源，同时有效控制过拟合。</p><p>池化层和卷积层一样也有一个空间窗口，通常采取这些窗口中的最大值作为暑促，然后不断滑动窗口，对输入数据每一个深度切片(channel)单独处理，减少它的尺寸空间。</p><p>最常用的池化层尺寸为2*2，滑动步长为2：</p><p><img src="https://s1.328888.xyz/2022/04/19/rRnti.png" alt="rRnti.png"></p><p>输出数据体与输入数据体尺寸关系如下：<br>$$<br>W_2=\frac{W_1-F}{S}+1<br>$$<br>F:池化窗口大小</p><p>W1：输入数据大小</p><p>W2:输出数据大小</p><p>S:步长</p><p>卷积层之间一般引入最大池化效果最好，而平均池化一般放在卷积神经网络的最后一层。</p><p><mark>谨慎使用比较大的池化窗口，以免对网络有破坏性</mark></p><h3 id="pytorch池化层模块">pytorch池化层模块</h3><p><code>nn.MaxPool2d()</code>参数:</p><p>kernel_size: 窗口大小</p><p>stride: 步长</p><p>padding: 一般不添加</p><p>dilation: 默认为1</p><p>return_indices：是否返回最大值下标，默认为False</p><h2 id="全连接层">全连接层</h2><p>全连接成与之前介绍的一般神经网络的结构是一样的。一般经过一系列卷积层核池化层之后，提取出图片的特征图，将特征图中所有神经元变为全连接层的样子。</p><p>比如卷积层和池化层的输出为3x3x512，那么将其变为3x3x512=4608个神经元，再经过几个隐藏层后输出结果，在这个过程中为了防止过拟合引入Dropout。</p><h2 id="小卷积核有效性">小卷积核有效性</h2><p>一般而言，几个小滤波器的卷积层的组合比一个大滤波器好。</p><p>比如3个3x3的卷积核堆叠，其最终第三层卷积层对第一层输入数据的感受野是7x7，但这样做比直接使用一个7x7的卷积核好，原因如下：</p><ul><li>多个卷积层之间还有激活函数，比单一卷积层的结构更能提取出深层的特征</li><li>3个3x3的卷积层才27个参数，一个7x7需要49个参数</li></ul>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 卷积神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MNIST数字分类</title>
      <link href="/2022/04/19/MNIST%E6%95%B0%E5%AD%97%E5%88%86%E7%B1%BB/"/>
      <url>/2022/04/19/MNIST%E6%95%B0%E5%AD%97%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h1>MNIST数字分类</h1><h2 id="模型结构">模型结构</h2><p>简单的三层全连接模型：两个隐藏层</p><p>超参数定义:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="comment">#参数定义</span></span><br><span class="line">parser=argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>,default=<span class="number">64</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--learning_rate&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">float</span>,default=<span class="number">1e-2</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--num_epochs&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">int</span>,default=<span class="number">20</span>)</span><br><span class="line">args=parser.parse_args()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>模型结构：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#模型结构与激活函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">simpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_dim,n_hidden1,n_hidden2,out_dim</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(simpleNet,self).__init__()</span><br><span class="line">        self.layer1=nn.Sequential(</span><br><span class="line">            nn.Linear(in_dim,n_hidden1),nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        self.layer2=nn.Sequential(</span><br><span class="line">            nn.Linear(n_hidden1,n_hidden2),nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        self.layer3=nn.Sequential(</span><br><span class="line">            nn.Linear(n_hidden2,out_dim))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        self.x1=self.layer1(x)</span><br><span class="line">        self.x2=self.layer2(self.x1)</span><br><span class="line">        self.x3=self.layer3(self.x2)</span><br><span class="line">        <span class="keyword">return</span> self.x3</span><br></pre></td></tr></table></figure><p>数据预处理：</p><p>此处<code>transforms.Compose()</code>将各种预处理操作组合在一起，<code>transforms.ToTensor()</code>将图片转换为Tensor数据类型，<code>transforms.Normalize()</code>完成数据的去中心化和标准化，减去均值再除以方差，输入第一个参数为均值，第二个参数为方差。因为本例中图片时灰度图片，只有一个通道，如果是彩色图片有三个通道，需要使用<code>transforms.Normalize([a,b,c],[d,e,f])</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#数据预处理</span></span><br><span class="line">data_tf=transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.5</span>],[<span class="number">0.5</span>])]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>读取数据集:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取数据集</span></span><br><span class="line">train_data=datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,train=<span class="literal">True</span>,transform=data_tf,download=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">test_data=train_data=datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,train=<span class="literal">False</span>,transform=data_tf)</span><br><span class="line">train_loader=DataLoader(train_data,batch_size=args.batch_size,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader=DataLoader(test_data,batch_size=args.batch_size,shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model=simpleNet(<span class="number">28</span>*<span class="number">28</span>,<span class="number">300</span>,<span class="number">100</span>,<span class="number">10</span>).cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model=simpleNet(<span class="number">28</span>*<span class="number">28</span>,<span class="number">300</span>,<span class="number">100</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">criterion=nn.CrossEntropyLoss()</span><br><span class="line">optimizer=optim.SGD(model.parameters(),lr=args.learning_rate,weight_decay=<span class="number">0.01</span>,momentum=<span class="number">0.78</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        img,label=batch</span><br><span class="line">        img=img.reshape(img.size(<span class="number">0</span>),<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            img=img.cuda()</span><br><span class="line">            label=label.cuda()</span><br><span class="line">        out=model(img)</span><br><span class="line">      </span><br><span class="line">        loss=criterion(out,label)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epochs:&#123;&#125;,loss:&#123;:.6f&#125;&quot;</span>.<span class="built_in">format</span>(epoch,loss))</span><br></pre></td></tr></table></figure><p>测试模型:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试模型</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">eval_loss=<span class="number">0</span></span><br><span class="line">eval_acc=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> test_loader:</span><br><span class="line">    img,label=batch</span><br><span class="line">    img=img.reshape(img.size(<span class="number">0</span>),<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        img=img.cuda()</span><br><span class="line">        label=label.cuda()</span><br><span class="line">    out=model(img)</span><br><span class="line">    loss=criterion(out,label)</span><br><span class="line"> </span><br><span class="line">    eval_loss+=loss.detach()*label.size(<span class="number">0</span>)</span><br><span class="line">    pred=torch.<span class="built_in">max</span>(out,dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">    num_correct=(pred==label).<span class="built_in">sum</span>()</span><br><span class="line">    eval_acc+=num_correct.detach()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test loss:&#123;&#125;,ACC:&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(eval_loss/<span class="built_in">len</span>(test_data),eval_acc/<span class="built_in">len</span>(test_data)))</span><br></pre></td></tr></table></figure><p>输出结果：</p><p><img src="https://s1.328888.xyz/2022/04/19/rs2XC.png" alt="rs2XC.png"></p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络结构</title>
      <link href="/2022/04/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
      <url>/2022/04/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>已经学过了懒得写了</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>数据预处理与训练模型的技巧</title>
      <link href="/2022/04/19/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E5%B7%A7/"/>
      <url>/2022/04/19/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h1>数据预处理</h1><h2 id="中心化">中心化</h2><p>所有数据每个特征维度减去均值，使数据均值为0</p><h2 id="标准化">标准化</h2><p>使得数据均值为0后，还需要使用标准化的做法让数据的也在维度都有着相同的规模，有两种常用方法：</p><ul><li>除以标准差，使得新数据的分布更接近高斯分布</li><li>让每个特征维度大的最大值和最小值按比例放到1~-1之间</li></ul><h2 id="主成分分析">主成分分析</h2><h3 id="相关背景">相关背景</h3><p>在许多领域的研究与应用中，通常需要对含有多个变量的数据进行观测，收集大量数据后进行分析寻找规律。多变量大数据集无疑会为研究和应用提供丰富的信息，但是也在一定程度上增加了数据采集的工作量。更重要的是在很多情形下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性。如果分别对每个指标进行分析，分析往往是孤立的，不能完全利用数据中的信息，因此盲目减少指标会损失很多有用的信息，从而产生错误的结论。</p><p>因此需要找到一种合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量之间存在一定的相关关系，因此可以考虑将关系紧密的变量变成尽可能少的新变量，使这些新变量是两两不相关的，那么就可以用较少的综合指标分别代表存在于各个变量中的各类信息。主成分分析与因子分析就属于这类降维算法。</p><h3 id="原理">原理</h3><p>PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。</p><p>emm，说的真是难懂，其实是就是如何对数据降维同时有保留最多的信息。推荐一下以数学方式解释的的视频：</p><p><a href="https://www.bilibili.com/video/BV1E5411E71z?spm_id_from=333.337.search-card.all.click">用最直观的方式告诉你：什么是主成分分析PCA_哔哩哔哩_bilibili</a></p><p>以及一个以图形方式解释的视频：</p><p><a href="https://www.bilibili.com/video/BV1C7411A7bj?spm_id_from=333.337.search-card.all.click">【中字】主成分分析法（PCA）| 分步步骤解析 看完你就懂了！_哔哩哔哩_bilibili</a></p><p>简单写一下上述视频所讲内容：</p><p>考虑二维情况，假设数据点有以下分布：</p><p><img src="https://s1.328888.xyz/2022/04/19/rtNDm.png" alt="rtNDm.png"></p><p>我们需要存储的信息为二维，即每个点的x,y坐标，有没有办法将其降为一维呢？肯定是有的，我们需要将坐标轴移动和旋转如下：</p><p><img src="https://s1.328888.xyz/2022/04/19/rtS3A.png" alt="rtS3A.png"></p><p>如此一来，我们需要存储的数据就只有新坐标系的原点，角度和新坐标点的x坐标，数据从二维降为了一维。那么如何实现坐标系的平移和旋转呢？寻找新的坐标系原点其实我们已经实现了，没错，就是上文所述的去中心化。那如何旋转？请看下文。</p><p>首先我们需要明确旋转到怎样一个角度我们认为好呢。上文说我们要实现降维的同时保留尽可能多的信息，何谓保留最多的信息呢？我的理解是，降维之前下隔得远的点降维后仍然隔得远，隔得近的点仍然隔得近，也就是数据之间的分布关系仍需要很好的保留。换句话说，数据之间的偏离程度要尽可能保留。诶，等等，数据之间的偏离程度？方差！！！！</p><center> <mark> Bingo！我们要找到的旋转角度需要使方差最大 </mark></center><p>从几何角度说，因为我们已经去中心化，我们需要找到角度使所有数据点在这条线上的投影点到原点的距离的平方和SSD最大。这条线被称为主成分1，或者说是旋转后的一条坐标轴，该SSD为主成分1的特征值，我们可以在之后证明，如此我们再找到另一条坐标轴主成分2与其SSD。那么我们降为时就可以选取SSD较小的维度删掉以实现降维。</p><p>问题是怎样找到这个角度呢？</p><p>先介绍一下数据的线性变换：</p><p>假设我们有数据集D：<br>$$<br>D=\begin{bmatrix}<br>x_1&amp;x_2&amp;x_3&amp;x_4\\<br>y_1&amp;y_2&amp;y_3&amp;y_4<br>\end{bmatrix}<br>$$<br>与一个对角矩阵S：<br>$$<br>S=\begin{bmatrix}<br>2&amp;0\\<br>0&amp;1<br>\end{bmatrix}<br>$$<br>那SD就等于：<br>$$<br>SD=\begin{bmatrix}<br>2x_1&amp;2x_2&amp;2x_3&amp;2x_4\\<br>y_1&amp;y_2&amp;y_3&amp;y_4<br>\end{bmatrix}<br>$$<br>相当于在x轴上对所有数据拉伸到了2倍。那这跟坐标轴旋转有什么关系呢？看图：</p><p><img src="https://s1.328888.xyz/2022/04/19/r58CT.png" alt="r58CT.png"></p><p>根据“相对论”，坐标轴相对于数据做旋转，相当于所有数据对原坐标轴旋转，如上图所述，假设原坐标轴逆时针旋转$\theta$到红色坐标轴，那点$(x,y)$旋转到点$(x’,y’)$，那对于黑色坐标轴，$(x’,y’)$等于多少呢，稍微用一用初中数学知识我们得到：<br>$$<br>(x’,y’)=(x\cos\theta-y\sin\theta,x\sin\theta+y\cos\theta)<br>$$<br>转化成上述线性变换形式，坐标轴的旋转就相当于：<br>$$<br>RD=\begin{bmatrix}<br>\cos\theta&amp;-\sin\theta\\<br>\sin\theta&amp;\cos\theta<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1&amp;x_2&amp;x_3&amp;x_4\\<br>y_1&amp;y_2&amp;y_3&amp;y_4<br>\end{bmatrix}<br>$$<br>那我们为了让数据的方差更大，可以旋转时对其做一个拉伸,拉伸方向也是使方差最大方向，即：<br>$$<br>D’=RSD=\begin{bmatrix}<br>\cos\theta&amp;-\sin\theta\\<br>\sin\theta&amp;\cos\theta<br>\end{bmatrix}<br>\begin{bmatrix}<br>a&amp;0\\<br>0&amp;b<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1&amp;x_2&amp;x_3&amp;x_4\\<br>y_1&amp;y_2&amp;y_3&amp;y_4<br>\end{bmatrix}<br>$$</p><hr><p>那假设有一个白数据W，其x与y都是服从标准正态分布且不相关，我们所有的数据集W’，其x,y都服从正态分布但非标准且相关，则W’可由W拉伸后旋转得来，即：<br>$$<br>W’=RSW<br>$$<br>反之：<br>$$<br>W=S^{-1}R^{-1}W’<br>$$<br>其中：<br>$$<br>S=\begin{bmatrix}<br>\frac{1}{a}&amp;0\\<br>0&amp;\frac{1}{b}<br>\end{bmatrix}<br>$$</p><p>$$<br>R=\begin{bmatrix}<br>\cos(-\theta)&amp;-\sin(-\theta)\\<br>\sin(-\theta)&amp;\cos(-\theta)<br>\end{bmatrix}<br>=\begin{bmatrix}<br>\cos\theta&amp;\sin\theta\\<br>-\sin\theta&amp;\cos\theta<br>\end{bmatrix}<br>=R^T<br>$$</p><p>问题又来了，R怎么求呢？ <strong>协方差矩阵的特征向量就是R</strong><br>$$<br>协方差：cov(x,y)=\frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{n-1}<br>$$<br>其代表：两个变量在变化过程中是同方向变化还是反方向变化？通向或反向程度如何？</p><p>那么对于我们已经 **去中心化 **的数据，其$\bar x=\bar y=0$,则协方差为：<br>$$<br>cov(x,y)=\frac{\sum_{i=1}^nx_iy_i}{n-1}<br>$$<br>那么协方差矩阵：<br>$$<br>\begin{aligned}<br>C&amp;=\begin{bmatrix}<br>cov(x,x)&amp;cov(x,y)\\<br>cov(x,y)&amp;cov(y,y)<br>\end{bmatrix}\\<br>&amp;=\begin{bmatrix}<br>\frac{\sum_{i=1}^nx_ix_i}{n-1}&amp;\frac{\sum_{i=1}^nx_iy_i}{n-1}\\<br>\frac{\sum_{i=1}^nx_iy_i}{n-1}&amp;\frac{\sum_{i=1}^ny_iy_i}{n-1}<br>\end{bmatrix}\\<br>&amp;=\frac{1}{n-1}\begin{bmatrix}<br>\sum_{i=1}^nx_ix_i&amp;\sum_{i=1}^nx_iy_i\\<br>\sum_{i=1}^nx_iy_i&amp;\sum_{i=1}^ny_iy_i<br>\end{bmatrix}\\<br>&amp;=\frac{1}{n-1}WW^T<br>\end{aligned}<br>$$<br>我们再求一下后的数据D’的协方差：<br>$$<br>\begin{aligned}<br>C’&amp;=\frac{1}{n-1}W’W’^T\\<br>&amp;=\frac{1}{n-1}RSW(RSW)^T\\<br>&amp;=\frac{1}{n-1}RSWW^TS^TR^T\\<br>&amp;=RS(\frac{1}{n-1}WW^T)S^TR^T\\<br>&amp;=RSCS^TR^T<br>\end{aligned}<br>$$<br>W是白数据，其协方差矩阵$C=E$单位矩阵，则上式可以化简为：<br>$$<br>C’=RSESR^{-1}=RS^2R^{-1}=RLR^{-1}<br>$$<br>其中L为一个对角矩阵。</p><p>那么C’的特征值与特征向量定义为：<br>$$<br>C’\vec v=\lambda \vec v<br>$$<br>则：<br>$$<br>C’[\vec v_1,\vec v_2,…,\vec v_n]=[\vec v_1,\vec v_2,…,\vec v_n]\begin{bmatrix}\lambda_1&amp;0&amp;\cdots &amp;0\\<br>0&amp;\lambda_2&amp;\cdots &amp;0\\<br>\vdots&amp;\vdots&amp;\ddots &amp;\vdots\\<br>0&amp;0&amp;\cdots &amp;\lambda_n<br>\end{bmatrix}<br>$$</p><p>$$<br>C’=[\vec v_1,\vec v_2,…,\vec v_n]\begin{bmatrix}\lambda_1&amp;0&amp;\cdots &amp;0\\<br>0&amp;\lambda_2&amp;\cdots &amp;0\\<br>\vdots&amp;\vdots&amp;\ddots &amp;\vdots\\<br>0&amp;0&amp;\cdots &amp;\lambda_n<br>\end{bmatrix}<br>[\vec v_1,\vec v_2,…,\vec v_n]^{-1}<br>$$</p><p>这不就是：<br>$$<br>C’=RLR^{-1}<br>$$<br>对于上述二位情况：<br>$$<br>L=\begin{bmatrix}a^2&amp;0\\0&amp;b^2\end{bmatrix}<br>$$<br>如此一来，我们就能得到旋转回来的数据$R^{-1}W’$的协方差为:<br>$$<br>\begin{aligned}<br>C_{R^{-1}W’}&amp;=\frac{1}{n-1}R^{-1}W’(R^{-1}W’)^T\\<br>&amp;=\frac{1}{n-1}R^{-1}W’W’^T(R^{-1})^T\\<br>&amp;=R^{-1}(\frac{1}{n-1}WW^T)(R^{-1})^T\\<br>&amp;=R^{-1}C’(R^{-1})^T\\<br>&amp;=R^{-1}RSS^TR^T(R^{-1})^T=SS^t=L<br>\end{aligned}<br>$$<br>则$a^2,b^2$既是两个轴方向的方差。同时又是协方差矩阵C’的特征值SSD。</p><p>那我们选择其中方差小的那个维度删掉，不就实现了降维？？？</p><p>总结一下PCA的步骤：</p><ul><li><p>去中心化</p></li><li><p>计算数据协方差矩阵及其特征值特征向量，求$R^{-1}W’$</p></li><li><p>降维：选择特征值较小的维度删除</p><p><strong>PCA的一个显著缺点是离群点会极大影响主成分的角度，造成算法效果不好</strong></p><h3 id="python实现">python实现</h3></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pca</span>(<span class="params">X,k</span>):<span class="comment">#k is the components you want</span></span><br><span class="line">  <span class="comment">#mean of each feature</span></span><br><span class="line">  n_samples, n_features = X.shape</span><br><span class="line">  mean=np.array([np.mean(X[:,i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_features)])</span><br><span class="line"> </span><br><span class="line">  norm_X=X-mean <span class="comment">#去中心化</span></span><br><span class="line"></span><br><span class="line">  scatter_matrix=np.dot(np.transpose(norm_X),norm_X)<span class="comment">#协方差矩阵</span></span><br><span class="line"> </span><br><span class="line">  eig_val, eig_vec = np.linalg.eig(scatter_matrix)<span class="comment">#协方差矩阵的特征向量和特征值</span></span><br><span class="line">  eig_pairs = [(np.<span class="built_in">abs</span>(eig_val[i]), eig_vec[:,i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_features)]</span><br><span class="line"></span><br><span class="line">  eig_pairs.sort(reverse=<span class="literal">True</span>)<span class="comment">#由大到小排序</span></span><br><span class="line"></span><br><span class="line">  feature=np.array([ele[<span class="number">1</span>] <span class="keyword">for</span> ele <span class="keyword">in</span> eig_pairs[:k]])<span class="comment">#留下k个主要特征</span></span><br><span class="line"></span><br><span class="line">  data=np.dot(norm_X,np.transpose(feature))</span><br><span class="line">  <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">X = np.array([[-<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">2</span>, -<span class="number">1</span>], [-<span class="number">3</span>, -<span class="number">2</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(pca(X,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/19/rW3cZ.png" alt="rW3cZ.png"></p><h2 id="白噪声处理">白噪声处理</h2><p>在PCA处理后在除以其特征值得到最开始的白数据：<br>$$<br>W=S^{-1}R^{-1}W’<br>$$<br>白噪声处理会增强数据中的噪声，因为其增强了所有维度，包括一些方差很小的不相关维度</p><h1>网络参数初始化</h1><h2 id="权重初始化">权重初始化</h2><p>除了数据需要预处理之外，网络权重也需要初始化。</p><ul><li><p><s>全0初始化</s>：看起来最简单但是 <strong>不可行</strong>，如果神经网络的每个权重都被初始化成相同的值，那么每个神经元就会就是计算出相同的结果，在反向传播时也会又相同的梯度，最后导致所有权重的都会有相同的更新，权重之间失去了对称性</p></li><li><p>随机初始化：初始化为一些靠近0的随机数</p></li><li><p>稀疏初始化：将权重全部初始化维0，然后道破对称性在立马随机挑选一些参数附上一些随机值</p></li></ul><h2 id="初始化偏置">初始化偏置</h2><p>一般初始化为全0</p><h2 id="批标准化-Batch-Normalization">批标准化(Batch Normalization)</h2>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 预处理 </tag>
            
            <tag> PCA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分类问题</title>
      <link href="/2022/04/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
      <url>/2022/04/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1>分类问题</h1><h2 id="问题描述">问题描述</h2><p>机器学习中有监督学习主要分为回归问题和分类问题，回归问题希望预测的结果是连续的，为什么不能用之前线性回归的方法来处理分类问题呢？答案其实是可以，不过我们在计算损失函数时需要对预测值远远偏离真实值的对象进行打击。理由也很简单，因为这些过大的偏差会使我们的回归结果向减小这类偏差的方向移动，也就是说距离分类边界太原的数据点会迫使分类边界向其方向移动，如图所示：</p><p><img src="https://s1.328888.xyz/2022/04/18/rr6XA.png" alt="rr6XA.png"></p><p>分类问题所预测的结果是离散的“类别”。这是输入变量可以是离散的也可以是连续的，监督学习从数据中学习一个分类模型或分类决策函数就是分类器，分类器根据输入变量对输出进行预测，即为分类，</p><p>分类问题太好理解了，生活中处处是分类，在此不再赘述。</p><p>分类问题中最简单的自然就是：二分类went</p><h2 id="二分类问题">二分类问题</h2><p>说起二分类问题，第一个想到的自然就是Logistic(对数几率），首先了解以下什么是Logistic分布：</p><h3 id="Logistic分布">Logistic分布</h3><p>服从Logistic分布是指X的分布函数和概率密度函数服从：<br>$$<br>F(x)=P(X\leq x)=\frac{1}{1+e^{-(x-\mu)\gamma}}<br>$$</p><p>$$<br>f(x)=\frac{e^{-(x-\mu)\gamma}}{\gamma(1+e^{-(x-\mu)\gamma})^2}<br>$$</p><p>其中$\mu$影响 <strong>中心对称点位置</strong> ，$\gamma$越小中心点附近的增长速度越开。而Sigmoid函数就是$\mu =0,\gamma=1$的一个特例：</p><p><img src="https://s1.328888.xyz/2022/04/18/r2QAF.png" alt="r2QAF.png"></p><h3 id="对数几率回归">对数几率回归</h3><p>对于二分类Logistic问题，其目标是希望找到一个区分度足够好的决策边界，能够将两类很好的分开。</p><p>​假设输入数据的特征向量$x\in R^n$,那么剧场边界可以表示为$\sum_{i=1}^n w_ix_i+b=0$,假设预测样本的$x_0$,使得$h(x_0)=\sum_{i=1}^n w_ix_i+b&gt;0$,则判断其为1类，反之若$h(x_0)=\sum_{i=1}^n w_ix_i+b&lt;0$,判断其为0类。而Logistic回归要跟进一步，利用比较概率值来判断类别。</p><p>将sigmoid函数应用于分类问题：<br>$$<br>y=\frac{1}{1+e^{({\bf w^Tx}+b)}}<br>$$<br>若将y视为样本x为正例的可能性$p(y=1\vert x)$，则1-y为样本为反例的可能性$p(y=0\vert x)$，用概率更改上述公式为：</p><p>显然：<br>$$<br>p(y=1\vert x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}<br>$$</p><p>$$<br>p(y=0\vert x)=\frac{1}{1+e^{w^Tx+b}}<br>$$</p><p>我们给出一个定义，几率：只一个事件发生的概率与不发生概率的比值，那某事件发生的对数几率就为：<br>$$<br>\ln it §=\ln \frac{p}{1-p}<br>$$</p><p>$$<br>\ln\frac{p(y=1\vert x)}{p(y=0\vert x)}=w^Tx+b<br>$$<br>观察上式等号右边项，只想道一声别来无恙，这不就是线性回归的模型吗？那不就是把线性回归的推到再推一遍呐，不过不一样的是我们此时不需要回归，而是分类，也就是说我们不需要其求出来的Loss最小，而是要似然概率最大。</p><p>什么？你不知道什么是似然概率？那您自己百度吧。简单的说就是根据结果推导分布。</p><p>给定数据集${(x_i,y_i)}_{i=1}^m$,对于每一对数据，预测正确的概率为：<br>$$<br>p(y_i\vert x_i;w,b)=y_ip(\hat y_i=1\vert x;w,b)+(1-y_i)p(\hat y_i=0\vert x;w,b)<br>$$<br>其中 $y_i$分为0，1两类.</p><p>假设其中有n个分类为1，则似然函数为：</p><p>$$<br>\prod _{i=1}^m[p(\hat y_i=1\vert x;w,b)]^n[p(\hat y_i=1\vert x;w,b)]^{m-n}<br>$$</p><p>二项分布嘛，我们找到合适的参数$w,b$使其最大，乘法肯定不好计算，所以我们使用对数简化为加法，即最大化似然对数：<br>$$<br>l(w,b)=\sum_{i=1}^m\ln p(y_i\vert x_i;w,b)<br>$$<br>上式等价于：<br>$$<br>\begin{aligned}<br>l(w,b)&amp;=\sum_{i=1}^mp(y_i\vert  x_i;w,b)\\<br>&amp;=\sum_{i=1}^m\ln \frac{y_i(e^{w^Tx_i+b})+(1-y_i)}{1+e^{w^T  x_i+b}}\\<br>&amp;=\sum_{i=1}^m[ln(y_ie^{w^T  x_i+b}+(1-y_i))-\ln(1+e^{w^T  x_i+b})]\\<br>&amp;=\sum_{i=1}^m(y_i(w^Tx_i+b)-\ln (1+e^{w^T  x_i+b} ))<br>\end{aligned}<br>$$</p><p>最简单就是使用梯度下降法求解上述最大化问题：<br>$$<br>\begin{aligned}<br>\frac{\partial L(w)}{\partial w}&amp;=\sum_{i=1}^my_ix_i-\sum_{i=1}^m\frac{e^{w^Tx_i+b}}{1+e^{w^Tx_i+b}}x_i\\<br>&amp;=\sum_{i=1}^m(y_i-p(y_i=1\vert x_i))x_i<br>\end{aligned}<br>$$</p><p>$$<br>\begin{aligned}<br>\frac{\partial L(w)}{\partial b}&amp;=\sum_{i=1}^my_ix_i-\sum_{i=1}^m\frac{e^{w^Tx_i+b}}{1+e^{w^Tx_i+b}}\\<br>&amp;=\sum_{i=1}^m(y_i-p(y_i=1\vert x_i))<br>\end{aligned}<br>$$</p><p>注意上述公式中$w,x_i$都为列向量，类似多元线性回归中的情形。</p><p>鸢尾花数据集的二分类实现：</p><p><img src="https://s1.328888.xyz/2022/04/18/rhaiq.png" alt="rhaiq.png"></p><p>输入x特征向量为4维,通过PCA降维到二维，二分类</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">import os,sys</span><br><span class="line">import matplotlib.pylab as plt </span><br><span class="line">os.chdir(sys.path[0])</span><br><span class="line">def pca(X,k):#k is the components you want</span><br><span class="line">  #mean of each feature</span><br><span class="line">  n_samples, n_features = X.shape</span><br><span class="line">  mean=np.array([np.mean(X[:,i]) for i in range(n_features)])</span><br><span class="line"> </span><br><span class="line">  norm_X=X-mean #去中心化</span><br><span class="line"></span><br><span class="line">  scatter_matrix=np.dot(np.transpose(norm_X),norm_X)#协方差矩阵</span><br><span class="line"> </span><br><span class="line">  eig_val, eig_vec = np.linalg.eig(scatter_matrix)#协方差矩阵的特征向量和特征值</span><br><span class="line">  eig_pairs = [(np.abs(eig_val[i]), eig_vec[:,i]) for i in range(n_features)]</span><br><span class="line"></span><br><span class="line">  eig_pairs.sort(reverse=True)#由大到小排序</span><br><span class="line"></span><br><span class="line">  feature=np.array([ele[1] for ele in eig_pairs[:k]])#留下k个主要特征</span><br><span class="line"></span><br><span class="line">  data=np.dot(norm_X,np.transpose(feature))</span><br><span class="line">  return data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#数据读取</span><br><span class="line">label_set=lambda x: 1 if x==&#x27;&quot;setosa&quot;&#x27; else 0</span><br><span class="line">with open(&#x27;./iris/iris.txt&#x27;) as f:</span><br><span class="line">    data_list=f.readlines()</span><br><span class="line">    data_list=[i.split(&#x27;\n&#x27;)[0]for i in data_list[1:]]</span><br><span class="line">    data_list=[i.split(&#x27; &#x27;) for i in data_list]</span><br><span class="line">    data=[[float(i[1]),float(i[2]),float(i[3]),float(i[4]),label_set(i[5])] for i in data_list]</span><br><span class="line"></span><br><span class="line">x_data=[[i[0],i[1],i[2],i[3]]for i in data]</span><br><span class="line">y_data=[[i[4]] for i in data]</span><br><span class="line">x_data=pca(np.array(x_data),2)</span><br><span class="line">x_data=torch.from_numpy(x_data).float()</span><br><span class="line">y_data=torch.from_numpy(np.array(y_data)).float()</span><br><span class="line">#模型定义</span><br><span class="line">class Logistic_Regreession(nn.Module):</span><br><span class="line">    def __init__(self) -&gt; None:</span><br><span class="line">        super(Logistic_Regreession,self).__init__()</span><br><span class="line">        self.LR=nn.Linear(2,1)</span><br><span class="line">        self.sm=nn.Sigmoid()</span><br><span class="line">    def forward(self,x):</span><br><span class="line">        y=self.LR(x)</span><br><span class="line">        output=self.sm(y)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">if torch.cuda.is_available():</span><br><span class="line">    x_data=x_data.cuda()</span><br><span class="line">    y_data=y_data.cuda()</span><br><span class="line">    model=Logistic_Regreession().cuda()</span><br><span class="line">else:</span><br><span class="line">    model=Logistic_Regreession()</span><br><span class="line">criterion=nn.MSELoss()</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=1e-3,weight_decay=0.05,momentum=0.78)</span><br><span class="line"></span><br><span class="line">epochs=2000</span><br><span class="line">for epoch in range(epochs):</span><br><span class="line">    output=model(x_data)</span><br><span class="line">    mask=output.ge(0.5).float()</span><br><span class="line"></span><br><span class="line">    correct=(mask==y_data).sum()</span><br><span class="line">    acc=correct/y_data.shape[0]</span><br><span class="line">    loss=criterion(output,y_data)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    if epoch%10==0:</span><br><span class="line">        print(&quot;epoch:&#123;&#125;,loss:&#123;:.6f&#125;,acc:&#123;:.4f&#125;&quot;.format(epoch,loss,acc))</span><br><span class="line">x=x_data.numpy()[:,0]</span><br><span class="line">y=x_data.numpy()[:,1]</span><br><span class="line">x1=[]</span><br><span class="line">y1=[]</span><br><span class="line">y2=[]</span><br><span class="line">x2=[]</span><br><span class="line">for i in range(len(x)):</span><br><span class="line">    if y_data[i]==0:</span><br><span class="line">        x1.append(x[i])</span><br><span class="line">        y1.append(y[i])</span><br><span class="line">    else:</span><br><span class="line">        x2.append(x[i])</span><br><span class="line">        y2.append(y[i])</span><br><span class="line">w=model.LR.weight[0]</span><br><span class="line">w0=w[0].detach().numpy()</span><br><span class="line">w1=w[1].detach().numpy()</span><br><span class="line">b=model.LR.bias.data[0].numpy()</span><br><span class="line">plot_x=np.arange(-2,3,0.01)</span><br><span class="line">plot_y=(-w0*plot_x+b)/w1</span><br><span class="line">plt.scatter(x1,y1,color=&#x27;red&#x27;)</span><br><span class="line">plt.scatter(x2,y2,color=&#x27;blue&#x27;)</span><br><span class="line">plt.plot(plot_x,plot_y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/18/rh3ce.png" alt="rh3ce.png"></p><p><img src="https://s1.328888.xyz/2022/04/19/rz92Q.png" alt="rz92Q.png"></p><h2 id="多分类问题">多分类问题</h2><p>多分类问题一般都是将其简化为多个二分类问题使其得以解决：</p><p>主要做法有以下三类：</p><h3 id="一对一-OvO">一对一(OvO)</h3><p>one vs one:</p><p>给定多分类$y_i \in {C_1,C_2,…C_N}$,n分类问题，OVO将这N个类别两两配对，从而产生$\frac{N(N-1)}{2}$个二分类任务，然后对某一输入做预测时，这$\frac{N(N-1)}{2}$个分类器分别各自作出分类，最终将被预测得最多的类别作为分类结果。参考如下图</p><h3 id="一对其余-OvR">一对其余(OvR)</h3><p>One VS Rest：</p><p>每次将一个类作为正例，其他所有类作为反例来训练N个分类器，在测试时若只有一个分类器作为正类，则将其作为最终分类。</p><p><img src="https://s1.328888.xyz/2022/04/18/rhFtS.png" alt="rhFtS.png"></p><h3 id="多对多-MvM">多对多(MvM)</h3><p>Many VS Many ：</p><p>每次将若干个类作为正类，若干个其他类作为反类。MVM的正反类构造必须有 <strong>特殊的设计</strong>，不能随意选取，常用技术为：纠错输出码’</p><h4 id="纠错输出码-Error-Correcting-Output-Codes">纠错输出码(Error Correcting Output Codes)</h4><p>ECOC将编码思想引入类别拆分：</p><ul><li>编码：对N个类别做M次划分，每次划分将一部分分类化为正类，一部分化为反类，从而形成一个二分类训练集；这样一共产生M个训练集，可以训练出M个分类器</li><li>M个分类器分别对测试样本进行预测，这些预测标记组成一个编码，将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的作为最终预测结果</li></ul><p>类别划分通过&quot;编码矩阵&quot;完成，常用编码矩阵有二元码和三元码，前者将类别分别指定为正类和反类，后者除了正反类之外，还可以指定停用类。</p><p><img src="https://s1.328888.xyz/2022/04/18/rhBZy.png" alt="rhBZy.png"></p><p>纠错输出码有一定的纠错能力，比如图(a)中，正确的预测编码为（-1，+1，+1，-1，+1），但$f_2$分类器出错导致其编码为(-1,-1,+1,-1,+1),但基于该编码仍然正确分类为$C_3$</p><h2 id="类别不平衡问题">类别不平衡问题</h2><p>以上各个分开类的实现前提都是 <strong>我的数据集中各类数据基本相当</strong>，若是出现类别不平衡情况：比如二分类问题中，数据集有998个正例却只有2个反例，这样的数据所训练的分类器没有任何价值，因为它会将所有测试数据预测为正例。</p><p>有办法解决这个问题吗？当然有，记得我们在分类器sigmoid函数输出之后，将其与0.5比较，大于0.5的判为正例，小于0.5的判为反例。为何一定是0.5呢？因为我们假设正反例可能性相同，即分类器比率决策规则为：<br>$$<br>若\frac{y}{1-y}&gt;1,则其为正例<br>$$<br>那既然数据集中正反例观测概率不为0.5，那我们何不改变此规则呢？</p><p>若数据集中正例的个数为$m^+$,反例数为$m^-$,则决策规则更改为：<br>$$<br>若\frac{y}{1-y}&gt;\frac{m^+}{m^-},则其为正例<br>$$<br>等价于：<br>$$<br>若\frac{y’}{1-y’}=\frac{y}{1-y}\times \frac{m^-}{m^+}&gt;1,则其为正例<br>$$<br>这就是所谓的——再放缩</p><p>该方法的思想虽然简单，但是在实现过程中不一定有效，因为我们所得的正反例比率仅仅是通过训练集统计而得，虽然我们假设“训练集是样本总体的无偏采样”，但实际却往往并不成立，也就是说，我们未必能基于训练集观测几率来推测出样本总体的真实几率。</p><p>所以实际中主要有三种办法：</p><ul><li>欠采样：除去一些反例或正例使得正反例数目接近</li><li>过采样：增加一些正例或者反例使得正反例数目接近</li><li>阈值移动：就是在决策时采用上文所述的再缩放方法</li></ul>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 分类问题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> logistic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FuzzyLogicReport2</title>
      <link href="/2022/04/17/FuzzyLogicReport2/"/>
      <url>/2022/04/17/FuzzyLogicReport2/</url>
      
        <content type="html"><![CDATA[<div class="row">    <embed src="https://yangyin.cool/pdf/FuzzyLogic/test.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>模型的保存与加载</title>
      <link href="/2022/04/17/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/"/>
      <url>/2022/04/17/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/</url>
      
        <content type="html"><![CDATA[<h1>模型的保存与加载</h1><h2 id="保存与加载整个模型的结构信息和参数信息">保存与加载整个模型的结构信息和参数信息</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.save(model,&#x27;./model.pth&#x27;)</span><br><span class="line">load_model=torch.load(&#x27;model.pth&#x27;)</span><br></pre></td></tr></table></figure><h2 id="保存与加载整个模型的参数信息">保存与加载整个模型的参数信息</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.save(model.state_dict(),&#x27;./model_state.pth&#x27;)</span><br><span class="line">load_model=model.load_state_dic(torch.load(&#x27;model_state.pth&#x27;))</span><br></pre></td></tr></table></figure><p><mark>load_model加载前需要先实例化</mark></p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPU加速与可视化</title>
      <link href="/2022/04/17/GPU%E5%8A%A0%E9%80%9F%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
      <url>/2022/04/17/GPU%E5%8A%A0%E9%80%9F%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1>GPU与可视化</h1><h2 id="Gpu加速">Gpu加速</h2><p>GPU加速</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device=torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">net=MLP.to(device)</span><br><span class="line">对应数据也需要加入.to(device)或者使用data.cuda（）</span><br></pre></td></tr></table></figure><p><code>.item()</code>：取tensor中的值</p><h2 id="Visdom可视化">Visdom可视化</h2><h3 id="tensorboardX">tensorboardX</h3><p><code>pip install tensorboardX</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer=SummaryWriter()</span><br><span class="line">writer.add_scalar(<span class="string">&#x27;data/scalar_group&#x27;</span>,&#123;<span class="string">&#x27;xsinx&#x27;</span>:n_iter*np.sin(n_iter),<span class="string">&#x27;xcos&#x27;</span>:n_iter*np.cos(n_iter),n_iter&#125;)</span><br><span class="line">writer.add_image(<span class="string">&#x27;Image&#x27;</span>,x,n_iter)</span><br><span class="line">writer.add_text(<span class="string">&#x27;Text&#x27;</span>,<span class="string">&#x27;text logged at step:&#x27;</span>+<span class="built_in">str</span>(n_iter),n_iter)</span><br><span class="line"><span class="keyword">for</span> name,param <span class="keyword">in</span> resnet18.named_parameters():</span><br><span class="line">    writer.add_histogram(name,param.clone().cpu().adta.numpy(),n_iter)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h3 id="Visdom">Visdom</h3><p>运行效率更高</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install visdom</span><br><span class="line">python -m visdom.server</span><br><span class="line">如果运行报错，则重新下载visdom后通过进入文件目录后</span><br><span class="line">pip install -e 安装</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#一条线</span></span><br><span class="line"><span class="keyword">from</span> visdom <span class="keyword">import</span> Visdom</span><br><span class="line">viz=Visdom()</span><br><span class="line">viz.line([<span class="number">0.</span>],[<span class="number">0.</span>],win=<span class="string">&#x27;train_loss&#x27;</span>,opts=<span class="built_in">dict</span>(title=<span class="string">&#x27;train loss&#x27;</span>))<span class="comment">#创建一条直线（y,x,ID,opt:属性配置）</span></span><br><span class="line">viz.line([loss.item()],[global_step],win=<span class="string">&#x27;train_loss&#x27;</span>,update=<span class="string">&#x27;append&#x27;</span>)<span class="comment">#将数据添加到直线中（update操作）</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#多条线</span></span><br><span class="line"><span class="keyword">from</span> visdom <span class="keyword">import</span> Visdom</span><br><span class="line">viz=Visdom()</span><br><span class="line">viz.line([<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">0.0</span>,<span class="number">0.0</span>],win=<span class="string">&#x27;test&#x27;</span>,opts=<span class="built_in">dict</span>(title=<span class="string">&#x27;test loss&amp;acc&#x27;</span>，legend=[<span class="string">&#x27;loss&#x27;</span>,<span class="string">&#x27;acc&#x27;</span>]))<span class="comment">#创建两条直线（[y1,y2],x,ID,opt:属性配置）</span></span><br><span class="line">viz.line([[test_loss,cprrect/<span class="built_in">len</span>(test_loader.dataset)]],[global_step],win=<span class="string">&#x27;test&#x27;</span>,update=<span class="string">&#x27;append&#x27;</span>)<span class="comment">#将数据添加到直线中（update操作）</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#可视化image与pred</span><br><span class="line">from visdom import Visdom</span><br><span class="line">viz=Visdom()</span><br><span class="line">viz.images(data.reshape(-1,1,28,28),win=&#x27;x&#x27;)</span><br><span class="line">viz.text(str(pred.detach().cpu().numpy),win=&#x27;pred&#x27;,opts=dict(title=&#x27;pred&#x27;))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> visdom </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性模型</title>
      <link href="/2022/04/17/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/2022/04/17/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1>线性模型</h1><h2 id="问题描述-2">问题描述</h2><p><img src="https://s1.328888.xyz/2022/04/17/reHbW.png" alt="线性回归"></p><p>通俗点讲，就是给一堆点，找到一条直线使所有点到直线的距离之和最小。数学描述是给定由d个属性描述的示例 ${\bf x}=(x_1,x_2,…,x_d)$,其中$x_i$表示第i个属性上的取值，线性模型试图学得一个通过属性线性组合来进行预测的函数：<br>$$<br>f(x)=w_1x_1+w_2x_2+…+w_dx_d+b<br>$$<br>写成向量形式：<br>$$<br>f(x)={\bf w^Tx}+b<br>$$<br>其中${\bf w}=(w_1,…,w_d)$与$b$ 需要由学习所得</p><h3 id="广义线性回归：">广义线性回归：</h3><p>考虑可微单调函数<code>g(·)</code><br>$$<br>y=g^{-1}(w^Tx)+b<br>$$</p><h2 id="一维线性回归">一维线性回归</h2><p>给定数据集$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$,要使的$f(x_i)=w_i+b$能够尽量与$y_i$接近</p><p>loss：<br>$$<br>Loss=\sum_{i=1}^m(f(x_i)-y_i)^2<br>$$<br>我们需要找到:</p><p>$$<br>\begin{aligned}<br>(w^*,b^*)&amp;=arg\min_{w,b}\sum_{i=1}^m(f(x_i)-y_i)^2\\<br>&amp;=arg\min_{w,b}\sum_{i=1}^m(y_i-wx_i-b)^2<br>\end{aligned}<br>$$</p><p>令其偏导等于0：<br>$$<br>\frac{\partial Loss_{(w,b)}}{\partial w}=2(w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i)=0<br>$$</p><p>$$<br>\frac{\partial Loss_{(w,b)}}{\partial b}=2(mb-\sum_{i=1}^m(y_i-wx_i))=0<br>$$</p><p>可得：<br>$$<br>w=\frac{\sum_{i=1}^my_i(x_i-\bar x)}{\sum_{i=1}^mx_i^2-\frac{1}{m}(\sum_{i=1}^mx_i)^2}<br>$$</p><p>$$<br>b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i)<br>$$</p><h2 id="多维线性回归">多维线性回归</h2><p>$$<br>f(x)={\bf w^Tx}+b<br>$$</p><p>将数据集表示为$m\times (d+1)$ 的矩阵形式：<br>$$<br>X=<br>\begin{pmatrix}<br>{x_{11}}&amp;{x_{12}}&amp;{\cdots}&amp;{x_{1d}}&amp;1\\<br>{x_{21}}&amp;{x_{22}}&amp;{\cdots}&amp;{x_{2d}}&amp;1\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}&amp;{\vdots}\\<br>{x_{m1}}&amp;{x_{m2}}&amp;{\cdots}&amp;{x_{md}}&amp;1<br>\end{pmatrix}<br>=\begin{pmatrix}<br>{x_1^T}&amp;1\\<br>{x_2^T}&amp;1\\<br>{\vdots}&amp;{\vdots}\\<br>{x_m^T}&amp;1<br>\end{pmatrix}<br>$$<br>目标y也写成向量形式$y=(y_1,y_2,…,y_m)$,则：<br>$$<br>w^*=(w_1,w_2,…,w_d,b)<br>$$</p><p>$$<br>w^*=arg\min_w(y-Xw)^T(y-Xw)<br>$$<br>矩阵求导可参考<a href="https://zhuanlan.zhihu.com/p/273729929">矩阵求导公式的数学推导（矩阵求导——基础篇） - 知乎 (zhihu.com)</a></p><p>对其求导：<br>$$<br>\frac{\partial Loss_{(w,b)}}{\partial w}=2X^T(Xw-y)=0<br>$$<br>若$X^TX$为满秩矩阵，则：<br>$$<br>w^*=(X^TX)^{-1}X^Ty<br>$$<br>此时：<br>$$<br>f(\hat x_i)=\hat x_i^T(X^TX)^{-1}X^Ty<br>$$</p><p>然而实际情况是，$X^TX$往往不可逆，此时可以解出多个$\hat w$, 他们都能使均方误差最小化，选择哪一个作为输出将由算法的归纳偏好决定，常见做法是引入正则化。</p><h2 id="pytorch一维线性回归代码实现">pytorch一维线性回归代码实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> turtle <span class="keyword">import</span> forward</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备</span></span><br><span class="line">x_train=np.array([[<span class="number">3.3</span>],[<span class="number">4.4</span>],[<span class="number">5.5</span>],[<span class="number">6.71</span>],[<span class="number">6.93</span>],[<span class="number">4.168</span>],[<span class="number">9.779</span>],[<span class="number">6.182</span>],[<span class="number">7.59</span>],[<span class="number">2.167</span>],[<span class="number">7.042</span>],[<span class="number">10.791</span>],[<span class="number">5.313</span>],[<span class="number">7.997</span>],[<span class="number">3.1</span>]],dtype=np.float32)</span><br><span class="line">y_train=np.array([[<span class="number">1.7</span>],[<span class="number">2.76</span>],[<span class="number">2.09</span>],[<span class="number">3.19</span>],[<span class="number">1.694</span>],[<span class="number">1.573</span>],[<span class="number">3.366</span>],[<span class="number">2.596</span>],[<span class="number">2.53</span>],[<span class="number">1.221</span>],[<span class="number">2.827</span>],[<span class="number">3.465</span>],[<span class="number">1.65</span>],[<span class="number">2.904</span>],[<span class="number">1.3</span>]],dtype=np.float32)</span><br><span class="line">point=plt.scatter(x_train,y_train)</span><br><span class="line">x_train=torch.from_numpy(x_train)</span><br><span class="line">y_train=torch.from_numpy(y_train)</span><br><span class="line"><span class="comment"># 模型定义y=wx+b</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegreession</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(LinearRegreession,self).__init__() <span class="comment">#调用父类的初始化方法</span></span><br><span class="line">        self.linear=nn.Linear(<span class="number">1</span>,<span class="number">1</span>)<span class="comment">#输入输出都为1维</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        out=self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model=LinearRegreession().cuda() <span class="comment">#如果有GPU,则加载到GPU上运行</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model=LinearRegreession()</span><br><span class="line"></span><br><span class="line"><span class="comment">#LOSS函数</span></span><br><span class="line">criterion=nn.MSELoss()</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=<span class="number">1e-3</span>)<span class="comment">#梯度下降算法</span></span><br><span class="line"></span><br><span class="line">epochs=<span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">#判断是否使用GPU</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        inputs=x_train.cuda()</span><br><span class="line">        target=y_train.cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inputs=x_train</span><br><span class="line">        target=y_train</span><br><span class="line">    <span class="comment">#forward</span></span><br><span class="line">    out=model(inputs)</span><br><span class="line">    loss=criterion(out,target)</span><br><span class="line">    <span class="comment">#backward</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    loss.backward()<span class="comment">#求导</span></span><br><span class="line">    optimizer.step()<span class="comment">#反向更新w与b</span></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>)%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch[&#123;&#125;/&#123;&#125;],loss:&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>,epochs,loss))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">predict=model(x_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(predict))</span><br><span class="line">predict=predict.detach().numpy()</span><br><span class="line">plt.plot(x_train.numpy(),y_train.numpy(),<span class="string">&#x27;ro&#x27;</span>,label=<span class="string">&#x27;Original data&#x27;</span>)</span><br><span class="line">plt.plot(x_train.numpy(),predict,label=<span class="string">&#x27;fitting line&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/17/ryT8g.png" alt="结果.png"></p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 线性模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>激活函数与loss函数</title>
      <link href="/2022/04/13/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8ELoss%E5%87%BD%E6%95%B0/"/>
      <url>/2022/04/13/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8ELoss%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1>激活函数</h1><p><img src="https://s1.328888.xyz/2022/04/13/fx5H4.png" alt="fx5H4.png"><br>$$<br>y=f(\sum_{i=1}^nw_ix_i+b)<br>$$<br>此f()为激活函数，输入为前一层所有输出的加权和再加上一个偏执b</p><h2 id="sigmoid">sigmoid</h2><p>$$<br>f(x)=\sigma(x)=\frac{1}{1+ e^x}\<br>$$</p><p>$$<br>\frac{\partial f(x)}{\partial x}=f(x)(1-f(x))<br>$$</p><p>优点：求导简单，数据可压缩到（0，1）</p><p>缺点：梯度离散，输入很大时输出太平缓</p><p>调用指令<code>torch.sigmoid()</code></p><h2 id="Tanh">Tanh</h2><p>$$<br>f(x)=tanh(x)=\frac{(e^x-e^{-x})}{(e^x+e^{-x})}<br>=2sigmoid(2x)-1<br>$$</p><p>$$<br>\frac{\partial f(x)}{\partial x}=1-tanh^2(x)<br>$$</p><p>调用指令<code>torch.tanh()</code></p><h2 id="ReLU">ReLU</h2><p>$$<br>f(x)=\begin{cases}0\ for\ x&lt;0 \\<br>x\ for\ x\geq0\end{cases}<br>$$</p><p>调用指令<code>torch.relu()</code></p><h1>Loss函数</h1><h2 id="MSE">MSE</h2><p>$$<br>loss_i=\sum(y_i-\bar y_i)^2=norm((y_i-\hat y_i),2)^2<br>$$</p><p>调用指令 torch.nn.functional.mse_loss($y,\hat y$')</p><p>自动求导:</p><p>​<strong>方法一</strong><br>1. 首先对需要求导的参数使用<code>requires_grad_()</code>方法标明需要求导<br>2. 计算mse：torch.nn.functional.mse_loss($y$,$\hat y$)<br>3. 使用<code>torch.autograd.grad(mse,[w])</code>对其进行求导</p><p>​<strong>方法二</strong></p><ol><li><p>首先对需要求导的参数使用<code>requires_grad_()</code>方法标明需要求导</p></li><li><p>计算mse：torch.nn.functional.mse_loss($y，\hat y$)</p></li><li><p>调用<code>mse.backward</code>该指令会计算mse对所有已设置需要求导变量的梯度</p></li><li><p>调用<code>w.grad</code>显示梯度</p><p><mark>backward设置(retain_graph=True)才可以再一次调用，不设置则会报错</mark></p></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">x=torch.ones(<span class="number">1</span>)</span><br><span class="line">w=torch.Tensor([<span class="number">2</span>])</span><br><span class="line">w.requires_grad_()</span><br><span class="line">mse=F.mse_loss(torch.ones(<span class="number">1</span>),x*w)</span><br><span class="line"><span class="comment">#torch.autograd.grad(mse,[w])</span></span><br><span class="line">mse.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([2.])</span></span><br></pre></td></tr></table></figure><h2 id="Softmax">Softmax</h2><p>$$<br>softmax(y_i)=\frac{e^{y_i}}{\sum_ je^{y_j}} \<br>$$</p><p>$$<br>\frac{\partial softmax(y_i)}{\partial y_j}=\begin{cases}softmax(y_i)(1-softmax(y_j))\ \ if\  i=j\\<br>-softmax(y_i)softmax(y_j)\ \ \ \ \ \ \ \ \ if\ i\neq j\end{cases}<br>$$</p><p>优点：将输出label的probability压缩到（0，1），且所有probability之和为1，原数据间隔拉大</p><p>调用指令 <code>torch.nn.functional.mse_loss(y,dim=x)</code></p><h2 id="交叉熵">交叉熵</h2><p>$$<br>H(P,Q)=-\sum_{i=1}^nP(i)logQ(i)<br>$$</p><p>通常用于分类问题</p><p>调用指令 <code>torch.nn.functional.cross_entropy(logits,y)</code></p><p>或者</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred=torch.nn.functional.softmax(logits,dim=<span class="number">1</span>)</span><br><span class="line">pred_log=torch.nn.functional.log(pred)</span><br><span class="line">torch.nn.functional.nll_loss(pred_log,y)</span><br></pre></td></tr></table></figure><p><mark>cross_entropy=softmax→log→nll_loss</mark></p><h2 id="一个二元二次函数梯度下降求极值算法">#一个二元二次函数梯度下降求极值算法</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#一个二元二次函数梯度下降求极值算法</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">himmelblau</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span>(x[<span class="number">0</span>]**<span class="number">2</span>+x[<span class="number">1</span>]-<span class="number">11</span>)**<span class="number">2</span>+(x[<span class="number">0</span>]+x[<span class="number">1</span>]**<span class="number">2</span>-<span class="number">7</span>)**<span class="number">2</span></span><br><span class="line">x=np.arange(-<span class="number">6</span>,<span class="number">6</span>,<span class="number">0.1</span>)</span><br><span class="line">y=np.arange(-<span class="number">6</span>,<span class="number">6</span>,<span class="number">0.1</span>)</span><br><span class="line">X,Y=np.meshgrid(x,y)</span><br><span class="line">Z=himmelblau([X,Y])</span><br><span class="line"></span><br><span class="line">fig=plt.figure(<span class="string">&quot;himelblau&quot;</span>)</span><br><span class="line">ax=fig.gca(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(X,Y,Z)</span><br><span class="line">ax.view_init(<span class="number">60</span>,-<span class="number">30</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.show()<span class="comment">#画出图像</span></span><br><span class="line">x=torch.tensor([<span class="number">4.</span>,<span class="number">0.</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line">optimizer=torch.optim.Adam([x],lr=<span class="number">1e-3</span>)<span class="comment">#lr:learning rate 实例化反向传播算法优化器</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line">    pred=himmelblau(x)</span><br><span class="line">    optimizer.zero_grad()<span class="comment">#参数梯度置为0</span></span><br><span class="line">    pred.backward()</span><br><span class="line">    optimizer.step()<span class="comment">#执行梯度下降</span></span><br><span class="line">    <span class="keyword">if</span> step%<span class="number">2000</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;step&#123;&#125;:x=&#123;&#125;,f(x)=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(step,x.tolist(),pred.item()))</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/13/fP7Oi.png" alt="fP7Oi.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#运算结果</span><br><span class="line">step0:x=[3.999000072479248, -0.0009999999310821295],f(x)=34.0</span><br><span class="line">step2000:x=[3.5741987228393555, -1.764183521270752],f(x)=0.09904692322015762</span><br><span class="line">step4000:x=[3.5844225883483887, -1.8481197357177734],f(x)=2.1100277081131935e-09</span><br><span class="line">step6000:x=[3.5844264030456543, -1.8481241464614868],f(x)=2.41016095969826e-10</span><br><span class="line">step8000:x=[3.58442759513855, -1.848125696182251],f(x)=2.9103830456733704e-11</span><br><span class="line">step10000:x=[3.584428310394287, -1.8481262922286987],f(x)=9.094947017729282e-13</span><br><span class="line">step12000:x=[3.584428310394287, -1.8481265306472778],f(x)=0.0</span><br><span class="line">step14000:x=[3.584428310394287, -1.8481265306472778],f(x)=0.0</span><br><span class="line">step16000:x=[3.584428310394287, -1.8481265306472778],f(x)=0.0</span><br><span class="line">step18000:x=[3.584428310394287, -1.8481265306472778],f(x)=0.0</span><br></pre></td></tr></table></figure><h2 id="一个交叉熵多分类问题">##一个交叉熵多分类问题</h2><p><img src="https://s1.328888.xyz/2022/04/13/fxuiq.png" alt="网络结构"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">##一个交叉熵多分类问题</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>过拟合与欠拟合</title>
      <link href="/2022/04/13/%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88/"/>
      <url>/2022/04/13/%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88/</url>
      
        <content type="html"><![CDATA[<h2 id="过拟合与欠拟合">过拟合与欠拟合</h2><p>欠拟合：训练集和测试集的acc都很差 可以适当增加模型复杂度再测试</p><p>过拟合：使用模型复杂度高于实际数据模型复杂度 训练多次后training acc很好，test acc效果不好 泛化能力变差</p><h3 id="交叉验证">交叉验证</h3><p>每训练多少次做一次test，只保存test acc 最好的模型</p><p>overfitting之后的模型都不会保存</p><h4 id="K-fold-cross-validation">K-fold cross-validation</h4><p>每次训练完以后重新划分Train Set 与Val Set</p><p>把最开始的Trainning set划分为K份，每次取K-1份做为Trainning Set 另外一份作为Val Set</p><h2 id="减少过拟合">减少过拟合</h2><p>增大数据集</p><p>减少模型复杂度</p><p>Dropout</p><p>Data argume</p><h3 id="Regularization">Regularization</h3><p>$$<br>J(\theta)=-\frac{1}{m}\sum_{i=1}^m[y_i\ln\hat y_i+(1-y_i)\ln(1-\bar y^i)]+\lambda \vert\theta_i\vert<br>$$</p><p>式子前半部分为交叉熵，后半部分为$\lambda$乘上网络参数的1或2范数，因为矩阵的1，2范数大于等于0，当我们对$J(\theta)$做最优化使其最小时，也相当于最优化$\vert\theta_i\vert$减小接近为0，达到简化网络的目的：高维参数接近为0，低维参数保持。</p><p>常用L2-Regularization：<br>$$<br>J(W;X,y)+\frac{1}{2}\Vert w\Vert^2<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer=optim.SGD(net.parameters(),lr=learn_rate,weight_decay=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>使用时只需在.optim时给定参数weight_decay=$\lambda$</p><p>L1-Regularization：pytorch 没有api支持，需要人为编写</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">regularization_loss=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">regularization_loss+=torch.<span class="built_in">sum</span>(torch.<span class="built_in">abs</span>(param))</span><br><span class="line"></span><br><span class="line">classify_loss=criteon(logits,target)</span><br><span class="line">loss=classify_loss+<span class="number">0.01</span>*regularzation_loss</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><h3 id="动量与学习率衰减">动量与学习率衰减</h3><h4 id="Momentum：用于减少停止在局部最优点的情况">Momentum：用于减少停止在局部最优点的情况</h4><p>原梯度更新公式：<br>$$<br>w^{k+1}=w^k-\alpha\nabla f(w^k)<br>$$<br>Momentum梯度更新公式：<br>$$<br>z^{k+1}=\beta z^k+\nabla f(w^k)<br>$$</p><p>$$<br>w^{k+1}=w^k-\alpha z^{k+1}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer=optim.SGD(net.parameters(),lr=learn_rate,momentum=<span class="number">0.78</span>,weight_decay=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>pytorch中调用只需在.optim中设置momentum=$\beta$即可</p><h4 id="Learning-rate-tunning：减少收敛点附近震荡">Learning rate tunning：减少收敛点附近震荡</h4><p>lr随着迭代的进行不断减小</p><p>Scheme1：<code>ReduceLROnPlateau(optimizer,'min')</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer=optim.SGD(net.parameters(),lr=learn_rate,momentum=<span class="number">0.78</span>,weight_decay=<span class="number">0.01</span>)</span><br><span class="line">scheduler=ReduceLROnPlateau(optimizer,<span class="string">&#x27;min&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(args.start_epoch,args.epochs):</span><br><span class="line">    train(train_loader,model,criterion,optimizer,epoch)</span><br><span class="line">    result_avg,loss_val=validate(val_loader,model,criterion,epoch)</span><br><span class="line">    scheduler.step(loss_val)</span><br></pre></td></tr></table></figure><p><code>scheduler.step(loss_val)</code>：loss函数连续一定不减小的话，就衰减lr</p><p>Scheme2:设置每30个epoch，lr=0.1*lr一次</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scheduler=StepLR(optimizer,step_size=30,gamma=0.1)</span><br><span class="line">for epoch in range(100):</span><br><span class="line">scheduler.step()</span><br><span class="line">train()</span><br><span class="line">validate()</span><br></pre></td></tr></table></figure><h3 id="Early-stopping：">Early stopping：</h3><p>train performance 还在上升，validation performance已经保持不变或者下降，则 <strong>early stoppping</strong>，保存Validation performance最大的模型</p><h3 id="Dropout：">Dropout：</h3><p>train前向传播时，每个connection有一定概率的输出为0，即令$w_ix_i=0$</p><p>可在任意连接层中添加<code>torch.nn.Dropout(dropout_prob)</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net_dropped=torch.nn.Sequential(</span><br><span class="line">torch.nn.Linear(<span class="number">784</span>,<span class="number">200</span>),</span><br><span class="line">torch.nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">torch.nn.ReLU(),</span><br><span class="line">torch.nn.Linear(<span class="number">200</span>,<span class="number">200</span>),</span><br><span class="line">torch.nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">torch.nn.ReLU(),</span><br><span class="line">torch.nn.Linear(<span class="number">200</span>,<span class="number">10</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><mark>validation 时不用Dropouot</mark></p><h2 id="梯度下降法的变式">梯度下降法的变式</h2><h3 id="SGD-Stochastic-Gradient-Descent">SGD: Stochastic Gradient Descent</h3><p>对于大数据集，由于显存有限，训练时不可能对所有数据求LOSS后进行梯度更新，所以每次随机选择一个batch的数据进行训练。</p><h3 id="Adagrad">Adagrad</h3><p>自衰减的学习率在某些情况下并不好，可能会造成学习过早停止，Adgrad是一种自学习方法，其参数更新公式为：<br>$$<br>w^{t+1}=w^t-\frac{\beta}{\sqrt{\sum_{i=0}^t(g^i)^2}+\xi}g^t<br>$$<br>其中$\xi$为平滑参数，大小通常为$10^{-4}-10^{-8}$主要用于防止分母为0,分母中的根号特别重要，没有该根号，算法表现非常差。</p><h3 id="RMSprop">RMSprop</h3><p>一种非常有效的自适应学习率改进方法：<br>$$<br>cache^t=\alpha * cache^{t-1}+(1-\alpha)(g^t)^2<br>$$</p><p>$$<br>w^{t+1}=w^t-\frac{\beta}{\sqrt{cache^t}+\xi}g^t<br>$$</p><p>RMSprop不在将前面所有梯度求平方和，二十引用了一个衰减率将其变小，采用一种滑动平均的方式，越靠近前面的梯度对自适应学习的影响率越小，能更快的收敛。</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> SGD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensor基本运算</title>
      <link href="/2022/04/13/%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/"/>
      <url>/2022/04/13/%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<h2 id="Tensor基本运算">Tensor基本运算</h2><h3 id="矩阵相乘">矩阵相乘</h3><p><a href="http://torch.mm">torch.mm</a>：只适合矩阵 dim=2情形</p><p>torch.matmul：适用任何形式</p><p>@：简便写法</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a=torch.rand(<span class="number">4</span>,<span class="number">784</span>)</span><br><span class="line">&gt;&gt;&gt;x=torch.rand(<span class="number">512</span>,<span class="number">784</span>)</span><br><span class="line">&gt;&gt;&gt;(a@x.t()).shape</span><br><span class="line"><span class="comment">#torch.Size([4, 512])</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a=torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">64</span>)</span><br><span class="line">&gt;&gt;&gt;b=torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">&gt;&gt;&gt;torch.matmul(a,b).shape</span><br><span class="line"><span class="comment">#torch.Size([4, 3, 28, 32])</span></span><br></pre></td></tr></table></figure><h3 id="乘方">乘方</h3><p>power</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a=torch.full([<span class="number">2</span>,<span class="number">2</span>],<span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt;a.<span class="built_in">pow</span>(<span class="number">2</span>)</span><br><span class="line"><span class="comment">#tensor([[9, 9],</span></span><br><span class="line"><span class="comment">#        [9, 9]])</span></span><br></pre></td></tr></table></figure><h3 id="取整">取整</h3><p>.floor()：向下取整</p><p>.ceil()：向上取整</p><p>.trunc()：取小数</p><p>.frac()：取整数</p><p>.round()：四舍五入</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.tensor(<span class="number">3.14</span>)</span><br><span class="line">a.floor(),a.ceil(),a.trunc(),a.frac(),a.<span class="built_in">round</span>()</span><br><span class="line"><span class="comment">#(tensor(3.), tensor(4.), tensor(3.), tensor(0.1400), tensor(3.))</span></span><br></pre></td></tr></table></figure><h3 id="裁剪">裁剪</h3><p>.clamp()：输入参数<code>min</code> ：将小于min的数都置为min</p><p>​ 输入参数<code>(min,max)</code>：将小于min的数都置为min，大于max的数都置为max</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">3</span>)*<span class="number">15</span></span><br><span class="line">a.clamp(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line"><span class="comment">#tensor([[10.0000, 10.0000,  2.5097],</span></span><br><span class="line"><span class="comment">#         [10.0000,  1.2573,  8.4877]])</span></span><br></pre></td></tr></table></figure><h3 id="自动求导">自动求导:</h3><p><strong>方法一</strong><br>1. 首先对需要求导的参数使用<code>requires_grad_()</code>方法标明需要求导<br>2. 计算mse：torch.nn.functional.mse_loss($y$,$\hat y$)<br>3. 使用<code>torch.autograd.grad(mse,[w])</code>对其进行求导</p><p><strong>方法二</strong></p><ol><li><p>首先对需要求导的参数使用<code>requires_grad_()</code>方法标明需要求导</p></li><li><p>计算mse：torch.nn.functional.mse_loss($y，\hat y$)</p></li><li><p>调用<code>mse.backward</code>该指令会计算mse对所有已设置需要求导变量的梯度</p></li><li><p>调用<code>w.grad</code>显示梯度</p><p><mark>backward设置(retain_graph=True)才可以再一次调用，不设置则会报错</mark></p></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">x=torch.ones(<span class="number">1</span>)</span><br><span class="line">w=torch.Tensor([<span class="number">2</span>])</span><br><span class="line">w.requires_grad_()</span><br><span class="line">mse=F.mse_loss(torch.ones(<span class="number">1</span>),x*w)</span><br><span class="line"><span class="comment">#torch.autograd.grad(mse,[w])</span></span><br><span class="line">mse.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([2.])</span></span><br></pre></td></tr></table></figure><h2 id="Tensor统计属性">Tensor统计属性</h2><h3 id="范数">范数</h3><p><img src="https://s1.328888.xyz/2022/04/13/fNR54.png" alt="范数"></p><p>.norm§：求矩阵的 <strong>p</strong> 范数</p><p>.norm(p,dim=x):在 <strong>x</strong> 维度上做p范数，输出shape为除了原维度去掉x维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">b = a.reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">b.norm(<span class="number">1</span>,dim=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#tensor([0.3336, 0.0033, 0.5679, 0.7974, 0.1241, 0.4108, 0.2766, 0.8038])</span></span><br><span class="line"><span class="comment">#tensor([[[0.3336, 0.0033],</span></span><br><span class="line"><span class="comment">#         [0.5679, 0.7974]],</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#        [[0.1241, 0.4108],</span></span><br><span class="line"><span class="comment">#         [0.2766, 0.8038]]])</span></span><br><span class="line"><span class="comment">#tensor([[0.4577, 0.4141],</span></span><br><span class="line"><span class="comment">#        [0.8445, 1.6012]])</span></span><br></pre></td></tr></table></figure><h3 id="统计属性">统计属性</h3><p>.prod()：累乘</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.prod()</span><br><span class="line"><span class="comment">#tensor(0.0008)</span></span><br></pre></td></tr></table></figure><p>.argmax（）：返回最大元素的索引，该索引是tensor打平为1维的索引</p><p>.argmin（）：返回最小元素的索引，该索引是tensor打平为1维的索引</p><p>.argmax（dim=x）：返回最大元素的索引，该索引是 <strong>x维度上</strong> 的索引</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.argmax()</span><br><span class="line"><span class="comment"># tensor([[[0.2517, 0.9526, 0.5908],</span></span><br><span class="line"><span class="comment">#          [0.1431, 0.3951, 0.5676]],</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [[0.7481, 0.8191, 0.4051],</span></span><br><span class="line"><span class="comment">#          [0.7140, 0.4541, 0.5540]]])</span></span><br><span class="line"><span class="comment"># tensor(1)</span></span><br><span class="line">a = torch.rand([<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.argmax(dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># tensor([[[0.0630, 0.4025, 0.8124],</span></span><br><span class="line"><span class="comment">#          [0.2175, 0.4514, 0.5231]],</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [[0.8366, 0.4124, 0.6334],</span></span><br><span class="line"><span class="comment">#          [0.3470, 0.0701, 0.2093]]])</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 0],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0]])</span></span><br></pre></td></tr></table></figure><p>keepdim=True :返回的tensor与原tensor维度一样</p><h3 id="TOPK与K-TH">TOPK与K-TH</h3><p>.topk(k,dim=x,largest=true): largest=true返回x维度上最大的k个值，largest=false返回x维度上最小的k个值，输出第一个参数为其值，第二个参数维其索引</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.topk(<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.2393, 0.7239, 0.3985, 0.5578],</span></span><br><span class="line"><span class="comment">#         [0.8645, 0.0815, 0.7446, 0.3979],</span></span><br><span class="line"><span class="comment">#         [0.6933, 0.7192, 0.4393, 0.2296],</span></span><br><span class="line"><span class="comment">#         [0.1022, 0.7430, 0.6715, 0.9983]])</span></span><br><span class="line"><span class="comment"># torch.return_types.topk(</span></span><br><span class="line"><span class="comment"># values=tensor([[0.7239, 0.5578, 0.3985],</span></span><br><span class="line"><span class="comment">#         [0.8645, 0.7446, 0.3979],</span></span><br><span class="line"><span class="comment">#         [0.7192, 0.6933, 0.4393],</span></span><br><span class="line"><span class="comment">#         [0.9983, 0.7430, 0.6715]]),</span></span><br><span class="line"><span class="comment"># indices=tensor([[1, 3, 2],</span></span><br><span class="line"><span class="comment">#         [0, 2, 3],</span></span><br><span class="line"><span class="comment">#         [1, 0, 2],</span></span><br><span class="line"><span class="comment">#         [3, 1, 2]]))</span></span><br></pre></td></tr></table></figure><p>.kthvalue(k,dim=x)：返回由小到大第k个值及其索引</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.kthvalue(<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.4287, 0.7747, 0.8699, 0.7784],</span></span><br><span class="line"><span class="comment">#         [0.1043, 0.4982, 0.5863, 0.3341],</span></span><br><span class="line"><span class="comment">#         [0.1408, 0.0510, 0.4056, 0.9592],</span></span><br><span class="line"><span class="comment">#         [0.3366, 0.1080, 0.8596, 0.3885]])</span></span><br><span class="line"><span class="comment"># torch.return_types.kthvalue(</span></span><br><span class="line"><span class="comment"># values=tensor([0.7784, 0.4982, 0.4056, 0.3885]),</span></span><br><span class="line"><span class="comment"># indices=tensor([3, 1, 2, 3]))</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="高阶操作">高阶操作</h2><h3 id="torch-where">torch.where</h3><p>torch.where(condition,x,y)→Tensor</p><p>$$<br>out_i=\begin{cases}x_i\ \ if\ condition_i\\y_i\ \ otherwise\end{cases}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.zeros([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">b=torch.ones([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">condition=torch.rand([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(condition)</span><br><span class="line">torch.where(condition&gt;<span class="number">0.5</span>,a,b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.5633, 0.7544, 0.6521, 0.6338],</span></span><br><span class="line"><span class="comment">#         [0.5439, 0.5644, 0.6126, 0.1168],</span></span><br><span class="line"><span class="comment">#         [0.6247, 0.4382, 0.4246, 0.2221],</span></span><br><span class="line"><span class="comment">#         [0.0017, 0.7347, 0.6782, 0.9357]])</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0., 1.],</span></span><br><span class="line"><span class="comment">#         [0., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 0., 0., 0.]])</span></span><br></pre></td></tr></table></figure><h3 id="torch-gather">torch.gather</h3><p>torch.gather(input,dim,index)→Tensor：根据将index的第dim维作为索引查取input中对应元素并生成Tensor输出<br>$$<br>input=\begin{bmatrix}cat\\dog\\fish\end{bmatrix}\ \ dim=0\ \ index=\begin{bmatrix}1\\2\\0\end{bmatrix}\Rightarrow\ \ out=\begin{bmatrix}dog\\fish\\cat\end{bmatrix}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.rand(<span class="number">4</span>,<span class="number">10</span>)</span><br><span class="line">a1=a.topk(<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line">i=a1[<span class="number">1</span>]</span><br><span class="line">b=torch.arange(<span class="number">10</span>)+<span class="number">100</span></span><br><span class="line">torch.gather(b.expand(<span class="number">4</span>,<span class="number">10</span>),dim=<span class="number">1</span>,index=i)</span><br><span class="line"><span class="comment"># tensor([[100, 105, 101],</span></span><br><span class="line"><span class="comment">#         [101, 105, 108],</span></span><br><span class="line"><span class="comment">#         [107, 104, 100],</span></span><br><span class="line"><span class="comment">#         [101, 107, 106]])</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensor数据类型</title>
      <link href="/2022/04/12/Tensor%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>/2022/04/12/Tensor%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1>Pytorch</h1><p><font size=4>本学习笔记基于<a href="https://www.bilibili.com/video/BV1J44y1i734?spm_id_from=333.337.search-card.all.click">【深度学习Pytorch入门】5天从Pytorch入门到实战！PyTorch深度学习快速入门教程 150全集 绝对通俗易懂（深度学习框架/神经网络）_哔哩哔哩_bilibili</a></font></p><p>Tensorflow：静态图优先</p><p>Pytorch：动态图优先</p><h2 id="Tensor数据类型">Tensor数据类型</h2><h3 id="数据类型">数据类型</h3><table><thead><tr><th>类型</th><th>类型</th></tr></thead><tbody><tr><td>32位浮点型</td><td>(默认)torch.FloatTensor</td></tr><tr><td>64位浮点型</td><td>torch.DoubleTensor</td></tr><tr><td>16位整型</td><td>torch.shortTensor</td></tr><tr><td>32位整型</td><td>torch.IntTensor</td></tr><tr><td>64位整型</td><td>torch.LongTensor</td></tr></tbody></table><h3 id="维度DIM">维度DIM</h3><p>Tensor：张量，可以理解为任意维度的矩阵</p><p><img src="https://s1.328888.xyz/2022/04/12/fREG1.png" alt="fREG1.png"></p><p><mark>Pytorch 没有string类型，其句子用编码one-bot or enbeding 向量表示</mark></p><p>Dim0(标量)：<code>torch.tensor(1.3)</code> 即生成了一个值为1.3的变量 注意 ：<strong>1.3为0维标量 [1.3]为1维矢量</strong></p><p>​通常应用于loss计算</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.tensor(<span class="number">1.2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([])<span class="comment">#空的矩阵，即0维矢量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(a.shape)</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.size()</span><br><span class="line">torch.Size([])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure><p>Dim1(向量)：通常应用于节点输入bias 或者是Linear Input</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.1</span>])</span><br><span class="line">tensor([<span class="number">1.1000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.1</span>,<span class="number">2.1</span>])</span><br><span class="line">tensor([<span class="number">1.1000</span>, <span class="number">2.1000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.FloatTensor(<span class="number">1</span>)</span><br><span class="line">tensor([-<span class="number">1.0842e-19</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.FloatTensor(<span class="number">2</span>)</span><br><span class="line">tensor([<span class="number">0.0000</span>, <span class="number">0.0078</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones(<span class="number">2</span>)</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure><p>Dim2：通常用于多张图片的 <strong>Linear Input Batch</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.randn(2,3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[-0.4689, -1.2038, -1.6282],</span><br><span class="line">        [-0.8379, -1.1376, -1.9624]])</span><br><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([2, 3])</span><br><span class="line">&gt;&gt;&gt; a.size(0)</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; a.size(1)</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; a.shape[0]</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; a.shape[1]</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>Dim3: 用于 <strong>RNN Input Batch</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.rand(1,2,3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[[0.4257, 0.1625, 0.1817],</span><br><span class="line">         [0.3695, 0.8208, 0.5442]]])</span><br><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([1, 2, 3])</span><br><span class="line">&gt;&gt;&gt; a[0]</span><br><span class="line">tensor([[0.4257, 0.1625, 0.1817],</span><br><span class="line">        [0.3695, 0.8208, 0.5442]])</span><br><span class="line">&gt;&gt;&gt; a[0][1]</span><br><span class="line">tensor([0.3695, 0.8208, 0.5442])</span><br><span class="line">&gt;&gt;&gt; a[0][1][1]</span><br><span class="line">tensor(0.8208)</span><br><span class="line">&gt;&gt;&gt; </span><br></pre></td></tr></table></figure><p>$$<br>\begin{bmatrix}\begin{bmatrix}0.4257&amp;0.1625&amp;0.1817\end{bmatrix}\\\begin{bmatrix}0.3695&amp;0.8208&amp;0.5442\end{bmatrix}\end{bmatrix}<br>$$<br>Dim4：适用于 <strong>图片</strong> [batch,channel,height,width]</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.numel()</span><br><span class="line"><span class="number">4704</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.dim()</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure><h3 id="创建Tensor">创建Tensor</h3><p>从np.array创建</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=np.array([<span class="number">2</span>,<span class="number">3.3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.from_numpy(a)</span><br><span class="line">tensor([<span class="number">2.0000</span>, <span class="number">3.3000</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=np.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.from_numpy(a)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p><code>numpy_a=a.numpy()</code>可将Tensor转换位numpy数据类型</p><p>从list创建：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.tensor([[1,2,3,4,5],[3,4,5,6,7]])</span><br><span class="line">tensor([[1, 2, 3, 4, 5],</span><br><span class="line">        [3, 4, 5, 6, 7]])</span><br></pre></td></tr></table></figure><p><font size =5><mark>注意：tensor()输入参数为初始化数据 Tensor()输入参数为shape或list</mark></font></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.Tensor(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor([[<span class="number">2.3063e-31</span>, <span class="number">8.6740e-43</span>, <span class="number">8.4078e-45</span>],</span><br><span class="line">        [<span class="number">0.0000e+00</span>, <span class="number">2.3073e-31</span>, <span class="number">8.6740e-43</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.Tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tensor([<span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">2</span>,<span class="number">3</span>) </span><br><span class="line">报错</span><br></pre></td></tr></table></figure><p>不初始化：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.FloatTensor(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor([[[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.IntTensor(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]]], dtype=torch.int32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.empty(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">tensor([[[ <span class="number">6.4011e+23</span>,  <span class="number">1.7866e+25</span>],</span><br><span class="line">         [-<span class="number">2.2864e-31</span>,  <span class="number">7.7961e+34</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.1093e+27</span>,  <span class="number">4.1709e-08</span>],</span><br><span class="line">         [ <span class="number">3.7392e-38</span>, -<span class="number">1.2803e-26</span>]]])</span><br></pre></td></tr></table></figure><p>随机初始化：<code>rand</code>:[0,1]间均匀分布</p><p><code>rand_like(a)</code>相当于<code>rand(a.shape)</code></p><p><code>rand_int(min,max,shape)</code></p><p><code>randn(shape)</code>(0,1)正态分布</p><p><code>normal(mean=torch.full([shape],mean),std=torch.full([shape],std))</code>:自定义正态分布，均值mean 方差 std</p><p>torch.full：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.full([2,3],1)</span><br><span class="line">tensor([[1, 1, 1],</span><br><span class="line">        [1, 1, 1]])</span><br></pre></td></tr></table></figure><p>torch.arrange:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.arange(1,10,2)</span><br><span class="line">tensor([1, 3, 5, 7, 9])</span><br></pre></td></tr></table></figure><p>torch.linspace:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">4</span>)</span><br><span class="line">tensor([ <span class="number">0.0000</span>,  <span class="number">3.3333</span>,  <span class="number">6.6667</span>, <span class="number">10.0000</span>])</span><br><span class="line">等分为<span class="number">4</span>个数据</span><br></pre></td></tr></table></figure><p>torch.eye：单位矩阵</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.eye(3,3)</span><br><span class="line">tensor([[1., 0., 0.],</span><br><span class="line">        [0., 1., 0.],</span><br><span class="line">        [0., 0., 1.]])</span><br><span class="line">&gt;&gt;&gt; torch.eye(3,5)</span><br><span class="line">tensor([[1., 0., 0., 0., 0.],</span><br><span class="line">        [0., 1., 0., 0., 0.],</span><br><span class="line">        [0., 0., 1., 0., 0.]])</span><br></pre></td></tr></table></figure><p>randperm: 随机打散 可设置为种子每次相同打散方法</p><h3 id="索引与切片">索引与切片</h3><p>与<code>python</code>一样</p><p>间隔切片</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.Tensor(4,3,28,28)</span><br><span class="line">&gt;&gt;&gt; a[:,:,0:28:2,0:28:2].shape</span><br><span class="line">torch.Size([4, 3, 14, 14])</span><br></pre></td></tr></table></figure><p><strong>第二个<code>:</code>后为步长</strong></p><p><code>index_select(维度，torch.tensor[所选index])</code>：<mark>第二个参数必须是tensor </mark></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.Tensor(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.index_select(<span class="number">0</span>,torch.tensor([<span class="number">0</span>,<span class="number">2</span>])).shape</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure><p><code>...</code>:所有的维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>,...].shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[...,:<span class="number">2</span>].shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><p>masked_select():</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0.2088</span>, -<span class="number">0.1852</span>,  <span class="number">0.6233</span>,  <span class="number">0.5107</span>],</span><br><span class="line">        [ <span class="number">1.6500</span>,  <span class="number">0.3151</span>,  <span class="number">1.1227</span>,  <span class="number">1.7956</span>],</span><br><span class="line">        [-<span class="number">1.1915</span>,  <span class="number">0.8243</span>, -<span class="number">0.0114</span>,  <span class="number">0.7303</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask=a.ge(<span class="number">0.5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask</span><br><span class="line">tensor([[<span class="literal">False</span>, <span class="literal">False</span>,  <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>, <span class="literal">False</span>,  <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>,  <span class="literal">True</span>, <span class="literal">False</span>,  <span class="literal">True</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.masked_select(mask)</span><br><span class="line">tensor([<span class="number">0.6233</span>, <span class="number">0.5107</span>, <span class="number">1.6500</span>, <span class="number">1.1227</span>, <span class="number">1.7956</span>, <span class="number">0.8243</span>, <span class="number">0.7303</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.masked_select(mask).shape</span><br><span class="line">torch.Size([<span class="number">7</span>])</span><br></pre></td></tr></table></figure><h3 id="维度变换">维度变换</h3><h4 id="reshape：">reshape：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.rand(<span class="number">4</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">a.reshape(<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">784</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">784</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">4</span>*<span class="number">28</span>,<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">112</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">4</span>*<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure><h4 id="squeeze-unsqueeze">squeeze/unsqueeze:</h4><p>unsqueeze():参数取值范围 <strong>[-dim-1,dim+1)  正索引</strong> 是在当前索引 <strong>之后</strong> 插入，<strong>负索引</strong>是在当前索引 <strong>之前</strong> 插入</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.unsqueeze(3).shape</span><br><span class="line">torch.Size([4, 1, 28, 1, 28])</span><br><span class="line">&gt;&gt;&gt; a.unsqueeze(-3).shape</span><br><span class="line">torch.Size([4, 1, 1, 28, 28])</span><br></pre></td></tr></table></figure><p>squee(idx): 删除当前维度</p><h4 id="transpose-t-permute：">transpose/t/permute：</h4><p>transpose：维度交换</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.transpose(1,3).shape</span><br><span class="line">torch.Size([4, 28, 28, 1])</span><br></pre></td></tr></table></figure><p>t：只能用于矩阵 二维</p><p>permute：重构 输入参数为维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><h4 id="expand-repeat：">expand/repeat：</h4><p>expand: 需要时才复制数据，输入参数为扩张后的大小，只有为1的才能扩张，-1表示该维度保持不限</p><p>repeat：一开始就复制数据,输入参数为复制次数</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.expand(-1,4,28,28).shape</span><br><span class="line">torch.Size([4, 4, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.repeat(4,1,1,1).shape</span><br><span class="line">torch.Size([16, 1, 28, 28])</span><br></pre></td></tr></table></figure><h3 id="合并与分割">合并与分割</h3><h4 id="cat">cat</h4><p><strong>除了需要合并的dim以外，其他的dim大小应该相同</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.rand(<span class="number">4</span>,<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b=torch.rand(<span class="number">5</span>,<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat([a,b],dim=<span class="number">0</span>).shape</span><br><span class="line">torch.Size([<span class="number">9</span>, <span class="number">32</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><h4 id="stack">stack</h4><p><strong>重新在合并的dim维度之前添加一个维度，该维度不同取值显示合并前不同内容</strong></p><p><strong>要求所有dim的大小一样</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack([a,b],dim=<span class="number">1</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">2</span>, <span class="number">32</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><h4 id="split">split</h4><p><strong>通过长度拆分</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([3, 4, 32])</span><br><span class="line">&gt;&gt;&gt; b.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; c.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; d.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; a1,a2=a.split([2,1],dim=0)</span><br><span class="line">&gt;&gt;&gt; a1.shape</span><br><span class="line">torch.Size([2, 4, 32])</span><br><span class="line">&gt;&gt;&gt; a2.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; a1,a2=a.split(2,dim=0)</span><br><span class="line">&gt;&gt;&gt; a1.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br><span class="line">&gt;&gt;&gt; a2.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br></pre></td></tr></table></figure><h4 id="chunk">chunk</h4><p><strong>通过数量拆分</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a1,a2=a.chunk(2,dim=0)</span><br><span class="line">&gt;&gt;&gt; a1.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br><span class="line">&gt;&gt;&gt; a2.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br></pre></td></tr></table></figure><h3 id="Broadcasting">Broadcasting</h3><p><mark>大维度缺失可自动添加，且每个维度可以自动扩张</mark></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.rand(4,4,32,32)</span><br><span class="line">&gt;&gt;&gt; b=torch.rand(4,1,1)</span><br><span class="line">&gt;&gt;&gt; c=a+b</span><br><span class="line">&gt;&gt;&gt; c.shape</span><br><span class="line">torch.Size([4, 4, 32, 32])</span><br></pre></td></tr></table></figure><p>b:[4,1,1]→[1,4,1,1]→[4,4,32,32]</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.rand(1,3)</span><br><span class="line">&gt;&gt;&gt; b=torch.rand(3,1)</span><br><span class="line">&gt;&gt;&gt; c=a+b</span><br><span class="line">&gt;&gt;&gt; c.shape</span><br><span class="line">torch.Size([3, 3])</span><br></pre></td></tr></table></figure><p>a:[1,3]→[3,3]</p><p>b:[3,1]→[3,3]</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mathjax基本语法</title>
      <link href="/2022/04/12/mathjax%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"/>
      <url>/2022/04/12/mathjax%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1>mathjax基本语法</h1><h2 id="1-基本语法">1.基本语法</h2><h3 id="1-1显示公式">1.1显示公式</h3><p><font size =4>行内公式：<code>$公式$</code></p><p>文内公式：单独一行</p><p><code>$$公式$$</code></p><h3 id="1-2-特殊字符">1.2 特殊字符</h3><h4 id="1-2-1-希腊字符">1.2.1 希腊字符</h4><table><thead><tr><th>显示</th><th>命令</th><th>显示</th><th>命令</th><th>显示</th><th>命令</th></tr></thead><tbody><tr><td>α</td><td>\alpha</td><td>β</td><td>\beta</td><td>υ</td><td>\upsilon</td></tr><tr><td>γ</td><td>\gamma</td><td>δ</td><td>\delta</td><td>ϕ</td><td>\phi</td></tr><tr><td>ϵ</td><td>\epsilon</td><td>ζ</td><td>\zeta</td><td>χ</td><td>\chi</td></tr><tr><td>ι</td><td>\iota</td><td>θ</td><td>\theta</td><td>ψ</td><td>\psi</td></tr><tr><td>η</td><td>\eta</td><td>κ</td><td>\kappa</td><td>ω</td><td>\omega</td></tr><tr><td>λ</td><td>\lambda</td><td>μ</td><td>\mu</td><td></td><td></td></tr><tr><td>ν</td><td>\nu</td><td>ξ</td><td>\xi</td><td></td><td></td></tr><tr><td>π</td><td>\pi</td><td>ρ</td><td>\rho</td><td></td><td></td></tr><tr><td>σ</td><td>\sigma</td><td>τ</td><td>\tau</td><td></td><td></td></tr></tbody></table><p><img src="https://appwk.baidu.com/naapi/doc/view?ih=621&amp;o=jpg_6_0_______&amp;iw=636&amp;ix=0&amp;iy=0&amp;aimw=636&amp;rn=1&amp;doc_id=84a3254a67ec102de3bd897b&amp;pn=1&amp;sign=00ee614654f2c18484257f890bd40217&amp;type=1&amp;app_ver=2.9.8.2&amp;ua=bd_800_800_IncredibleS_2.9.8.2_2.3.7&amp;bid=1&amp;app_ua=IncredibleS&amp;uid=&amp;cuid=&amp;fr=3&amp;Bdi_bear=WIFI&amp;from=3_10000&amp;bduss=&amp;pid=1&amp;screen=800_800&amp;sys_ver=2.3.7" alt="希腊字母大小写对照表"></p><p><mark>需要大写字母则将第一个字母大写</mark></p><p><code>\Gamma</code> :$\Gamma$</p><p><mark>需要斜体大写字母则在前加var</mark></p><p><code>\varGamma</code>: $\varGamma$</p><h4 id="1-2-2-特殊格式">1.2.2 特殊格式</h4><ul><li><p>上下标</p><p>上标：<code>^</code> $x^2$</p><p>下标：<code>_</code>$x_2$</p></li><li><p>向量</p><p>短箭头：<code>\vec a</code> $\vec a$ <code>\vec &#123;ab&#125;</code> $\vec {ab}$</p><p>长箭头：<code>\overrightarrow a</code> $\overrightarrow a$ <code>\overrightarrow &#123;ab&#125;</code> $\overrightarrow {ab}$</p></li><li><p>bar</p><p>上箭头：<code>\hat a</code> $\hat a$</p><p>横线：<code>\overline a</code> $\overline a$</p><p>下划线 <code>\underline a</code> $\underline a$</p></li><li><p>字体</p><p>空心字：<code>\mathbb &#123;a&#125;</code> $\mathbb {ABCDEFG}$</p></li><li><p><mark>空格</mark></p><p>空格需要转义字符\ ：<code>a\ b</code> $a\ b$</p></li></ul><h4 id="1-2-3括号与分组">1.2.3括号与分组</h4><ul><li><p>同一级用{}处理：<code>x_i^2</code> $x_i^2$ <code>x_&#123;i^2&#125;</code> $x_{i^2}$</p></li><li><p>小括号中括号可直接使用，大括号需要专业字符: <code>\&#123;...\&#125;</code></p></li><li><p>尖括号 <code>\langle...\rangle</code> $\langle ab\rangle$</p></li><li><p>绝对值 <code>\vert ... \vert</code> $\vert ab \vert$</p></li><li><p>双竖线 <code>\Vert ...\Vert</code> $\Vert ab \Vert$</p></li><li><p>使用<code>\left</code>和<code>\right</code>)使符号大小与邻近的公式相适应,该语句适用于所有括号类型</p></li><li><p><code>\left\&#123;\frac&#123;(x+y)&#125;&#123;[\alpha+\beta]&#125;\right\&#125;</code>显示为$\left\{\frac{(x+y)}{[\alpha+\beta]}\right\}$</p><p><font size=6><mark>hexo 解析时<code>\left\&#123;</code>和<code>\right\&#125;</code>会分别解析为<code>\left&#123;</code>与<code>\right&#125;</code>,想要正确解析需要更改为<code>\\&#123;  \\&#125;</code></mark></font></p></li></ul><h4 id="1-2-4-运算符">1.2.4 运算符</h4><table><thead><tr><th>运算符</th><th>演示</th><th>运算符</th><th>演示</th></tr></thead><tbody><tr><td>\times</td><td>$x \times y$</td><td>\cdot</td><td>$x \cdot y$</td></tr><tr><td>\ast</td><td>$x \ast y$</td><td>\div</td><td>$x \div y$</td></tr><tr><td>\pm</td><td>$x \pm y$</td><td>\mp</td><td>$x \mp y$</td></tr><tr><td>\leq</td><td>$x \leq y$</td><td>\geq</td><td>$x \geq y$</td></tr><tr><td>\approx</td><td>$x \approx y$</td><td>\equiv</td><td>$x \equiv y$</td></tr><tr><td>\bigodot</td><td>$x \bigodot y$</td><td>\bigtimes</td><td>$x \bigtimes y$</td></tr></tbody></table><table><thead><tr><th>运算符</th><th>演示</th><th>运算符</th><th>演示</th></tr></thead><tbody><tr><td>\in</td><td>$x \in y$</td><td>\subset</td><td>$x \subset y$</td></tr><tr><td>\subseteq</td><td>$x \subseteq y$</td><td>\varnothing</td><td>$\varnothing$</td></tr><tr><td>\cup</td><td>$x \cup y$</td><td>\cap</td><td>$x \cap y$</td></tr></tbody></table><h4 id="1-2-5-特殊符号">1.2.5 特殊符号</h4><table><thead><tr><th>代码</th><th>演示</th><th>命令</th></tr></thead><tbody><tr><td>\overbrace</td><td>$\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}$</td><td>\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}</td></tr><tr><td>\underbrace</td><td>$\underbrace{b+c}_{1.0}$</td><td>\underbrace{b+c}_{1.0}</td></tr><tr><td>\partial</td><td>$\frac{\partial z}{\partial x}$</td><td>\frac{\partial z}{\partial x}</td></tr><tr><td>\idots</td><td>$1,2,\ldots,n$</td><td>1,2,\ldots,n</td></tr><tr><td>\cdots</td><td>$1,2,\cdots,n$</td><td>1,2,\cdots,n$</td></tr><tr><td>\infty</td><td>$\infty$</td><td>–</td></tr><tr><td>\nabla</td><td>$\nabla$</td><td>–</td></tr><tr><td>\forall</td><td>$\forall$</td><td>–</td></tr><tr><td>\exists</td><td>$\exists$</td><td>–</td></tr><tr><td>\triangle</td><td>$\triangle$</td><td>–</td></tr><tr><td>\lnot</td><td>$\lnot$</td><td></td></tr></tbody></table><table><thead><tr><th>\uparrow</th><th>$\uparrow$</th><th>\Uparrow</th><th>$\Uparrow$</th></tr></thead><tbody><tr><td>\downarrow</td><td>$\downarrow$</td><td>\Downarrow</td><td>$\Downarrow$</td></tr><tr><td>\leftarrow</td><td>$\leftarrow$</td><td>\Leftarrow</td><td>$\Leftarrow$</td></tr><tr><td>\rightarrow</td><td>$\rightarrow$</td><td>\Rightarrow</td><td>$\Rightarrow$</td></tr></tbody></table><ul><li><p>求和 符号与积分</p><table><thead><tr><th>\sum</th><th>$\sum$</th><th>\sum_{i=0}^n</th><th>$\sum_{i=0}^n$</th><th>\displaystyle\sum_{i=0}^n</th><th>$\displaystyle\sum_{i=0}^n$</th></tr></thead><tbody><tr><td>\lim</td><td>$\lim$</td><td>\lim_{x\to\infty}</td><td>$\lim_{x\to\infty}$</td><td>\displaystyle\lim_{x\to\infty}</td><td>$\displaystyle\lim_{x\to\infty}$</td></tr><tr><td>\int</td><td>$\int$</td><td>\iint</td><td>$\iint$</td><td>\iiint</td><td>$\iiint$</td></tr><tr><td>\oint</td><td>$\oint$</td><td>\int_0^\infty{fxdx}</td><td>$\int_0^\infty{fxdx}$</td><td>\prod</td><td>$\prod$</td></tr></tbody></table></li></ul><h3 id="1-3-分式与根式">1.3 分式与根式</h3><ul><li>分式 <code>\frac&#123;分子&#125;&#123;分母&#125;</code> $\frac{x}{y}$</li><li>根式 <code>\sqrt[x]&#123;y&#125;</code>$\sqrt[x]{y}$</li></ul><h3 id="1-4-特殊函数">1.4 特殊函数</h3><p><code>\函数名</code> $\sin{x}$ $\log_2{x}$</p><h2 id="2-矩阵">2.矩阵</h2><h3 id="2-1矩阵生成">2.1矩阵生成</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;matrix&#125; </span><br><span class="line">1&amp;0&amp;1\\</span><br><span class="line">0&amp;1&amp;0\\</span><br><span class="line">1&amp;1&amp;0\\</span><br><span class="line">\end&#123;matrix&#125;</span><br></pre></td></tr></table></figure><ul><li>开始和结束需要输入 <code>\begin&#123;matrix&#125; \end&#123;matrix&#125;</code></li><li>同一行元素之间用 <strong>&amp;</strong> 符号连接</li><li><font size=6>换行 <strong>\\\\</strong> <mark>在hexo中时，<strong>\\\\<strong>会解析为一个</strong>\</strong>,故换行时需要输入 <strong>\\\\\\\\</strong> 且该方法只有在 <strong>矩阵、列表、方程组</strong> 等有<code>\beng&#123;&#125;\end&#123;&#125;</code>包围的公式中有效，若想写两行普通公式，还请插入 <strong>两个</strong> 公式块 </mark></font></li></ul><p>$$\begin{matrix}<br>1&amp;0&amp;1\\<br>0&amp;1&amp;0\\<br>1&amp;1&amp;0\\<br>\end{matrix} $$</p><h3 id="2-2矩阵边框">2.2矩阵边框</h3><p>在起始、结束位置替换matrix</p><table><thead><tr><th>pmatrix</th><th>$\begin{pmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{pmatrix}$</th><th>bmatrix</th><th>$\begin{bmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{bmatrix}$</th></tr></thead><tbody><tr><td>Bmatrix</td><td>$\begin{Bmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{Bmatrix}$</td><td>vmatrix</td><td>$\begin{vmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{vmatrix}$</td></tr><tr><td>Vmatrix</td><td>$\begin{Vmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{Vmatrix}$</td><td></td><td></td></tr></tbody></table><h3 id="2-3-高维矩阵的表示">2.3 高维矩阵的表示</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;bmatrix&#125;</span><br><span class="line">&#123;a_&#123;11&#125;&#125;&amp;&#123;a_&#123;12&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;1n&#125;&#125;\\</span><br><span class="line">&#123;a_&#123;21&#125;&#125;&amp;&#123;a_&#123;22&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;2n&#125;&#125;\\</span><br><span class="line">&#123;\vdots&#125;&amp;&#123;\vdots&#125;&amp;&#123;\ddots&#125;&amp;&#123;\vdots&#125;\\</span><br><span class="line">&#123;a_&#123;n1&#125;&#125;&amp;&#123;a_&#123;n2&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;nn&#125;&#125;\\</span><br><span class="line">\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure><p>$$\begin{bmatrix}<br>{a_{11}}&amp;{a_{12}}&amp;{\cdots}&amp;{a_{1n}}\\<br>{a_{21}}&amp;{a_{22}}&amp;{\cdots}&amp;{a_{2n}}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{a_{n1}}&amp;{a_{n2}}&amp;{\cdots}&amp;{a_{nn}}\\<br>\end{bmatrix}$$</p><ul><li><p>横省略号：<code>\cdots</code></p></li><li><p>竖省略号： <code>\vdots</code></p></li><li><p>斜省略号：<code>\ddots</code></p><h2 id="3-列表">3.列表</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;array&#125;&#123;c|lll&#125;</span><br><span class="line">&#123;\downarrow&#125;&amp;&#123;name&#125;&amp;&#123;age&#125;&amp;&#123;ID&#125;\\</span><br><span class="line">\hine</span><br><span class="line">&#123;num1&#125;&amp;&#123;yy&#125;&amp;&#123;22&#125;&amp;&#123;1320&#125;\\</span><br><span class="line">&#123;num2&#125;&amp;&#123;lw&#125;&amp;&#123;22&#125;&amp;&#123;1111&#125;\\</span><br><span class="line">\end&#123;array&#125;</span><br></pre></td></tr></table></figure><p>$$\begin{array}{c|lll}<br>{\downarrow}&amp;{name}&amp;{age}&amp;{ID}\\<br>\hline<br>{num1}&amp;{yy}&amp;{22}&amp;{1320}\\<br>{num2}&amp;{lw}&amp;{22}&amp;{1111}\\<br>\end{array}$$</p></li><li><p>起始、结束处以 <strong>{array}</strong> 声明</p></li><li><p>对齐方式:在{array}后以{}逐列统一声明</p></li><li><p>左对齐:l；居中：c；右对齐：r</p></li><li><p>竖直线:在声明对齐方式时，插入 <strong>|</strong> 建立竖直线</p></li><li><p>插入水平线:<strong>\hline</strong></p></li></ul><h2 id="3-方程组">3.方程组</h2><ul><li><p>起始结束为 <strong>{cases}</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;cases&#125;</span><br><span class="line">a_1x+b_1y+c_1z=d_1\\</span><br><span class="line">a_2x+b_2y+c_2z=d_2\\</span><br><span class="line">a_3x+b_3y+c_3z=d_3\\</span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure><p>$$\begin{cases}<br>a_1x+b_1y+c_1z=d_1\\<br>a_2x+b_2y+c_2z=d_2\\<br>a_3x+b_3y+c_3z=d_3\\<br>\end{cases}$$</p></li></ul><h2 id="长公式换行">长公式换行</h2><p>用<code>&amp;</code>表示对齐位置，<code>\\</code>换行</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line">\frac&#123;\partial w^l&#125;&#123;\partial m_&#123;F_i^l&#125;&#125;&amp;=\frac&#123;\partial&#125;&#123;\partial m_&#123;F_i^l&#125;&#125;\prod_&#123;j=1&#125;^pexp\left\&#123;-\frac&#123;1&#125;&#123;2&#125;[(x_j^&#123;(t)&#125;-m_&#123;F_j^l&#125;)/\sigma_&#123;F_j^l&#125;]^2 \right\&#125;\\&amp;=\frac&#123;\partial &#125;&#123;\partial m_&#123;F_i^l&#125;&#125;exp\left\&#123;  -\frac&#123;1&#125;&#123;2&#125;[(x_i^&#123;(t)&#125;-m_&#123;F_i^l&#125;)/\sigma_&#123;F_i^l&#125;]^2 \right\&#125;\times \prod_&#123;j=1\\j\neq i&#125;^pexp\left\&#123;  -\frac&#123;1&#125;&#123;2&#125;[(x_j^&#123;(t)&#125;-m_&#123;F_j^l&#125;)/\sigma_&#123;F_j^l&#125;]^2\right\&#125;\\&amp;=\prod_&#123;j=1&#125;^pexp\left\&#123;  -\frac&#123;1&#125;&#123;2&#125;[(x_j^&#123;(t)&#125;-m_&#123;F_j^l&#125;)/\sigma_&#123;F_j^l&#125;]^2 \right\&#125;\times \frac&#123;x_i^&#123;(i)&#125;-m_&#123;F_i^l&#125;&#125;&#123;\sigma_&#123;F_i^l&#125;^2&#125;\\&amp;=\frac&#123;x_i^&#123;(i)&#125;-m_&#123;F_i^l&#125;&#125;&#123;\sigma_&#123;F_i^l&#125;^2&#125;\times w^l</span><br><span class="line">\end&#123;split&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>\begin{split}{}<br>\frac{\partial w^l}{\partial m_{F_i^l}}&amp;=\frac{\partial}{\partial m_{F_i^l}}\prod_{j=1}^pexp\left{-\frac{1}{2}[(x_j^{(t)}-m_{F_j^l})/\sigma_{F_j^l}]^2 \right}\<br>&amp;=\frac{\partial }{\partial m_{F_i^l}}exp\left{  -\frac{1}{2}[(x_i^{(t)}-m_{F_i^l})/\sigma_{F_i^l}]^2 \right}\times \prod_{j=1\j\neq i}^pexp\left{  -\frac{1}{2}[(x_j^{(t)}-m_{F_j^l})/\sigma_{F_j^l}]^2\right}\<br>&amp;=\prod_{j=1}^pexp\left{  -\frac{1}{2}[(x_j^{(t)}-m_{F_j^l})/\sigma_{F_j^l}]^2 \right}\times \frac{x_i^{(i)}-m_{F_i^l}}{\sigma_{F_i^l}^2}\<br>&amp;=\frac{x_i^{(i)}-m_{F_i^l}}{\sigma_{F_i^l}^2}\times w^l<br>\end{split}<br>$$</p><blockquote><p>参考</p><p><a href="https://blog.csdn.net/ajacker/article/details/80301378?spm=1001.2014.3001.5502">Mathjax语法总结_ajacker的博客-CSDN博客_mathjax语法</a></p><p><a href="https://blog.csdn.net/ethmery/article/details/50670297">基本数学公式语法(of MathJax)_PUMC芋圆四号的博客-CSDN博客_mathjax语法</a></font></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 杂文 </category>
          
          <category> mathjax </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathjax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown与Typora</title>
      <link href="/2022/04/11/Markdown%E4%B8%8ETypora/"/>
      <url>/2022/04/11/Markdown%E4%B8%8ETypora/</url>
      
        <content type="html"><![CDATA[<h1>markdown基本语法与Typora</h1><p><font size=5>本文既是对markdown语法及Typora快捷键的记录 也是练习</font></p><p><mark>注意在行内插入加粗、斜体等最好在*符号与左右文本之间最好加上一个半角空格，不然hexo可能解析错误</mark></p><p>比如应<code>你好 **yy** 很高兴</code>而不是<code>你好**yy**很高兴</code></p><ol><li><p>加粗 <code>ctrl+B</code></p><p><code> **文字**</code><br><strong>文字</strong></p></li><li><p>倾斜 <code>Ctrl+I</code></p><p><code>*斜体字*</code><br><em>斜体字</em></p></li><li><p>下划线 <code>ctrl+U</code></p><p><code>&lt;u&gt; 下划线&lt;/u&gt;</code><br><u>下划线</u></p></li><li><p>多级标题 <code>Ctrl+1~6</code></p><p><code># 一级标题</code></p><h1>一级标题</h1><p><code>## 二级标题</code></p><h2 id="二级标题">二级标题</h2><p>以此类推</p></li><li><p>有序列表 <code>Ctrl+Shift+[</code></p><p><code>1. 文字</code></p><ol><li><p>一</p></li><li><p>二</p></li></ol></li><li><p>无序列表 <code>Ctrl+Shift+]</code></p><p><code>- 无序列表</code></p><ul><li>无序列表</li></ul></li><li><p>降级 <code>Tab</code></p></li><li><p>升级 <code>Shift+Tab</code></p></li><li><p>插入链接 <code>Ctrl+K</code></p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">文字链接:：[链接名称]（http://链接网址） </span><br><span class="line">网址链接：&lt;http://...&gt;</span><br></pre></td></tr></table></figure><p><code>&lt;网址&gt;</code> <a href="http://baidu.com">http://baidu.com</a></p><p><code>[百度](http://)</code> <a href="http://www.baidu.com">百度</a></p></li><li><p>插入公式 <code>Ctrl+Shift+M</code><br>使用时需要在front-matter中加上mathjax: true</p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">数学公式</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$\lim_{x\to\infty}\exp(-x)=0$$</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">内联公式 $\lim_&#123;x\to\infty&#125;\exp(-x)=0$</span><br></pre></td></tr></table></figure><p>内联公式: $\lim_{x\to\infty}\exp(-x)=0$<br><font size=5>注意使用内联公式时可能与非内联公式样式不同，比如上示lim下标  </font></p></li><li><p>行内代码 <code>Ctrl+shift+k</code></p><p>````代码` ```</p></li><li><p>插入图片 <code>Ctrl+Shift+I</code></p><p><code>![图片名称](http://)</code></p><p><img src="https://s1.328888.xyz/2022/04/11/fXIeF.png" alt="图片1"></p></li><li><p>创建表格 <code>Ctrl+T</code></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| 姓名 | 性别 |</span><br><span class="line">| :--- | ---：|</span><br><span class="line">| 张三 | 男 |</span><br><span class="line">| 李四 | 女 |</span><br></pre></td></tr></table></figure><table><thead><tr><th>姓名</th><th>性别</th></tr></thead><tbody><tr><td>张三</td><td>男</td></tr><tr><td>李四</td><td>女</td></tr></tbody></table></li><li><p>删除线 <code>ALT+Shift+5</code></p><p><code>~~删除线~~</code></p><p><s>删除线</s></p></li><li><p>引用 <code>Ctrl+Shift+Q</code></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; 这是一个引用</span><br><span class="line">&gt;&gt;这是一个嵌套引用</span><br></pre></td></tr></table></figure><blockquote><p>这是一个引用</p><blockquote><p>这是一个引用嵌套</p></blockquote></blockquote></li><li><p>上标</p><p><code> X&lt;sup&gt;2&lt;sup&gt;</code></p><p>X<sup>2</sup></p></li><li><p>下标</p><p><code>H&lt;sub&gt;2&lt;/suB&gt;O</code></p><p>H<sub>2</sub>O</p></li><li><p>分割线 <code>*** 或者 ___</code></p><hr><hr></li><li><p>自动产生目录 <code>[TOC]+Enter</code></p><p><code>[TOC]</code></p></li><li><p>改变字体大小</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span>=<span class="string">1</span>&gt;</span>字体大小size=1<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span>=<span class="string">3</span>&gt;</span>字体大小size=3<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span>=<span class="string">5</span>&gt;</span>字体大小size=5<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font size=1>字体大小size=1</font></p><p><font size=3>字体大小size=3</font></p><p><font size=5>字体大小size=5</font></p></li><li><p>改变字体颜色</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">red</span>&gt;</span>红色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">&quot;blue&quot;</span>&gt;</span>蓝色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">Yellow</span>&gt;</span>黄色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">YellowGreen</span>&gt;</span>黄绿色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font color=red>红色</font><br><font color="blue">蓝色</font><br><font color=Yellow>黄色</font><br><font color=YellowGreen>黄绿色</font></p></li><li><p>改变字体类型</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;黑体&quot;</span>&gt;</span>黑体<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;宋体&quot;</span>&gt;</span>宋体<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;仿宋&quot;</span>&gt;</span>仿宋<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;幼圆&quot;</span>&gt;</span>幼圆<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;楷书&quot;</span>&gt;</span>楷书<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文行楷&quot;</span>&gt;</span>华文行楷<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文隶书&quot;</span>&gt;</span>华文隶书<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文新魏&quot;</span>&gt;</span>华文新魏<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文彩云&quot;</span>&gt;</span>华文彩云<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文琥珀&quot;</span>&gt;</span>华文琥珀<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font face="黑体">黑体</font><br><font face="宋体">宋体</font><br><font face="仿宋">仿宋</font><br><font face="幼圆">幼圆</font><br><font face="楷书">楷书</font><br><font face="华文行楷">华文行楷</font><br><font face="华文隶书">华文隶书</font><br><font face="华文新魏">华文新魏</font><br><font face="华文彩云">华文彩云</font><br><font face="华文琥珀">华文琥珀</font></p></li><li><p>文本高亮</p><p><code>&lt;mark&gt;highlight 2&lt;/mark&gt;</code></p><p><mark>highlight 2</mark></p></li></ol><p>​</p><p>​</p>]]></content>
      
      
      <categories>
          
          <category> 杂文 </category>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> Typora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>序言</title>
      <link href="/2022/04/11/%E5%BA%8F%E8%A8%80/"/>
      <url>/2022/04/11/%E5%BA%8F%E8%A8%80/</url>
      
        <content type="html"><![CDATA[<h1>剑谱</h1><h2 id="序言">序言</h2><p><img src="https://s1.328888.xyz/2022/04/11/fXIeF.png" alt="fXIeF.png"></p><p><font size=4>   小生于壬寅年元月痛失所爱，数月以来郁郁寡欢，再不得昨日之愉，然偶见各路大神奉为顶上珍宝的格言：</p><blockquote><p>剑谱第一页 无爱既是神</p></blockquote><p>  日思夜想之下竟真悟出了几番道理，称其为道理确些许有失偏颇，然实有些许感悟，故在此立此blog，欲决心 <strong><s>发奋学习</s></strong> 练剑，将些许 <strong><s>学习笔记</s></strong> 心得写于此剑谱之中，望暮年之日回首往事，仍有迹可循。</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 序言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 骚话 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
