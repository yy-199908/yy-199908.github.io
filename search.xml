<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CNN原理</title>
      <link href="/2022/04/19/CNN%E5%8E%9F%E7%90%86/"/>
      <url>/2022/04/19/CNN%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1>CNN</h1><p><img src="https://s1.328888.xyz/2022/04/19/rYL5g.png" alt="rYL5g.png"></p><p>卷积神经网络(CNN)的结构:</p><p>卷积层→池化层→全连接层</p><p>那为何不用传统全连接神经网络处理图像呢？其缺点也是很明显的，对于大尺寸图片：</p><ul><li>首先将图片展开为张量会丢失信息</li><li>处理大尺寸图片需要巨量的参数</li><li>参数太多容易导致过拟合</li></ul><h2 id="卷积层">卷积层</h2><p>卷积层是构建卷积神经网络的<strong>核心层</strong>，它产生了网络中大部分的<strong>计算量</strong>。注意是计算量而不是参数量。</p><p><img src="https://s1.328888.xyz/2022/04/19/rI8et.png" alt="rI8et.png"></p><h3 id="卷积">卷积</h3><p>何谓卷积？其实就是乘加运算与位置移动。上图所示，左边第一个矩阵为输入x，第二个矩阵为卷积核，所谓卷积就是他们对应位置相乘相加，最后得到了右图中235的结果。那让卷积核不停在输入x上移动过再乘加，得到的就是卷积层的输出。如下图所示卷积核在输入x上不停移动和乘加即为卷积。</p><p><img src="https://s1.328888.xyz/2022/04/19/rIYvk.gif" alt="rIYvk.gif"></p><h3 id="卷积运算的维度">卷积运算的维度</h3><p>既然引入了上图，那我们就借此说一下卷积运算的维度。都说卷积运算处理高度，宽度以外还有一个深度，何谓深度呢？我们先对以上gif做一个分析，就明白了。</p><p>首先输入为最左边一列三个矩阵，这三个矩阵来自于同一张图片的R,G,B三个通道，高度为5，宽度为5，外面灰色的0为padding(之后我们会提到)，不是原图像数据内容。那么我们输入卷积层的Tensor的size就是：<br>$$<br>x[1,3,5,5]<br>$$<br>第一个维度表示一张图片，第二个维度为3个channel:RGB,也就是输入x的深度，第三、第四个维度为高度和宽度</p><p>图中第二列和第三列为两个卷积核，因为输入深度是3，所以他需要对R,G,B三个矩阵都做卷积，其深度也是3，那么卷积核的<code>Tensor.size()</code>就是：<br>$$<br>K[2,3,3,3]<br>$$<br>第一个维度表示两个不同卷积核，第二个维度表示3个channel,也就是深度，第三、第四个维度为高度和宽度</p><p>那输出参数呢?看图中最后一列：<br>$$<br>out[1,2,3,3]<br>$$<br>因为这是一张图片通过两个卷积核所得的两个3x3输出。</p><h3 id="感受野">感受野</h3><p>如果将上图中每个像素点看作神经节点，对比全连接神经网络，我们发现每个神经节点的输入仅与9个输入节点和权重有关，而不再是全连接中的与所有上一层输入有关。也就是说下一层节点仅与上一层一些神经节点连接，而这些神经节点数量仅决于卷积核的长和宽，这就叫做感受野，即与神经元连接的空间大小。</p><p>如此一来，参数的数量不就打打减小了吗，对于28*28的图片，全连接神经网络第一个隐藏层每个节点需要784个参数，而卷积层中只需要卷积核个数x卷积核大小 这么多个参数。</p><h3 id="空间排列">空间排列</h3><p>稍微动一下脑筋，若卷积核在移动过程中不允许超出输入x的边界范围，那输出矩阵的大小肯定小于原图片大小，所以我们需要引入 <strong>padding</strong>，即在图像最外面加一层或几层全为0的像素，如此已到达输出核输入矩阵大小相同的目的。</p><p>如上图所示，输入为5x5，如果padding=0，卷积核为3x3，那么输出只有2x2，加上一层padding，输出才能达到3x3.</p><p>步长就更好理解了，即卷积核每次移动几个单位。</p><p>那么以上几个参数都确定了的话，输出尺寸为多大呢，公式如下：<br>$$<br>outsize=\frac{W-F+2P}{S}+1<br>$$<br>其中</p><p>W:输入数据大小</p><p>F:卷积核尺寸</p><p>S:步长</p><p>P:padding</p><h3 id="pytorch卷积层模块">pytorch卷积层模块</h3><p><code>nn.Conv2d()</code>:其输入参数如下</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        in_channels: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        out_channels: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        kernel_size: _size_2_t,</span></span><br><span class="line"><span class="params">        stride: _size_2_t = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        padding: <span class="type">Union</span>[<span class="built_in">str</span>, _size_2_t] = <span class="number">0</span>,</span></span><br><span class="line"><span class="params">        dilation: _size_2_t = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        bias: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        padding_mode: <span class="built_in">str</span> = <span class="string">&#x27;zeros&#x27;</span>,  <span class="comment"># <span class="doctag">TODO:</span> refine this type</span></span></span><br><span class="line"><span class="params">        device=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        dtype=<span class="literal">None</span></span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br></pre></td></tr></table></figure><p>in_channels：输入数据的channel数（深度）,为3的化通常为RGB</p><p>out_channels：输出数据的深度，与卷积核的个数相同</p><p>kernel_size：感受野的大小，卷积核的尺寸</p><p>stride：移动步长</p><p>padding：padding=0表示不填充，padding=1表示填充一层</p><p>dilation：卷积杜宇输入数据体的空间间隔，默认为1</p><p>dilation=2时计算如下：</p><p><img src="https://s1.328888.xyz/2022/04/19/rRubR.png" alt="rRubR.png"></p><p>卷积时输入x不在紧邻，而是间隔一个像素点</p><p>groups：输出数据体深度上和输入数据体深度上的联系，默认为1，相关联。如果groups=2，则输入深度被分割成两份，输出数据也被分割成两份，他们之间分别对应起来，要求输入输出深度都能被groups整除。</p><p>bias：是否添加偏执</p><center>卷积层输出也需要激活函数</center><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#example</span></span><br><span class="line">layer1=nn.sequential(</span><br><span class="line">nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">3</span>,<span class="number">1</span>,padding=<span class="number">1</span>),nn.ReLU(<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><p>上述代码添加了一个输入channel为3，卷积核个数32，卷积核尺寸3x3，步长为1，padding=1，采用ReLU激活函数的卷积层。</p><h2 id="池化层">池化层</h2><p>通常会在卷积层之间周期性的插入一个池化层，其作用是逐渐减低数据体的空间尺寸，这样就能减少网络中的参数量，减少计算耗费资源，同时有效控制过拟合。</p><p>池化层和卷积层一样也有一个空间窗口，通常采取这些窗口中的最大值作为暑促，然后不断滑动窗口，对输入数据每一个深度切片(channel)单独处理，减少它的尺寸空间。</p><p>最常用的池化层尺寸为2*2，滑动步长为2：</p><p><img src="https://s1.328888.xyz/2022/04/19/rRnti.png" alt="rRnti.png"></p><p>输出数据体与输入数据体尺寸关系如下：<br>$$<br>W_2=\frac{W_1-F}{S}+1<br>$$<br>F:池化窗口大小</p><p>W1：输入数据大小</p><p>W2:输出数据大小</p><p>S:步长</p><p>卷积层之间一般引入最大池化效果最好，而平均池化一般放在卷积神经网络的最后一层。</p><p><mark>谨慎使用比较大的池化窗口，以免对网络有破坏性</mark></p><h3 id="pytorch池化层模块">pytorch池化层模块</h3><p><code>nn.MaxPool2d()</code>参数:</p><p>kernel_size: 窗口大小</p><p>stride: 步长</p><p>padding: 一般不添加</p><p>dilation: 默认为1</p><p>return_indices：是否返回最大值下标，默认为False</p><h2 id="全连接层">全连接层</h2><p>全连接成与之前介绍的一般神经网络的结构是一样的。一般经过一系列卷积层核池化层之后，提取出图片的特征图，将特征图中所有神经元变为全连接层的样子。</p><p>比如卷积层和池化层的输出为3x3x512，那么将其变为3x3x512=4608个神经元，再经过几个隐藏层后输出结果，在这个过程中为了防止过拟合引入Dropout。</p><h2 id="小卷积核有效性">小卷积核有效性</h2><p>一般而言，几个小滤波器的卷积层的组合比一个大滤波器好。</p><p>比如3个3x3的卷积核堆叠，其最终第三层卷积层对第一层输入数据的感受野是7x7，但这样做比直接使用一个7x7的卷积核好，原因如下：</p><ul><li>多个卷积层之间还有激活函数，比单一卷积层的结构更能提取出深层的特征</li><li>3个3x3的卷积层才27个参数，一个7x7需要49个参数</li></ul>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 卷积神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MNIST数字分类</title>
      <link href="/2022/04/19/MNIST%E6%95%B0%E5%AD%97%E5%88%86%E7%B1%BB/"/>
      <url>/2022/04/19/MNIST%E6%95%B0%E5%AD%97%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h1>MNIST数字分类</h1><h2 id="模型结构">模型结构</h2><p>简单的三层全连接模型：两个隐藏层</p><p>超参数定义:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="comment">#参数定义</span></span><br><span class="line">parser=argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>,default=<span class="number">64</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--learning_rate&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">float</span>,default=<span class="number">1e-2</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--num_epochs&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">int</span>,default=<span class="number">20</span>)</span><br><span class="line">args=parser.parse_args()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>模型结构：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#模型结构与激活函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">simpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_dim,n_hidden1,n_hidden2,out_dim</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(simpleNet,self).__init__()</span><br><span class="line">        self.layer1=nn.Sequential(</span><br><span class="line">            nn.Linear(in_dim,n_hidden1),nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        self.layer2=nn.Sequential(</span><br><span class="line">            nn.Linear(n_hidden1,n_hidden2),nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        self.layer3=nn.Sequential(</span><br><span class="line">            nn.Linear(n_hidden2,out_dim))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        self.x1=self.layer1(x)</span><br><span class="line">        self.x2=self.layer2(self.x1)</span><br><span class="line">        self.x3=self.layer3(self.x2)</span><br><span class="line">        <span class="keyword">return</span> self.x3</span><br></pre></td></tr></table></figure><p>数据预处理：</p><p>此处<code>transforms.Compose()</code>将各种预处理操作组合在一起，<code>transforms.ToTensor()</code>将图片转换为Tensor数据类型，<code>transforms.Normalize()</code>完成数据的去中心化和标准化，减去均值再除以方差，输入第一个参数为均值，第二个参数为方差。因为本例中图片时灰度图片，只有一个通道，如果是彩色图片有三个通道，需要使用<code>transforms.Normalize([a,b,c],[d,e,f])</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#数据预处理</span></span><br><span class="line">data_tf=transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.5</span>],[<span class="number">0.5</span>])]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>读取数据集:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取数据集</span></span><br><span class="line">train_data=datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,train=<span class="literal">True</span>,transform=data_tf,download=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">test_data=train_data=datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,train=<span class="literal">False</span>,transform=data_tf)</span><br><span class="line">train_loader=DataLoader(train_data,batch_size=args.batch_size,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader=DataLoader(test_data,batch_size=args.batch_size,shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model=simpleNet(<span class="number">28</span>*<span class="number">28</span>,<span class="number">300</span>,<span class="number">100</span>,<span class="number">10</span>).cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model=simpleNet(<span class="number">28</span>*<span class="number">28</span>,<span class="number">300</span>,<span class="number">100</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">criterion=nn.CrossEntropyLoss()</span><br><span class="line">optimizer=optim.SGD(model.parameters(),lr=args.learning_rate,weight_decay=<span class="number">0.01</span>,momentum=<span class="number">0.78</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        img,label=batch</span><br><span class="line">        img=img.reshape(img.size(<span class="number">0</span>),<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            img=img.cuda()</span><br><span class="line">            label=label.cuda()</span><br><span class="line">        out=model(img)</span><br><span class="line">      </span><br><span class="line">        loss=criterion(out,label)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epochs:&#123;&#125;,loss:&#123;:.6f&#125;&quot;</span>.<span class="built_in">format</span>(epoch,loss))</span><br></pre></td></tr></table></figure><p>测试模型:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试模型</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">eval_loss=<span class="number">0</span></span><br><span class="line">eval_acc=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> test_loader:</span><br><span class="line">    img,label=batch</span><br><span class="line">    img=img.reshape(img.size(<span class="number">0</span>),<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        img=img.cuda()</span><br><span class="line">        label=label.cuda()</span><br><span class="line">    out=model(img)</span><br><span class="line">    loss=criterion(out,label)</span><br><span class="line"> </span><br><span class="line">    eval_loss+=loss.detach()*label.size(<span class="number">0</span>)</span><br><span class="line">    pred=torch.<span class="built_in">max</span>(out,dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">    num_correct=(pred==label).<span class="built_in">sum</span>()</span><br><span class="line">    eval_acc+=num_correct.detach()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test loss:&#123;&#125;,ACC:&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(eval_loss/<span class="built_in">len</span>(test_data),eval_acc/<span class="built_in">len</span>(test_data)))</span><br></pre></td></tr></table></figure><p>输出结果：</p><p><img src="https://s1.328888.xyz/2022/04/19/rs2XC.png" alt="rs2XC.png"></p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络结构</title>
      <link href="/2022/04/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
      <url>/2022/04/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>已经学过了懒得写了</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>数据预处理与训练模型的技巧</title>
      <link href="/2022/04/19/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E5%B7%A7/"/>
      <url>/2022/04/19/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h1>数据预处理</h1><h2 id="中心化">中心化</h2><p>所有数据每个特征维度减去均值，使数据均值为0</p><h2 id="标准化">标准化</h2><p>使得数据均值为0后，还需要使用标准化的做法让数据的也在维度都有着相同的规模，有两种常用方法：</p><ul><li>除以标准差，使得新数据的分布更接近高斯分布</li><li>让每个特征维度大的最大值和最小值按比例放到1~-1之间</li></ul><h2 id="主成分分析">主成分分析</h2><h3 id="相关背景">相关背景</h3><p>在许多领域的研究与应用中，通常需要对含有多个变量的数据进行观测，收集大量数据后进行分析寻找规律。多变量大数据集无疑会为研究和应用提供丰富的信息，但是也在一定程度上增加了数据采集的工作量。更重要的是在很多情形下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性。如果分别对每个指标进行分析，分析往往是孤立的，不能完全利用数据中的信息，因此盲目减少指标会损失很多有用的信息，从而产生错误的结论。</p><p>因此需要找到一种合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量之间存在一定的相关关系，因此可以考虑将关系紧密的变量变成尽可能少的新变量，使这些新变量是两两不相关的，那么就可以用较少的综合指标分别代表存在于各个变量中的各类信息。主成分分析与因子分析就属于这类降维算法。</p><h3 id="原理">原理</h3><p>PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。</p><p>emm，说的真是难懂，其实是就是如何对数据降维同时有保留最多的信息。推荐一下以数学方式解释的的视频：</p><p><a href="https://www.bilibili.com/video/BV1E5411E71z?spm_id_from=333.337.search-card.all.click">用最直观的方式告诉你：什么是主成分分析PCA_哔哩哔哩_bilibili</a></p><p>以及一个以图形方式解释的视频：</p><p><a href="https://www.bilibili.com/video/BV1C7411A7bj?spm_id_from=333.337.search-card.all.click">【中字】主成分分析法（PCA）| 分步步骤解析 看完你就懂了！_哔哩哔哩_bilibili</a></p><p>简单写一下上述视频所讲内容：</p><p>考虑二维情况，假设数据点有以下分布：</p><p><img src="https://s1.328888.xyz/2022/04/19/rtNDm.png" alt="rtNDm.png"></p><p>我们需要存储的信息为二维，即每个点的x,y坐标，有没有办法将其降为一维呢？肯定是有的，我们需要将坐标轴移动和旋转如下：</p><p><img src="https://s1.328888.xyz/2022/04/19/rtS3A.png" alt="rtS3A.png"></p><p>如此一来，我们需要存储的数据就只有新坐标系的原点，角度和新坐标点的x坐标，数据从二维降为了一维。那么如何实现坐标系的平移和旋转呢？寻找新的坐标系原点其实我们已经实现了，没错，就是上文所述的去中心化。那如何旋转？请看下文。</p><p>首先我们需要明确旋转到怎样一个角度我们认为好呢。上文说我们要实现降维的同时保留尽可能多的信息，何谓保留最多的信息呢？我的理解是，降维之前下隔得远的点降维后仍然隔得远，隔得近的点仍然隔得近，也就是数据之间的分布关系仍需要很好的保留。换句话说，数据之间的偏离程度要尽可能保留。诶，等等，数据之间的偏离程度？方差！！！！</p><center> <mark> Bingo！我们要找到的旋转角度需要使方差最大 </mark></center><p>从几何角度说，因为我们已经去中心化，我们需要找到角度使所有数据点在这条线上的投影点到原点的距离的平方和SSD最大。这条线被称为主成分1，或者说是旋转后的一条坐标轴，该SSD为主成分1的特征值，我们可以在之后证明，如此我们再找到另一条坐标轴主成分2与其SSD。那么我们降为时就可以选取SSD较小的维度删掉以实现降维。</p><p>问题是怎样找到这个角度呢？</p><p>先介绍一下数据的线性变换：</p><p>假设我们有数据集D：<br>$$<br>D=\begin{bmatrix}<br>x_1&amp;x_2&amp;x_3&amp;x_4\\<br>y_1&amp;y_2&amp;y_3&amp;y_4<br>\end{bmatrix}<br>$$<br>与一个对角矩阵S：<br>$$<br>S=\begin{bmatrix}<br>2&amp;0\\<br>0&amp;1<br>\end{bmatrix}<br>$$<br>那SD就等于：<br>$$<br>SD=\begin{bmatrix}<br>2x_1&amp;2x_2&amp;2x_3&amp;2x_4\\<br>y_1&amp;y_2&amp;y_3&amp;y_4<br>\end{bmatrix}<br>$$<br>相当于在x轴上对所有数据拉伸到了2倍。那这跟坐标轴旋转有什么关系呢？看图：</p><p><img src="https://s1.328888.xyz/2022/04/19/r58CT.png" alt="r58CT.png"></p><p>根据“相对论”，坐标轴相对于数据做旋转，相当于所有数据对原坐标轴旋转，如上图所述，假设原坐标轴逆时针旋转$\theta$到红色坐标轴，那点$(x,y)$旋转到点$(x’,y’)$，那对于黑色坐标轴，$(x’,y’)$等于多少呢，稍微用一用初中数学知识我们得到：<br>$$<br>(x’,y’)=(x\cos\theta-y\sin\theta,x\sin\theta+y\cos\theta)<br>$$<br>转化成上述线性变换形式，坐标轴的旋转就相当于：<br>$$<br>RD=\begin{bmatrix}<br>\cos\theta&amp;-\sin\theta\\<br>\sin\theta&amp;\cos\theta<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1&amp;x_2&amp;x_3&amp;x_4\\<br>y_1&amp;y_2&amp;y_3&amp;y_4<br>\end{bmatrix}<br>$$<br>那我们为了让数据的方差更大，可以旋转时对其做一个拉伸,拉伸方向也是使方差最大方向，即：<br>$$<br>D’=RSD=\begin{bmatrix}<br>\cos\theta&amp;-\sin\theta\\<br>\sin\theta&amp;\cos\theta<br>\end{bmatrix}<br>\begin{bmatrix}<br>a&amp;0\\<br>0&amp;b<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1&amp;x_2&amp;x_3&amp;x_4\\<br>y_1&amp;y_2&amp;y_3&amp;y_4<br>\end{bmatrix}<br>$$</p><hr><p>那假设有一个白数据W，其x与y都是服从标准正态分布且不相关，我们所有的数据集W’，其x,y都服从正态分布但非标准且相关，则W’可由W拉伸后旋转得来，即：<br>$$<br>W’=RSW<br>$$<br>反之：<br>$$<br>W=S^{-1}R^{-1}W’<br>$$<br>其中：<br>$$<br>S=\begin{bmatrix}<br>\frac{1}{a}&amp;0\\<br>0&amp;\frac{1}{b}<br>\end{bmatrix}<br>$$</p><p>$$<br>R=\begin{bmatrix}<br>\cos(-\theta)&amp;-\sin(-\theta)\\<br>\sin(-\theta)&amp;\cos(-\theta)<br>\end{bmatrix}<br>=\begin{bmatrix}<br>\cos\theta&amp;\sin\theta\\<br>-\sin\theta&amp;\cos\theta<br>\end{bmatrix}<br>=R^T<br>$$</p><p>问题又来了，R怎么求呢？ <strong>协方差矩阵的特征向量就是R</strong><br>$$<br>协方差：cov(x,y)=\frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{n-1}<br>$$<br>其代表：两个变量在变化过程中是同方向变化还是反方向变化？通向或反向程度如何？</p><p>那么对于我们已经 **去中心化 **的数据，其$\bar x=\bar y=0$,则协方差为：<br>$$<br>cov(x,y)=\frac{\sum_{i=1}^nx_iy_i}{n-1}<br>$$<br>那么协方差矩阵：<br>$$<br>\begin{aligned}<br>C&amp;=\begin{bmatrix}<br>cov(x,x)&amp;cov(x,y)\\<br>cov(x,y)&amp;cov(y,y)<br>\end{bmatrix}\\<br>&amp;=\begin{bmatrix}<br>\frac{\sum_{i=1}^nx_ix_i}{n-1}&amp;\frac{\sum_{i=1}^nx_iy_i}{n-1}\\<br>\frac{\sum_{i=1}^nx_iy_i}{n-1}&amp;\frac{\sum_{i=1}^ny_iy_i}{n-1}<br>\end{bmatrix}\\<br>&amp;=\frac{1}{n-1}\begin{bmatrix}<br>\sum_{i=1}^nx_ix_i&amp;\sum_{i=1}^nx_iy_i\\<br>\sum_{i=1}^nx_iy_i&amp;\sum_{i=1}^ny_iy_i<br>\end{bmatrix}\\<br>&amp;=\frac{1}{n-1}WW^T<br>\end{aligned}<br>$$<br>我们再求一下后的数据D’的协方差：<br>$$<br>\begin{aligned}<br>C’&amp;=\frac{1}{n-1}W’W’^T\\<br>&amp;=\frac{1}{n-1}RSW(RSW)^T\\<br>&amp;=\frac{1}{n-1}RSWW^TS^TR^T\\<br>&amp;=RS(\frac{1}{n-1}WW^T)S^TR^T\\<br>&amp;=RSCS^TR^T<br>\end{aligned}<br>$$<br>W是白数据，其协方差矩阵$C=E$单位矩阵，则上式可以化简为：<br>$$<br>C’=RSESR^{-1}=RS^2R^{-1}=RLR^{-1}<br>$$<br>其中L为一个对角矩阵。</p><p>那么C’的特征值与特征向量定义为：<br>$$<br>C’\vec v=\lambda \vec v<br>$$<br>则：<br>$$<br>C’[\vec v_1,\vec v_2,…,\vec v_n]=[\vec v_1,\vec v_2,…,\vec v_n]\begin{bmatrix}\lambda_1&amp;0&amp;\cdots &amp;0\\<br>0&amp;\lambda_2&amp;\cdots &amp;0\\<br>\vdots&amp;\vdots&amp;\ddots &amp;\vdots\\<br>0&amp;0&amp;\cdots &amp;\lambda_n<br>\end{bmatrix}<br>$$</p><p>$$<br>C’=[\vec v_1,\vec v_2,…,\vec v_n]\begin{bmatrix}\lambda_1&amp;0&amp;\cdots &amp;0\\<br>0&amp;\lambda_2&amp;\cdots &amp;0\\<br>\vdots&amp;\vdots&amp;\ddots &amp;\vdots\\<br>0&amp;0&amp;\cdots &amp;\lambda_n<br>\end{bmatrix}<br>[\vec v_1,\vec v_2,…,\vec v_n]^{-1}<br>$$</p><p>这不就是：<br>$$<br>C’=RLR^{-1}<br>$$<br>对于上述二位情况：<br>$$<br>L=\begin{bmatrix}a^2&amp;0\\0&amp;b^2\end{bmatrix}<br>$$<br>如此一来，我们就能得到旋转回来的数据$R^{-1}W’$的协方差为:<br>$$<br>\begin{aligned}<br>C_{R^{-1}W’}&amp;=\frac{1}{n-1}R^{-1}W’(R^{-1}W’)^T\\<br>&amp;=\frac{1}{n-1}R^{-1}W’W’^T(R^{-1})^T\\<br>&amp;=R^{-1}(\frac{1}{n-1}WW^T)(R^{-1})^T\\<br>&amp;=R^{-1}C’(R^{-1})^T\\<br>&amp;=R^{-1}RSS^TR^T(R^{-1})^T=SS^t=L<br>\end{aligned}<br>$$<br>则$a^2,b^2$既是两个轴方向的方差。同时又是协方差矩阵C’的特征值SSD。</p><p>那我们选择其中方差小的那个维度删掉，不就实现了降维？？？</p><p>总结一下PCA的步骤：</p><ul><li><p>去中心化</p></li><li><p>计算数据协方差矩阵及其特征值特征向量，求$R^{-1}W’$</p></li><li><p>降维：选择特征值较小的维度删除</p><p><strong>PCA的一个显著缺点是离群点会极大影响主成分的角度，造成算法效果不好</strong></p><h3 id="python实现">python实现</h3></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pca</span>(<span class="params">X,k</span>):<span class="comment">#k is the components you want</span></span><br><span class="line">  <span class="comment">#mean of each feature</span></span><br><span class="line">  n_samples, n_features = X.shape</span><br><span class="line">  mean=np.array([np.mean(X[:,i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_features)])</span><br><span class="line"> </span><br><span class="line">  norm_X=X-mean <span class="comment">#去中心化</span></span><br><span class="line"></span><br><span class="line">  scatter_matrix=np.dot(np.transpose(norm_X),norm_X)<span class="comment">#协方差矩阵</span></span><br><span class="line"> </span><br><span class="line">  eig_val, eig_vec = np.linalg.eig(scatter_matrix)<span class="comment">#协方差矩阵的特征向量和特征值</span></span><br><span class="line">  eig_pairs = [(np.<span class="built_in">abs</span>(eig_val[i]), eig_vec[:,i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_features)]</span><br><span class="line"></span><br><span class="line">  eig_pairs.sort(reverse=<span class="literal">True</span>)<span class="comment">#由大到小排序</span></span><br><span class="line"></span><br><span class="line">  feature=np.array([ele[<span class="number">1</span>] <span class="keyword">for</span> ele <span class="keyword">in</span> eig_pairs[:k]])<span class="comment">#留下k个主要特征</span></span><br><span class="line"></span><br><span class="line">  data=np.dot(norm_X,np.transpose(feature))</span><br><span class="line">  <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">X = np.array([[-<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">2</span>, -<span class="number">1</span>], [-<span class="number">3</span>, -<span class="number">2</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(pca(X,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/19/rW3cZ.png" alt="rW3cZ.png"></p><h2 id="白噪声处理">白噪声处理</h2><p>在PCA处理后在除以其特征值得到最开始的白数据：<br>$$<br>W=S^{-1}R^{-1}W’<br>$$<br>白噪声处理会增强数据中的噪声，因为其增强了所有维度，包括一些方差很小的不相关维度</p><h1>网络参数初始化</h1><h2 id="权重初始化">权重初始化</h2><p>除了数据需要预处理之外，网络权重也需要初始化。</p><ul><li><p><s>全0初始化</s>：看起来最简单但是 <strong>不可行</strong>，如果神经网络的每个权重都被初始化成相同的值，那么每个神经元就会就是计算出相同的结果，在反向传播时也会又相同的梯度，最后导致所有权重的都会有相同的更新，权重之间失去了对称性</p></li><li><p>随机初始化：初始化为一些靠近0的随机数</p></li><li><p>稀疏初始化：将权重全部初始化维0，然后道破对称性在立马随机挑选一些参数附上一些随机值</p></li></ul><h2 id="初始化偏置">初始化偏置</h2><p>一般初始化为全0</p><h2 id="批标准化-Batch-Normalization">批标准化(Batch Normalization)</h2>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 预处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分类问题</title>
      <link href="/2022/04/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
      <url>/2022/04/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1>分类问题</h1><h2 id="问题描述">问题描述</h2><p>机器学习中有监督学习主要分为回归问题和分类问题，回归问题希望预测的结果是连续的，为什么不能用之前线性回归的方法来处理分类问题呢？答案其实是可以，不过我们在计算损失函数时需要对预测值远远偏离真实值的对象进行打击。理由也很简单，因为这些过大的偏差会使我们的回归结果向减小这类偏差的方向移动，也就是说距离分类边界太原的数据点会迫使分类边界向其方向移动，如图所示：</p><p><img src="https://s1.328888.xyz/2022/04/18/rr6XA.png" alt="rr6XA.png"></p><p>分类问题所预测的结果是离散的“类别”。这是输入变量可以是离散的也可以是连续的，监督学习从数据中学习一个分类模型或分类决策函数就是分类器，分类器根据输入变量对输出进行预测，即为分类，</p><p>分类问题太好理解了，生活中处处是分类，在此不再赘述。</p><p>分类问题中最简单的自然就是：二分类went</p><h2 id="二分类问题">二分类问题</h2><p>说起二分类问题，第一个想到的自然就是Logistic(对数几率），首先了解以下什么是Logistic分布：</p><h3 id="Logistic分布">Logistic分布</h3><p>服从Logistic分布是指X的分布函数和概率密度函数服从：<br>$$<br>F(x)=P(X\leq x)=\frac{1}{1+e^{-(x-\mu)\gamma}}<br>$$</p><p>$$<br>f(x)=\frac{e^{-(x-\mu)\gamma}}{\gamma(1+e^{-(x-\mu)\gamma})^2}<br>$$</p><p>其中$\mu$影响 <strong>中心对称点位置</strong> ，$\gamma$越小中心点附近的增长速度越开。而Sigmoid函数就是$\mu =0,\gamma=1$的一个特例：</p><p><img src="https://s1.328888.xyz/2022/04/18/r2QAF.png" alt="r2QAF.png"></p><h3 id="对数几率回归">对数几率回归</h3><p>对于二分类Logistic问题，其目标是希望找到一个区分度足够好的决策边界，能够将两类很好的分开。</p><p>​假设输入数据的特征向量$x\in R^n$,那么剧场边界可以表示为$\sum_{i=1}^n w_ix_i+b=0$,假设预测样本的$x_0$,使得$h(x_0)=\sum_{i=1}^n w_ix_i+b&gt;0$,则判断其为1类，反之若$h(x_0)=\sum_{i=1}^n w_ix_i+b&lt;0$,判断其为0类。而Logistic回归要跟进一步，利用比较概率值来判断类别。</p><p>将sigmoid函数应用于分类问题：<br>$$<br>y=\frac{1}{1+e^{({\bf w^Tx}+b)}}<br>$$<br>若将y视为样本x为正例的可能性$p(y=1\vert x)$，则1-y为样本为反例的可能性$p(y=0\vert x)$，用概率更改上述公式为：</p><p>显然：<br>$$<br>p(y=1\vert x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}<br>$$</p><p>$$<br>p(y=0\vert x)=\frac{1}{1+e^{w^Tx+b}}<br>$$</p><p>我们给出一个定义，几率：只一个事件发生的概率与不发生概率的比值，那某事件发生的对数几率就为：<br>$$<br>\ln it §=\ln \frac{p}{1-p}<br>$$</p><p>$$<br>\ln\frac{p(y=1\vert x)}{p(y=0\vert x)}=w^Tx+b<br>$$<br>观察上式等号右边项，只想道一声别来无恙，这不就是线性回归的模型吗？那不就是把线性回归的推到再推一遍呐，不过不一样的是我们此时不需要回归，而是分类，也就是说我们不需要其求出来的Loss最小，而是要似然概率最大。</p><p>什么？你不知道什么是似然概率？那您自己百度吧。简单的说就是根据结果推导分布。</p><p>给定数据集${(x_i,y_i)}_{i=1}^m$,对于每一对数据，预测正确的概率为：<br>$$<br>p(y_i\vert x_i;w,b)=y_ip(\hat y_i=1\vert x;w,b)+(1-y_i)p(\hat y_i=0\vert x;w,b)<br>$$<br>其中 $y_i$分为0，1两类.</p><p>假设其中有n个分类为1，则似然函数为：</p><p>$$<br>\prod _{i=1}^m[p(\hat y_i=1\vert x;w,b)]^n[p(\hat y_i=1\vert x;w,b)]^{m-n}<br>$$</p><p>二项分布嘛，我们找到合适的参数$w,b$使其最大，乘法肯定不好计算，所以我们使用对数简化为加法，即最大化似然对数：<br>$$<br>l(w,b)=\sum_{i=1}^m\ln p(y_i\vert x_i;w,b)<br>$$<br>上式等价于：<br>$$<br>\begin{aligned}<br>l(w,b)&amp;=\sum_{i=1}^mp(y_i\vert  x_i;w,b)\\<br>&amp;=\sum_{i=1}^m\ln \frac{y_i(e^{w^Tx_i+b})+(1-y_i)}{1+e^{w^T  x_i+b}}\\<br>&amp;=\sum_{i=1}^m[ln(y_ie^{w^T  x_i+b}+(1-y_i))-\ln(1+e^{w^T  x_i+b})]\\<br>&amp;=\sum_{i=1}^m(y_i(w^Tx_i+b)-\ln (1+e^{w^T  x_i+b} ))<br>\end{aligned}<br>$$</p><p>最简单就是使用梯度下降法求解上述最大化问题：<br>$$<br>\begin{aligned}<br>\frac{\partial L(w)}{\partial w}&amp;=\sum_{i=1}^my_ix_i-\sum_{i=1}^m\frac{e^{w^Tx_i+b}}{1+e^{w^Tx_i+b}}x_i\\<br>&amp;=\sum_{i=1}^m(y_i-p(y_i=1\vert x_i))x_i<br>\end{aligned}<br>$$</p><p>$$<br>\begin{aligned}<br>\frac{\partial L(w)}{\partial b}&amp;=\sum_{i=1}^my_ix_i-\sum_{i=1}^m\frac{e^{w^Tx_i+b}}{1+e^{w^Tx_i+b}}\\<br>&amp;=\sum_{i=1}^m(y_i-p(y_i=1\vert x_i))<br>\end{aligned}<br>$$</p><p>注意上述公式中$w,x_i$都为列向量，类似多元线性回归中的情形。</p><p>鸢尾花数据集的二分类实现：</p><p><img src="https://s1.328888.xyz/2022/04/18/rhaiq.png" alt="rhaiq.png"></p><p>输入x特征向量为4维,通过PCA降维到二维，二分类</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">import os,sys</span><br><span class="line">import matplotlib.pylab as plt </span><br><span class="line">os.chdir(sys.path[0])</span><br><span class="line">def pca(X,k):#k is the components you want</span><br><span class="line">  #mean of each feature</span><br><span class="line">  n_samples, n_features = X.shape</span><br><span class="line">  mean=np.array([np.mean(X[:,i]) for i in range(n_features)])</span><br><span class="line"> </span><br><span class="line">  norm_X=X-mean #去中心化</span><br><span class="line"></span><br><span class="line">  scatter_matrix=np.dot(np.transpose(norm_X),norm_X)#协方差矩阵</span><br><span class="line"> </span><br><span class="line">  eig_val, eig_vec = np.linalg.eig(scatter_matrix)#协方差矩阵的特征向量和特征值</span><br><span class="line">  eig_pairs = [(np.abs(eig_val[i]), eig_vec[:,i]) for i in range(n_features)]</span><br><span class="line"></span><br><span class="line">  eig_pairs.sort(reverse=True)#由大到小排序</span><br><span class="line"></span><br><span class="line">  feature=np.array([ele[1] for ele in eig_pairs[:k]])#留下k个主要特征</span><br><span class="line"></span><br><span class="line">  data=np.dot(norm_X,np.transpose(feature))</span><br><span class="line">  return data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#数据读取</span><br><span class="line">label_set=lambda x: 1 if x==&#x27;&quot;setosa&quot;&#x27; else 0</span><br><span class="line">with open(&#x27;./iris/iris.txt&#x27;) as f:</span><br><span class="line">    data_list=f.readlines()</span><br><span class="line">    data_list=[i.split(&#x27;\n&#x27;)[0]for i in data_list[1:]]</span><br><span class="line">    data_list=[i.split(&#x27; &#x27;) for i in data_list]</span><br><span class="line">    data=[[float(i[1]),float(i[2]),float(i[3]),float(i[4]),label_set(i[5])] for i in data_list]</span><br><span class="line"></span><br><span class="line">x_data=[[i[0],i[1],i[2],i[3]]for i in data]</span><br><span class="line">y_data=[[i[4]] for i in data]</span><br><span class="line">x_data=pca(np.array(x_data),2)</span><br><span class="line">x_data=torch.from_numpy(x_data).float()</span><br><span class="line">y_data=torch.from_numpy(np.array(y_data)).float()</span><br><span class="line">#模型定义</span><br><span class="line">class Logistic_Regreession(nn.Module):</span><br><span class="line">    def __init__(self) -&gt; None:</span><br><span class="line">        super(Logistic_Regreession,self).__init__()</span><br><span class="line">        self.LR=nn.Linear(2,1)</span><br><span class="line">        self.sm=nn.Sigmoid()</span><br><span class="line">    def forward(self,x):</span><br><span class="line">        y=self.LR(x)</span><br><span class="line">        output=self.sm(y)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">if torch.cuda.is_available():</span><br><span class="line">    x_data=x_data.cuda()</span><br><span class="line">    y_data=y_data.cuda()</span><br><span class="line">    model=Logistic_Regreession().cuda()</span><br><span class="line">else:</span><br><span class="line">    model=Logistic_Regreession()</span><br><span class="line">criterion=nn.MSELoss()</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=1e-3,weight_decay=0.05,momentum=0.78)</span><br><span class="line"></span><br><span class="line">epochs=2000</span><br><span class="line">for epoch in range(epochs):</span><br><span class="line">    output=model(x_data)</span><br><span class="line">    mask=output.ge(0.5).float()</span><br><span class="line"></span><br><span class="line">    correct=(mask==y_data).sum()</span><br><span class="line">    acc=correct/y_data.shape[0]</span><br><span class="line">    loss=criterion(output,y_data)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    if epoch%10==0:</span><br><span class="line">        print(&quot;epoch:&#123;&#125;,loss:&#123;:.6f&#125;,acc:&#123;:.4f&#125;&quot;.format(epoch,loss,acc))</span><br><span class="line">x=x_data.numpy()[:,0]</span><br><span class="line">y=x_data.numpy()[:,1]</span><br><span class="line">x1=[]</span><br><span class="line">y1=[]</span><br><span class="line">y2=[]</span><br><span class="line">x2=[]</span><br><span class="line">for i in range(len(x)):</span><br><span class="line">    if y_data[i]==0:</span><br><span class="line">        x1.append(x[i])</span><br><span class="line">        y1.append(y[i])</span><br><span class="line">    else:</span><br><span class="line">        x2.append(x[i])</span><br><span class="line">        y2.append(y[i])</span><br><span class="line">w=model.LR.weight[0]</span><br><span class="line">w0=w[0].detach().numpy()</span><br><span class="line">w1=w[1].detach().numpy()</span><br><span class="line">b=model.LR.bias.data[0].numpy()</span><br><span class="line">plot_x=np.arange(-2,3,0.01)</span><br><span class="line">plot_y=(-w0*plot_x+b)/w1</span><br><span class="line">plt.scatter(x1,y1,color=&#x27;red&#x27;)</span><br><span class="line">plt.scatter(x2,y2,color=&#x27;blue&#x27;)</span><br><span class="line">plt.plot(plot_x,plot_y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/18/rh3ce.png" alt="rh3ce.png"></p><p><img src="https://s1.328888.xyz/2022/04/19/rz92Q.png" alt="rz92Q.png"></p><h2 id="多分类问题">多分类问题</h2><p>多分类问题一般都是将其简化为多个二分类问题使其得以解决：</p><p>主要做法有以下三类：</p><h3 id="一对一-OvO">一对一(OvO)</h3><p>one vs one:</p><p>给定多分类$y_i \in {C_1,C_2,…C_N}$,n分类问题，OVO将这N个类别两两配对，从而产生$\frac{N(N-1)}{2}$个二分类任务，然后对某一输入做预测时，这$\frac{N(N-1)}{2}$个分类器分别各自作出分类，最终将被预测得最多的类别作为分类结果。参考如下图</p><h3 id="一对其余-OvR">一对其余(OvR)</h3><p>One VS Rest：</p><p>每次将一个类作为正例，其他所有类作为反例来训练N个分类器，在测试时若只有一个分类器作为正类，则将其作为最终分类。</p><p><img src="https://s1.328888.xyz/2022/04/18/rhFtS.png" alt="rhFtS.png"></p><h3 id="多对多-MvM">多对多(MvM)</h3><p>Many VS Many ：</p><p>每次将若干个类作为正类，若干个其他类作为反类。MVM的正反类构造必须有 <strong>特殊的设计</strong>，不能随意选取，常用技术为：纠错输出码’</p><h4 id="纠错输出码-Error-Correcting-Output-Codes">纠错输出码(Error Correcting Output Codes)</h4><p>ECOC将编码思想引入类别拆分：</p><ul><li>编码：对N个类别做M次划分，每次划分将一部分分类化为正类，一部分化为反类，从而形成一个二分类训练集；这样一共产生M个训练集，可以训练出M个分类器</li><li>M个分类器分别对测试样本进行预测，这些预测标记组成一个编码，将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的作为最终预测结果</li></ul><p>类别划分通过&quot;编码矩阵&quot;完成，常用编码矩阵有二元码和三元码，前者将类别分别指定为正类和反类，后者除了正反类之外，还可以指定停用类。</p><p><img src="https://s1.328888.xyz/2022/04/18/rhBZy.png" alt="rhBZy.png"></p><p>纠错输出码有一定的纠错能力，比如图(a)中，正确的预测编码为（-1，+1，+1，-1，+1），但$f_2$分类器出错导致其编码为(-1,-1,+1,-1,+1),但基于该编码仍然正确分类为$C_3$</p><h2 id="类别不平衡问题">类别不平衡问题</h2><p>以上各个分开类的实现前提都是 <strong>我的数据集中各类数据基本相当</strong>，若是出现类别不平衡情况：比如二分类问题中，数据集有998个正例却只有2个反例，这样的数据所训练的分类器没有任何价值，因为它会将所有测试数据预测为正例。</p><p>有办法解决这个问题吗？当然有，记得我们在分类器sigmoid函数输出之后，将其与0.5比较，大于0.5的判为正例，小于0.5的判为反例。为何一定是0.5呢？因为我们假设正反例可能性相同，即分类器比率决策规则为：<br>$$<br>若\frac{y}{1-y}&gt;1,则其为正例<br>$$<br>那既然数据集中正反例观测概率不为0.5，那我们何不改变此规则呢？</p><p>若数据集中正例的个数为$m^+$,反例数为$m^-$,则决策规则更改为：<br>$$<br>若\frac{y}{1-y}&gt;\frac{m^+}{m^-},则其为正例<br>$$<br>等价于：<br>$$<br>若\frac{y’}{1-y’}=\frac{y}{1-y}\times \frac{m^-}{m^+}&gt;1,则其为正例<br>$$<br>这就是所谓的——再放缩</p><p>该方法的思想虽然简单，但是在实现过程中不一定有效，因为我们所得的正反例比率仅仅是通过训练集统计而得，虽然我们假设“训练集是样本总体的无偏采样”，但实际却往往并不成立，也就是说，我们未必能基于训练集观测几率来推测出样本总体的真实几率。</p><p>所以实际中主要有三种办法：</p><ul><li>欠采样：除去一些反例或正例使得正反例数目接近</li><li>过采样：增加一些正例或者反例使得正反例数目接近</li><li>阈值移动：就是在决策时采用上文所述的再缩放方法</li></ul>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 分类问题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> logistic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FuzzyLogicReport2</title>
      <link href="/2022/04/17/FuzzyLogicReport2/"/>
      <url>/2022/04/17/FuzzyLogicReport2/</url>
      
        <content type="html"><![CDATA[<div class="row">    <embed src="https://yangyin.cool/pdf/FuzzyLogic/test.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>模型的保存与加载</title>
      <link href="/2022/04/17/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/"/>
      <url>/2022/04/17/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/</url>
      
        <content type="html"><![CDATA[<h1>模型的保存与加载</h1><h2 id="保存与加载整个模型的结构信息和参数信息">保存与加载整个模型的结构信息和参数信息</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.save(model,&#x27;./model.pth&#x27;)</span><br><span class="line">load_model=torch.load(&#x27;model.pth&#x27;)</span><br></pre></td></tr></table></figure><h2 id="保存与加载整个模型的参数信息">保存与加载整个模型的参数信息</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.save(model.state_dict(),&#x27;./model_state.pth&#x27;)</span><br><span class="line">load_model=model.load_state_dic(torch.load(&#x27;model_state.pth&#x27;))</span><br></pre></td></tr></table></figure><p><mark>load_model加载前需要先实例化</mark></p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPU加速与可视化</title>
      <link href="/2022/04/17/GPU%E5%8A%A0%E9%80%9F%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
      <url>/2022/04/17/GPU%E5%8A%A0%E9%80%9F%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1>GPU与可视化</h1><h2 id="Gpu加速">Gpu加速</h2><p>GPU加速</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device=torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">net=MLP.to(device)</span><br><span class="line">对应数据也需要加入.to(device)或者使用data.cuda（）</span><br></pre></td></tr></table></figure><p><code>.item()</code>：取tensor中的值</p><h2 id="Visdom可视化">Visdom可视化</h2><h3 id="tensorboardX">tensorboardX</h3><p><code>pip install tensorboardX</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer=SummaryWriter()</span><br><span class="line">writer.add_scalar(<span class="string">&#x27;data/scalar_group&#x27;</span>,&#123;<span class="string">&#x27;xsinx&#x27;</span>:n_iter*np.sin(n_iter),<span class="string">&#x27;xcos&#x27;</span>:n_iter*np.cos(n_iter),n_iter&#125;)</span><br><span class="line">writer.add_image(<span class="string">&#x27;Image&#x27;</span>,x,n_iter)</span><br><span class="line">writer.add_text(<span class="string">&#x27;Text&#x27;</span>,<span class="string">&#x27;text logged at step:&#x27;</span>+<span class="built_in">str</span>(n_iter),n_iter)</span><br><span class="line"><span class="keyword">for</span> name,param <span class="keyword">in</span> resnet18.named_parameters():</span><br><span class="line">    writer.add_histogram(name,param.clone().cpu().adta.numpy(),n_iter)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h3 id="Visdom">Visdom</h3><p>运行效率更高</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install visdom</span><br><span class="line">python -m visdom.server</span><br><span class="line">如果运行报错，则重新下载visdom后通过进入文件目录后</span><br><span class="line">pip install -e 安装</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#一条线</span></span><br><span class="line"><span class="keyword">from</span> visdom <span class="keyword">import</span> Visdom</span><br><span class="line">viz=Visdom()</span><br><span class="line">viz.line([<span class="number">0.</span>],[<span class="number">0.</span>],win=<span class="string">&#x27;train_loss&#x27;</span>,opts=<span class="built_in">dict</span>(title=<span class="string">&#x27;train loss&#x27;</span>))<span class="comment">#创建一条直线（y,x,ID,opt:属性配置）</span></span><br><span class="line">viz.line([loss.item()],[global_step],win=<span class="string">&#x27;train_loss&#x27;</span>,update=<span class="string">&#x27;append&#x27;</span>)<span class="comment">#将数据添加到直线中（update操作）</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#多条线</span></span><br><span class="line"><span class="keyword">from</span> visdom <span class="keyword">import</span> Visdom</span><br><span class="line">viz=Visdom()</span><br><span class="line">viz.line([<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">0.0</span>,<span class="number">0.0</span>],win=<span class="string">&#x27;test&#x27;</span>,opts=<span class="built_in">dict</span>(title=<span class="string">&#x27;test loss&amp;acc&#x27;</span>，legend=[<span class="string">&#x27;loss&#x27;</span>,<span class="string">&#x27;acc&#x27;</span>]))<span class="comment">#创建两条直线（[y1,y2],x,ID,opt:属性配置）</span></span><br><span class="line">viz.line([[test_loss,cprrect/<span class="built_in">len</span>(test_loader.dataset)]],[global_step],win=<span class="string">&#x27;test&#x27;</span>,update=<span class="string">&#x27;append&#x27;</span>)<span class="comment">#将数据添加到直线中（update操作）</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#可视化image与pred</span><br><span class="line">from visdom import Visdom</span><br><span class="line">viz=Visdom()</span><br><span class="line">viz.images(data.reshape(-1,1,28,28),win=&#x27;x&#x27;)</span><br><span class="line">viz.text(str(pred.detach().cpu().numpy),win=&#x27;pred&#x27;,opts=dict(title=&#x27;pred&#x27;))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性模型</title>
      <link href="/2022/04/17/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/2022/04/17/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1>线性模型</h1><h2 id="问题描述-2">问题描述</h2><p><img src="https://s1.328888.xyz/2022/04/17/reHbW.png" alt="线性回归"></p><p>通俗点讲，就是给一堆点，找到一条直线使所有点到直线的距离之和最小。数学描述是给定由d个属性描述的示例 ${\bf x}=(x_1,x_2,…,x_d)$,其中$x_i$表示第i个属性上的取值，线性模型试图学得一个通过属性线性组合来进行预测的函数：<br>$$<br>f(x)=w_1x_1+w_2x_2+…+w_dx_d+b<br>$$<br>写成向量形式：<br>$$<br>f(x)={\bf w^Tx}+b<br>$$<br>其中${\bf w}=(w_1,…,w_d)$与$b$ 需要由学习所得</p><h3 id="广义线性回归：">广义线性回归：</h3><p>考虑可微单调函数<code>g(·)</code><br>$$<br>y=g^{-1}(w^Tx)+b<br>$$</p><h2 id="一维线性回归">一维线性回归</h2><p>给定数据集$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$,要使的$f(x_i)=w_i+b$能够尽量与$y_i$接近</p><p>loss：<br>$$<br>Loss=\sum_{i=1}^m(f(x_i)-y_i)^2<br>$$<br>我们需要找到:</p><p>$$<br>\begin{aligned}<br>(w^*,b^*)&amp;=arg\min_{w,b}\sum_{i=1}^m(f(x_i)-y_i)^2\\<br>&amp;=arg\min_{w,b}\sum_{i=1}^m(y_i-wx_i-b)^2<br>\end{aligned}<br>$$</p><p>令其偏导等于0：<br>$$<br>\frac{\partial Loss_{(w,b)}}{\partial w}=2(w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i)=0<br>$$</p><p>$$<br>\frac{\partial Loss_{(w,b)}}{\partial b}=2(mb-\sum_{i=1}^m(y_i-wx_i))=0<br>$$</p><p>可得：<br>$$<br>w=\frac{\sum_{i=1}^my_i(x_i-\bar x)}{\sum_{i=1}^mx_i^2-\frac{1}{m}(\sum_{i=1}^mx_i)^2}<br>$$</p><p>$$<br>b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i)<br>$$</p><h2 id="多维线性回归">多维线性回归</h2><p>$$<br>f(x)={\bf w^Tx}+b<br>$$</p><p>将数据集表示为$m\times (d+1)$ 的矩阵形式：<br>$$<br>X=<br>\begin{pmatrix}<br>{x_{11}}&amp;{x_{12}}&amp;{\cdots}&amp;{x_{1d}}&amp;1\\<br>{x_{21}}&amp;{x_{22}}&amp;{\cdots}&amp;{x_{2d}}&amp;1\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}&amp;{\vdots}\\<br>{x_{m1}}&amp;{x_{m2}}&amp;{\cdots}&amp;{x_{md}}&amp;1<br>\end{pmatrix}<br>=\begin{pmatrix}<br>{x_1^T}&amp;1\\<br>{x_2^T}&amp;1\\<br>{\vdots}&amp;{\vdots}\\<br>{x_m^T}&amp;1<br>\end{pmatrix}<br>$$<br>目标y也写成向量形式$y=(y_1,y_2,…,y_m)$,则：<br>$$<br>w^*=(w_1,w_2,…,w_d,b)<br>$$</p><p>$$<br>w^*=arg\min_w(y-Xw)^T(y-Xw)<br>$$<br>矩阵求导可参考<a href="https://zhuanlan.zhihu.com/p/273729929">矩阵求导公式的数学推导（矩阵求导——基础篇） - 知乎 (zhihu.com)</a></p><p>对其求导：<br>$$<br>\frac{\partial Loss_{(w,b)}}{\partial w}=2X^T(Xw-y)=0<br>$$<br>若$X^TX$为满秩矩阵，则：<br>$$<br>w^*=(X^TX)^{-1}X^Ty<br>$$<br>此时：<br>$$<br>f(\hat x_i)=\hat x_i^T(X^TX)^{-1}X^Ty<br>$$</p><p>然而实际情况是，$X^TX$往往不可逆，此时可以解出多个$\hat w$, 他们都能使均方误差最小化，选择哪一个作为输出将由算法的归纳偏好决定，常见做法是引入正则化。</p><h2 id="pytorch一维线性回归代码实现">pytorch一维线性回归代码实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> turtle <span class="keyword">import</span> forward</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备</span></span><br><span class="line">x_train=np.array([[<span class="number">3.3</span>],[<span class="number">4.4</span>],[<span class="number">5.5</span>],[<span class="number">6.71</span>],[<span class="number">6.93</span>],[<span class="number">4.168</span>],[<span class="number">9.779</span>],[<span class="number">6.182</span>],[<span class="number">7.59</span>],[<span class="number">2.167</span>],[<span class="number">7.042</span>],[<span class="number">10.791</span>],[<span class="number">5.313</span>],[<span class="number">7.997</span>],[<span class="number">3.1</span>]],dtype=np.float32)</span><br><span class="line">y_train=np.array([[<span class="number">1.7</span>],[<span class="number">2.76</span>],[<span class="number">2.09</span>],[<span class="number">3.19</span>],[<span class="number">1.694</span>],[<span class="number">1.573</span>],[<span class="number">3.366</span>],[<span class="number">2.596</span>],[<span class="number">2.53</span>],[<span class="number">1.221</span>],[<span class="number">2.827</span>],[<span class="number">3.465</span>],[<span class="number">1.65</span>],[<span class="number">2.904</span>],[<span class="number">1.3</span>]],dtype=np.float32)</span><br><span class="line">point=plt.scatter(x_train,y_train)</span><br><span class="line">x_train=torch.from_numpy(x_train)</span><br><span class="line">y_train=torch.from_numpy(y_train)</span><br><span class="line"><span class="comment"># 模型定义y=wx+b</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegreession</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(LinearRegreession,self).__init__() <span class="comment">#调用父类的初始化方法</span></span><br><span class="line">        self.linear=nn.Linear(<span class="number">1</span>,<span class="number">1</span>)<span class="comment">#输入输出都为1维</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        out=self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model=LinearRegreession().cuda() <span class="comment">#如果有GPU,则加载到GPU上运行</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model=LinearRegreession()</span><br><span class="line"></span><br><span class="line"><span class="comment">#LOSS函数</span></span><br><span class="line">criterion=nn.MSELoss()</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=<span class="number">1e-3</span>)<span class="comment">#梯度下降算法</span></span><br><span class="line"></span><br><span class="line">epochs=<span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">#判断是否使用GPU</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        inputs=x_train.cuda()</span><br><span class="line">        target=y_train.cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inputs=x_train</span><br><span class="line">        target=y_train</span><br><span class="line">    <span class="comment">#forward</span></span><br><span class="line">    out=model(inputs)</span><br><span class="line">    loss=criterion(out,target)</span><br><span class="line">    <span class="comment">#backward</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    loss.backward()<span class="comment">#求导</span></span><br><span class="line">    optimizer.step()<span class="comment">#反向更新w与b</span></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>)%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch[&#123;&#125;/&#123;&#125;],loss:&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>,epochs,loss))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">predict=model(x_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(predict))</span><br><span class="line">predict=predict.detach().numpy()</span><br><span class="line">plt.plot(x_train.numpy(),y_train.numpy(),<span class="string">&#x27;ro&#x27;</span>,label=<span class="string">&#x27;Original data&#x27;</span>)</span><br><span class="line">plt.plot(x_train.numpy(),predict,label=<span class="string">&#x27;fitting line&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/17/ryT8g.png" alt="结果.png"></p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 线性模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>激活函数与loss函数</title>
      <link href="/2022/04/13/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8ELoss%E5%87%BD%E6%95%B0/"/>
      <url>/2022/04/13/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8ELoss%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1>激活函数</h1><p><img src="https://s1.328888.xyz/2022/04/13/fx5H4.png" alt="fx5H4.png"><br>$$<br>y=f(\sum_{i=1}^nw_ix_i+b)<br>$$<br>此f()为激活函数，输入为前一层所有输出的加权和再加上一个偏执b</p><h2 id="sigmoid">sigmoid</h2><p>$$<br>f(x)=\sigma(x)=\frac{1}{1+ e^x}\<br>$$</p><p>$$<br>\frac{\partial f(x)}{\partial x}=f(x)(1-f(x))<br>$$</p><p>优点：求导简单，数据可压缩到（0，1）</p><p>缺点：梯度离散，输入很大时输出太平缓</p><p>调用指令<code>torch.sigmoid()</code></p><h2 id="Tanh">Tanh</h2><p>$$<br>f(x)=tanh(x)=\frac{(e^x-e^{-x})}{(e^x+e^{-x})}<br>=2sigmoid(2x)-1<br>$$</p><p>$$<br>\frac{\partial f(x)}{\partial x}=1-tanh^2(x)<br>$$</p><p>调用指令<code>torch.tanh()</code></p><h2 id="ReLU">ReLU</h2><p>$$<br>f(x)=\begin{cases}0\ for\ x&lt;0 \\<br>x\ for\ x\geq0\end{cases}<br>$$</p><p>调用指令<code>torch.relu()</code></p><h1>Loss函数</h1><h2 id="MSE">MSE</h2><p>$$<br>loss_i=\sum(y_i-\bar y_i)^2=norm((y_i-\hat y_i),2)^2<br>$$</p><p>调用指令 torch.nn.functional.mse_loss($y,\hat y$')</p><p>自动求导:</p><p>​<strong>方法一</strong><br>1. 首先对需要求导的参数使用<code>requires_grad_()</code>方法标明需要求导<br>2. 计算mse：torch.nn.functional.mse_loss($y$,$\hat y$)<br>3. 使用<code>torch.autograd.grad(mse,[w])</code>对其进行求导</p><p>​<strong>方法二</strong></p><ol><li><p>首先对需要求导的参数使用<code>requires_grad_()</code>方法标明需要求导</p></li><li><p>计算mse：torch.nn.functional.mse_loss($y，\hat y$)</p></li><li><p>调用<code>mse.backward</code>该指令会计算mse对所有已设置需要求导变量的梯度</p></li><li><p>调用<code>w.grad</code>显示梯度</p><p><mark>backward设置(retain_graph=True)才可以再一次调用，不设置则会报错</mark></p></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">x=torch.ones(<span class="number">1</span>)</span><br><span class="line">w=torch.Tensor([<span class="number">2</span>])</span><br><span class="line">w.requires_grad_()</span><br><span class="line">mse=F.mse_loss(torch.ones(<span class="number">1</span>),x*w)</span><br><span class="line"><span class="comment">#torch.autograd.grad(mse,[w])</span></span><br><span class="line">mse.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([2.])</span></span><br></pre></td></tr></table></figure><h2 id="Softmax">Softmax</h2><p>$$<br>softmax(y_i)=\frac{e^{y_i}}{\sum_ je^{y_j}} \<br>$$</p><p>$$<br>\frac{\partial softmax(y_i)}{\partial y_j}=\begin{cases}softmax(y_i)(1-softmax(y_j))\ \ if\  i=j\\<br>-softmax(y_i)softmax(y_j)\ \ \ \ \ \ \ \ \ if\ i\neq j\end{cases}<br>$$</p><p>优点：将输出label的probability压缩到（0，1），且所有probability之和为1，原数据间隔拉大</p><p>调用指令 <code>torch.nn.functional.mse_loss(y,dim=x)</code></p><h2 id="交叉熵">交叉熵</h2><p>$$<br>H(P,Q)=-\sum_{i=1}^nP(i)logQ(i)<br>$$</p><p>通常用于分类问题</p><p>调用指令 <code>torch.nn.functional.cross_entropy(logits,y)</code></p><p>或者</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred=torch.nn.functional.softmax(logits,dim=<span class="number">1</span>)</span><br><span class="line">pred_log=torch.nn.functional.log(pred)</span><br><span class="line">torch.nn.functional.nll_loss(pred_log,y)</span><br></pre></td></tr></table></figure><p><mark>cross_entropy=softmax→log→nll_loss</mark></p><h2 id="一个二元二次函数梯度下降求极值算法">#一个二元二次函数梯度下降求极值算法</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#一个二元二次函数梯度下降求极值算法</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">himmelblau</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span>(x[<span class="number">0</span>]**<span class="number">2</span>+x[<span class="number">1</span>]-<span class="number">11</span>)**<span class="number">2</span>+(x[<span class="number">0</span>]+x[<span class="number">1</span>]**<span class="number">2</span>-<span class="number">7</span>)**<span class="number">2</span></span><br><span class="line">x=np.arange(-<span class="number">6</span>,<span class="number">6</span>,<span class="number">0.1</span>)</span><br><span class="line">y=np.arange(-<span class="number">6</span>,<span class="number">6</span>,<span class="number">0.1</span>)</span><br><span class="line">X,Y=np.meshgrid(x,y)</span><br><span class="line">Z=himmelblau([X,Y])</span><br><span class="line"></span><br><span class="line">fig=plt.figure(<span class="string">&quot;himelblau&quot;</span>)</span><br><span class="line">ax=fig.gca(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(X,Y,Z)</span><br><span class="line">ax.view_init(<span class="number">60</span>,-<span class="number">30</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.show()<span class="comment">#画出图像</span></span><br><span class="line">x=torch.tensor([<span class="number">4.</span>,<span class="number">0.</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line">optimizer=torch.optim.Adam([x],lr=<span class="number">1e-3</span>)<span class="comment">#lr:learning rate 实例化反向传播算法优化器</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line">    pred=himmelblau(x)</span><br><span class="line">    optimizer.zero_grad()<span class="comment">#参数梯度置为0</span></span><br><span class="line">    pred.backward()</span><br><span class="line">    optimizer.step()<span class="comment">#执行梯度下降</span></span><br><span class="line">    <span class="keyword">if</span> step%<span class="number">2000</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;step&#123;&#125;:x=&#123;&#125;,f(x)=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(step,x.tolist(),pred.item()))</span><br></pre></td></tr></table></figure><p><img src="https://s1.328888.xyz/2022/04/13/fP7Oi.png" alt="fP7Oi.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#运算结果</span><br><span class="line">step0:x=[3.999000072479248, -0.0009999999310821295],f(x)=34.0</span><br><span class="line">step2000:x=[3.5741987228393555, -1.764183521270752],f(x)=0.09904692322015762</span><br><span class="line">step4000:x=[3.5844225883483887, -1.8481197357177734],f(x)=2.1100277081131935e-09</span><br><span class="line">step6000:x=[3.5844264030456543, -1.8481241464614868],f(x)=2.41016095969826e-10</span><br><span class="line">step8000:x=[3.58442759513855, -1.848125696182251],f(x)=2.9103830456733704e-11</span><br><span class="line">step10000:x=[3.584428310394287, -1.8481262922286987],f(x)=9.094947017729282e-13</span><br><span class="line">step12000:x=[3.584428310394287, -1.8481265306472778],f(x)=0.0</span><br><span class="line">step14000:x=[3.584428310394287, -1.8481265306472778],f(x)=0.0</span><br><span class="line">step16000:x=[3.584428310394287, -1.8481265306472778],f(x)=0.0</span><br><span class="line">step18000:x=[3.584428310394287, -1.8481265306472778],f(x)=0.0</span><br></pre></td></tr></table></figure><h2 id="一个交叉熵多分类问题">##一个交叉熵多分类问题</h2><p><img src="https://s1.328888.xyz/2022/04/13/fxuiq.png" alt="网络结构"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">##一个交叉熵多分类问题</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>过拟合与欠拟合</title>
      <link href="/2022/04/13/%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88/"/>
      <url>/2022/04/13/%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88/</url>
      
        <content type="html"><![CDATA[<h2 id="过拟合与欠拟合">过拟合与欠拟合</h2><p>欠拟合：训练集和测试集的acc都很差 可以适当增加模型复杂度再测试</p><p>过拟合：使用模型复杂度高于实际数据模型复杂度 训练多次后training acc很好，test acc效果不好 泛化能力变差</p><h3 id="交叉验证">交叉验证</h3><p>每训练多少次做一次test，只保存test acc 最好的模型</p><p>overfitting之后的模型都不会保存</p><h4 id="K-fold-cross-validation">K-fold cross-validation</h4><p>每次训练完以后重新划分Train Set 与Val Set</p><p>把最开始的Trainning set划分为K份，每次取K-1份做为Trainning Set 另外一份作为Val Set</p><h2 id="减少过拟合">减少过拟合</h2><p>增大数据集</p><p>减少模型复杂度</p><p>Dropout</p><p>Data argume</p><h3 id="Regularization">Regularization</h3><p>$$<br>J(\theta)=-\frac{1}{m}\sum_{i=1}^m[y_i\ln\hat y_i+(1-y_i)\ln(1-\bar y^i)]+\lambda \vert\theta_i\vert<br>$$</p><p>式子前半部分为交叉熵，后半部分为$\lambda$乘上网络参数的1或2范数，因为矩阵的1，2范数大于等于0，当我们对$J(\theta)$做最优化使其最小时，也相当于最优化$\vert\theta_i\vert$减小接近为0，达到简化网络的目的：高维参数接近为0，低维参数保持。</p><p>常用L2-Regularization：<br>$$<br>J(W;X,y)+\frac{1}{2}\Vert w\Vert^2<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer=optim.SGD(net.parameters(),lr=learn_rate,weight_decay=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>使用时只需在.optim时给定参数weight_decay=$\lambda$</p><p>L1-Regularization：pytorch 没有api支持，需要人为编写</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">regularization_loss=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">regularization_loss+=torch.<span class="built_in">sum</span>(torch.<span class="built_in">abs</span>(param))</span><br><span class="line"></span><br><span class="line">classify_loss=criteon(logits,target)</span><br><span class="line">loss=classify_loss+<span class="number">0.01</span>*regularzation_loss</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><h3 id="动量与学习率衰减">动量与学习率衰减</h3><h4 id="Momentum：用于减少停止在局部最优点的情况">Momentum：用于减少停止在局部最优点的情况</h4><p>原梯度更新公式：<br>$$<br>w^{k+1}=w^k-\alpha\nabla f(w^k)<br>$$<br>Momentum梯度更新公式：<br>$$<br>z^{k+1}=\beta z^k+\nabla f(w^k)<br>$$</p><p>$$<br>w^{k+1}=w^k-\alpha z^{k+1}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer=optim.SGD(net.parameters(),lr=learn_rate,momentum=<span class="number">0.78</span>,weight_decay=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>pytorch中调用只需在.optim中设置momentum=$\beta$即可</p><h4 id="Learning-rate-tunning：减少收敛点附近震荡">Learning rate tunning：减少收敛点附近震荡</h4><p>lr随着迭代的进行不断减小</p><p>Scheme1：<code>ReduceLROnPlateau(optimizer,'min')</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer=optim.SGD(net.parameters(),lr=learn_rate,momentum=<span class="number">0.78</span>,weight_decay=<span class="number">0.01</span>)</span><br><span class="line">scheduler=ReduceLROnPlateau(optimizer,<span class="string">&#x27;min&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(args.start_epoch,args.epochs):</span><br><span class="line">    train(train_loader,model,criterion,optimizer,epoch)</span><br><span class="line">    result_avg,loss_val=validate(val_loader,model,criterion,epoch)</span><br><span class="line">    scheduler.step(loss_val)</span><br></pre></td></tr></table></figure><p><code>scheduler.step(loss_val)</code>：loss函数连续一定不减小的话，就衰减lr</p><p>Scheme2:设置每30个epoch，lr=0.1*lr一次</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scheduler=StepLR(optimizer,step_size=30,gamma=0.1)</span><br><span class="line">for epoch in range(100):</span><br><span class="line">scheduler.step()</span><br><span class="line">train()</span><br><span class="line">validate()</span><br></pre></td></tr></table></figure><h3 id="Early-stopping：">Early stopping：</h3><p>train performance 还在上升，validation performance已经保持不变或者下降，则 <strong>early stoppping</strong>，保存Validation performance最大的模型</p><h3 id="Dropout：">Dropout：</h3><p>train前向传播时，每个connection有一定概率的输出为0，即令$w_ix_i=0$</p><p>可在任意连接层中添加<code>torch.nn.Dropout(dropout_prob)</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net_dropped=torch.nn.Sequential(</span><br><span class="line">torch.nn.Linear(<span class="number">784</span>,<span class="number">200</span>),</span><br><span class="line">torch.nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">torch.nn.ReLU(),</span><br><span class="line">torch.nn.Linear(<span class="number">200</span>,<span class="number">200</span>),</span><br><span class="line">torch.nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">torch.nn.ReLU(),</span><br><span class="line">torch.nn.Linear(<span class="number">200</span>,<span class="number">10</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><mark>validation 时不用Dropouot</mark></p><h2 id="梯度下降法的变式">梯度下降法的变式</h2><h3 id="SGD-Stochastic-Gradient-Descent">SGD: Stochastic Gradient Descent</h3><p>对于大数据集，由于显存有限，训练时不可能对所有数据求LOSS后进行梯度更新，所以每次随机选择一个batch的数据进行训练。</p><h3 id="Adagrad">Adagrad</h3><p>自衰减的学习率在某些情况下并不好，可能会造成学习过早停止，Adgrad是一种自学习方法，其参数更新公式为：<br>$$<br>w^{t+1}=w^t-\frac{\beta}{\sqrt{\sum_{i=0}^t(g^i)^2}+\xi}g^t<br>$$<br>其中$\xi$为平滑参数，大小通常为$10^{-4}-10^{-8}$主要用于防止分母为0,分母中的根号特别重要，没有该根号，算法表现非常差。</p><h3 id="RMSprop">RMSprop</h3><p>一种非常有效的自适应学习率改进方法：<br>$$<br>cache^t=\alpha * cache^{t-1}+(1-\alpha)(g^t)^2<br>$$</p><p>$$<br>w^{t+1}=w^t-\frac{\beta}{\sqrt{cache^t}+\xi}g^t<br>$$</p><p>RMSprop不在将前面所有梯度求平方和，二十引用了一个衰减率将其变小，采用一种滑动平均的方式，越靠近前面的梯度对自适应学习的影响率越小，能更快的收敛。</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensor基本运算</title>
      <link href="/2022/04/13/%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/"/>
      <url>/2022/04/13/%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<h2 id="Tensor基本运算">Tensor基本运算</h2><h3 id="矩阵相乘">矩阵相乘</h3><p><a href="http://torch.mm">torch.mm</a>：只适合矩阵 dim=2情形</p><p>torch.matmul：适用任何形式</p><p>@：简便写法</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a=torch.rand(<span class="number">4</span>,<span class="number">784</span>)</span><br><span class="line">&gt;&gt;&gt;x=torch.rand(<span class="number">512</span>,<span class="number">784</span>)</span><br><span class="line">&gt;&gt;&gt;(a@x.t()).shape</span><br><span class="line"><span class="comment">#torch.Size([4, 512])</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a=torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">64</span>)</span><br><span class="line">&gt;&gt;&gt;b=torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">&gt;&gt;&gt;torch.matmul(a,b).shape</span><br><span class="line"><span class="comment">#torch.Size([4, 3, 28, 32])</span></span><br></pre></td></tr></table></figure><h3 id="乘方">乘方</h3><p>power</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a=torch.full([<span class="number">2</span>,<span class="number">2</span>],<span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt;a.<span class="built_in">pow</span>(<span class="number">2</span>)</span><br><span class="line"><span class="comment">#tensor([[9, 9],</span></span><br><span class="line"><span class="comment">#        [9, 9]])</span></span><br></pre></td></tr></table></figure><h3 id="取整">取整</h3><p>.floor()：向下取整</p><p>.ceil()：向上取整</p><p>.trunc()：取小数</p><p>.frac()：取整数</p><p>.round()：四舍五入</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.tensor(<span class="number">3.14</span>)</span><br><span class="line">a.floor(),a.ceil(),a.trunc(),a.frac(),a.<span class="built_in">round</span>()</span><br><span class="line"><span class="comment">#(tensor(3.), tensor(4.), tensor(3.), tensor(0.1400), tensor(3.))</span></span><br></pre></td></tr></table></figure><h3 id="裁剪">裁剪</h3><p>.clamp()：输入参数<code>min</code> ：将小于min的数都置为min</p><p>​ 输入参数<code>(min,max)</code>：将小于min的数都置为min，大于max的数都置为max</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">3</span>)*<span class="number">15</span></span><br><span class="line">a.clamp(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line"><span class="comment">#tensor([[10.0000, 10.0000,  2.5097],</span></span><br><span class="line"><span class="comment">#         [10.0000,  1.2573,  8.4877]])</span></span><br></pre></td></tr></table></figure><h3 id="自动求导">自动求导:</h3><p><strong>方法一</strong><br>1. 首先对需要求导的参数使用<code>requires_grad_()</code>方法标明需要求导<br>2. 计算mse：torch.nn.functional.mse_loss($y$,$\hat y$)<br>3. 使用<code>torch.autograd.grad(mse,[w])</code>对其进行求导</p><p><strong>方法二</strong></p><ol><li><p>首先对需要求导的参数使用<code>requires_grad_()</code>方法标明需要求导</p></li><li><p>计算mse：torch.nn.functional.mse_loss($y，\hat y$)</p></li><li><p>调用<code>mse.backward</code>该指令会计算mse对所有已设置需要求导变量的梯度</p></li><li><p>调用<code>w.grad</code>显示梯度</p><p><mark>backward设置(retain_graph=True)才可以再一次调用，不设置则会报错</mark></p></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">x=torch.ones(<span class="number">1</span>)</span><br><span class="line">w=torch.Tensor([<span class="number">2</span>])</span><br><span class="line">w.requires_grad_()</span><br><span class="line">mse=F.mse_loss(torch.ones(<span class="number">1</span>),x*w)</span><br><span class="line"><span class="comment">#torch.autograd.grad(mse,[w])</span></span><br><span class="line">mse.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([2.])</span></span><br></pre></td></tr></table></figure><h2 id="Tensor统计属性">Tensor统计属性</h2><h3 id="范数">范数</h3><p><img src="https://s1.328888.xyz/2022/04/13/fNR54.png" alt="范数"></p><p>.norm§：求矩阵的 <strong>p</strong> 范数</p><p>.norm(p,dim=x):在 <strong>x</strong> 维度上做p范数，输出shape为除了原维度去掉x维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">b = a.reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">b.norm(<span class="number">1</span>,dim=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#tensor([0.3336, 0.0033, 0.5679, 0.7974, 0.1241, 0.4108, 0.2766, 0.8038])</span></span><br><span class="line"><span class="comment">#tensor([[[0.3336, 0.0033],</span></span><br><span class="line"><span class="comment">#         [0.5679, 0.7974]],</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#        [[0.1241, 0.4108],</span></span><br><span class="line"><span class="comment">#         [0.2766, 0.8038]]])</span></span><br><span class="line"><span class="comment">#tensor([[0.4577, 0.4141],</span></span><br><span class="line"><span class="comment">#        [0.8445, 1.6012]])</span></span><br></pre></td></tr></table></figure><h3 id="统计属性">统计属性</h3><p>.prod()：累乘</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.prod()</span><br><span class="line"><span class="comment">#tensor(0.0008)</span></span><br></pre></td></tr></table></figure><p>.argmax（）：返回最大元素的索引，该索引是tensor打平为1维的索引</p><p>.argmin（）：返回最小元素的索引，该索引是tensor打平为1维的索引</p><p>.argmax（dim=x）：返回最大元素的索引，该索引是 <strong>x维度上</strong> 的索引</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.argmax()</span><br><span class="line"><span class="comment"># tensor([[[0.2517, 0.9526, 0.5908],</span></span><br><span class="line"><span class="comment">#          [0.1431, 0.3951, 0.5676]],</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [[0.7481, 0.8191, 0.4051],</span></span><br><span class="line"><span class="comment">#          [0.7140, 0.4541, 0.5540]]])</span></span><br><span class="line"><span class="comment"># tensor(1)</span></span><br><span class="line">a = torch.rand([<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.argmax(dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># tensor([[[0.0630, 0.4025, 0.8124],</span></span><br><span class="line"><span class="comment">#          [0.2175, 0.4514, 0.5231]],</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [[0.8366, 0.4124, 0.6334],</span></span><br><span class="line"><span class="comment">#          [0.3470, 0.0701, 0.2093]]])</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 0],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0]])</span></span><br></pre></td></tr></table></figure><p>keepdim=True :返回的tensor与原tensor维度一样</p><h3 id="TOPK与K-TH">TOPK与K-TH</h3><p>.topk(k,dim=x,largest=true): largest=true返回x维度上最大的k个值，largest=false返回x维度上最小的k个值，输出第一个参数为其值，第二个参数维其索引</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.topk(<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.2393, 0.7239, 0.3985, 0.5578],</span></span><br><span class="line"><span class="comment">#         [0.8645, 0.0815, 0.7446, 0.3979],</span></span><br><span class="line"><span class="comment">#         [0.6933, 0.7192, 0.4393, 0.2296],</span></span><br><span class="line"><span class="comment">#         [0.1022, 0.7430, 0.6715, 0.9983]])</span></span><br><span class="line"><span class="comment"># torch.return_types.topk(</span></span><br><span class="line"><span class="comment"># values=tensor([[0.7239, 0.5578, 0.3985],</span></span><br><span class="line"><span class="comment">#         [0.8645, 0.7446, 0.3979],</span></span><br><span class="line"><span class="comment">#         [0.7192, 0.6933, 0.4393],</span></span><br><span class="line"><span class="comment">#         [0.9983, 0.7430, 0.6715]]),</span></span><br><span class="line"><span class="comment"># indices=tensor([[1, 3, 2],</span></span><br><span class="line"><span class="comment">#         [0, 2, 3],</span></span><br><span class="line"><span class="comment">#         [1, 0, 2],</span></span><br><span class="line"><span class="comment">#         [3, 1, 2]]))</span></span><br></pre></td></tr></table></figure><p>.kthvalue(k,dim=x)：返回由小到大第k个值及其索引</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.kthvalue(<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.4287, 0.7747, 0.8699, 0.7784],</span></span><br><span class="line"><span class="comment">#         [0.1043, 0.4982, 0.5863, 0.3341],</span></span><br><span class="line"><span class="comment">#         [0.1408, 0.0510, 0.4056, 0.9592],</span></span><br><span class="line"><span class="comment">#         [0.3366, 0.1080, 0.8596, 0.3885]])</span></span><br><span class="line"><span class="comment"># torch.return_types.kthvalue(</span></span><br><span class="line"><span class="comment"># values=tensor([0.7784, 0.4982, 0.4056, 0.3885]),</span></span><br><span class="line"><span class="comment"># indices=tensor([3, 1, 2, 3]))</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="高阶操作">高阶操作</h2><h3 id="torch-where">torch.where</h3><p>torch.where(condition,x,y)→Tensor</p><p>$$<br>out_i=\begin{cases}x_i\ \ if\ condition_i\\y_i\ \ otherwise\end{cases}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.zeros([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">b=torch.ones([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">condition=torch.rand([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(condition)</span><br><span class="line">torch.where(condition&gt;<span class="number">0.5</span>,a,b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.5633, 0.7544, 0.6521, 0.6338],</span></span><br><span class="line"><span class="comment">#         [0.5439, 0.5644, 0.6126, 0.1168],</span></span><br><span class="line"><span class="comment">#         [0.6247, 0.4382, 0.4246, 0.2221],</span></span><br><span class="line"><span class="comment">#         [0.0017, 0.7347, 0.6782, 0.9357]])</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0., 1.],</span></span><br><span class="line"><span class="comment">#         [0., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 0., 0., 0.]])</span></span><br></pre></td></tr></table></figure><h3 id="torch-gather">torch.gather</h3><p>torch.gather(input,dim,index)→Tensor：根据将index的第dim维作为索引查取input中对应元素并生成Tensor输出<br>$$<br>input=\begin{bmatrix}cat\\dog\\fish\end{bmatrix}\ \ dim=0\ \ index=\begin{bmatrix}1\\2\\0\end{bmatrix}\Rightarrow\ \ out=\begin{bmatrix}dog\\fish\\cat\end{bmatrix}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.rand(<span class="number">4</span>,<span class="number">10</span>)</span><br><span class="line">a1=a.topk(<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line">i=a1[<span class="number">1</span>]</span><br><span class="line">b=torch.arange(<span class="number">10</span>)+<span class="number">100</span></span><br><span class="line">torch.gather(b.expand(<span class="number">4</span>,<span class="number">10</span>),dim=<span class="number">1</span>,index=i)</span><br><span class="line"><span class="comment"># tensor([[100, 105, 101],</span></span><br><span class="line"><span class="comment">#         [101, 105, 108],</span></span><br><span class="line"><span class="comment">#         [107, 104, 100],</span></span><br><span class="line"><span class="comment">#         [101, 107, 106]])</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensor数据类型</title>
      <link href="/2022/04/12/Tensor%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>/2022/04/12/Tensor%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1>Pytorch</h1><p><font size=4>本学习笔记基于<a href="https://www.bilibili.com/video/BV1J44y1i734?spm_id_from=333.337.search-card.all.click">【深度学习Pytorch入门】5天从Pytorch入门到实战！PyTorch深度学习快速入门教程 150全集 绝对通俗易懂（深度学习框架/神经网络）_哔哩哔哩_bilibili</a></font></p><p>Tensorflow：静态图优先</p><p>Pytorch：动态图优先</p><h2 id="Tensor数据类型">Tensor数据类型</h2><h3 id="数据类型">数据类型</h3><table><thead><tr><th>类型</th><th>类型</th></tr></thead><tbody><tr><td>32位浮点型</td><td>(默认)torch.FloatTensor</td></tr><tr><td>64位浮点型</td><td>torch.DoubleTensor</td></tr><tr><td>16位整型</td><td>torch.shortTensor</td></tr><tr><td>32位整型</td><td>torch.IntTensor</td></tr><tr><td>64位整型</td><td>torch.LongTensor</td></tr></tbody></table><h3 id="维度DIM">维度DIM</h3><p>Tensor：张量，可以理解为任意维度的矩阵</p><p><img src="https://s1.328888.xyz/2022/04/12/fREG1.png" alt="fREG1.png"></p><p><mark>Pytorch 没有string类型，其句子用编码one-bot or enbeding 向量表示</mark></p><p>Dim0(标量)：<code>torch.tensor(1.3)</code> 即生成了一个值为1.3的变量 注意 ：<strong>1.3为0维标量 [1.3]为1维矢量</strong></p><p>​通常应用于loss计算</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.tensor(<span class="number">1.2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([])<span class="comment">#空的矩阵，即0维矢量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(a.shape)</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.size()</span><br><span class="line">torch.Size([])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure><p>Dim1(向量)：通常应用于节点输入bias 或者是Linear Input</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.1</span>])</span><br><span class="line">tensor([<span class="number">1.1000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.1</span>,<span class="number">2.1</span>])</span><br><span class="line">tensor([<span class="number">1.1000</span>, <span class="number">2.1000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.FloatTensor(<span class="number">1</span>)</span><br><span class="line">tensor([-<span class="number">1.0842e-19</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.FloatTensor(<span class="number">2</span>)</span><br><span class="line">tensor([<span class="number">0.0000</span>, <span class="number">0.0078</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones(<span class="number">2</span>)</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure><p>Dim2：通常用于多张图片的 <strong>Linear Input Batch</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.randn(2,3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[-0.4689, -1.2038, -1.6282],</span><br><span class="line">        [-0.8379, -1.1376, -1.9624]])</span><br><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([2, 3])</span><br><span class="line">&gt;&gt;&gt; a.size(0)</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; a.size(1)</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; a.shape[0]</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; a.shape[1]</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>Dim3: 用于 <strong>RNN Input Batch</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.rand(1,2,3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[[0.4257, 0.1625, 0.1817],</span><br><span class="line">         [0.3695, 0.8208, 0.5442]]])</span><br><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([1, 2, 3])</span><br><span class="line">&gt;&gt;&gt; a[0]</span><br><span class="line">tensor([[0.4257, 0.1625, 0.1817],</span><br><span class="line">        [0.3695, 0.8208, 0.5442]])</span><br><span class="line">&gt;&gt;&gt; a[0][1]</span><br><span class="line">tensor([0.3695, 0.8208, 0.5442])</span><br><span class="line">&gt;&gt;&gt; a[0][1][1]</span><br><span class="line">tensor(0.8208)</span><br><span class="line">&gt;&gt;&gt; </span><br></pre></td></tr></table></figure><p>$$<br>\begin{bmatrix}\begin{bmatrix}0.4257&amp;0.1625&amp;0.1817\end{bmatrix}\\\begin{bmatrix}0.3695&amp;0.8208&amp;0.5442\end{bmatrix}\end{bmatrix}<br>$$<br>Dim4：适用于 <strong>图片</strong> [batch,channel,height,width]</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.numel()</span><br><span class="line"><span class="number">4704</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.dim()</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure><h3 id="创建Tensor">创建Tensor</h3><p>从np.array创建</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=np.array([<span class="number">2</span>,<span class="number">3.3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.from_numpy(a)</span><br><span class="line">tensor([<span class="number">2.0000</span>, <span class="number">3.3000</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=np.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.from_numpy(a)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p><code>numpy_a=a.numpy()</code>可将Tensor转换位numpy数据类型</p><p>从list创建：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.tensor([[1,2,3,4,5],[3,4,5,6,7]])</span><br><span class="line">tensor([[1, 2, 3, 4, 5],</span><br><span class="line">        [3, 4, 5, 6, 7]])</span><br></pre></td></tr></table></figure><p><font size =5><mark>注意：tensor()输入参数为初始化数据 Tensor()输入参数为shape或list</mark></font></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.Tensor(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor([[<span class="number">2.3063e-31</span>, <span class="number">8.6740e-43</span>, <span class="number">8.4078e-45</span>],</span><br><span class="line">        [<span class="number">0.0000e+00</span>, <span class="number">2.3073e-31</span>, <span class="number">8.6740e-43</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.Tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tensor([<span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">2</span>,<span class="number">3</span>) </span><br><span class="line">报错</span><br></pre></td></tr></table></figure><p>不初始化：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.FloatTensor(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor([[[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.IntTensor(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]]], dtype=torch.int32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.empty(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">tensor([[[ <span class="number">6.4011e+23</span>,  <span class="number">1.7866e+25</span>],</span><br><span class="line">         [-<span class="number">2.2864e-31</span>,  <span class="number">7.7961e+34</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.1093e+27</span>,  <span class="number">4.1709e-08</span>],</span><br><span class="line">         [ <span class="number">3.7392e-38</span>, -<span class="number">1.2803e-26</span>]]])</span><br></pre></td></tr></table></figure><p>随机初始化：<code>rand</code>:[0,1]间均匀分布</p><p><code>rand_like(a)</code>相当于<code>rand(a.shape)</code></p><p><code>rand_int(min,max,shape)</code></p><p><code>randn(shape)</code>(0,1)正态分布</p><p><code>normal(mean=torch.full([shape],mean),std=torch.full([shape],std))</code>:自定义正态分布，均值mean 方差 std</p><p>torch.full：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.full([2,3],1)</span><br><span class="line">tensor([[1, 1, 1],</span><br><span class="line">        [1, 1, 1]])</span><br></pre></td></tr></table></figure><p>torch.arrange:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.arange(1,10,2)</span><br><span class="line">tensor([1, 3, 5, 7, 9])</span><br></pre></td></tr></table></figure><p>torch.linspace:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">4</span>)</span><br><span class="line">tensor([ <span class="number">0.0000</span>,  <span class="number">3.3333</span>,  <span class="number">6.6667</span>, <span class="number">10.0000</span>])</span><br><span class="line">等分为<span class="number">4</span>个数据</span><br></pre></td></tr></table></figure><p>torch.eye：单位矩阵</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.eye(3,3)</span><br><span class="line">tensor([[1., 0., 0.],</span><br><span class="line">        [0., 1., 0.],</span><br><span class="line">        [0., 0., 1.]])</span><br><span class="line">&gt;&gt;&gt; torch.eye(3,5)</span><br><span class="line">tensor([[1., 0., 0., 0., 0.],</span><br><span class="line">        [0., 1., 0., 0., 0.],</span><br><span class="line">        [0., 0., 1., 0., 0.]])</span><br></pre></td></tr></table></figure><p>randperm: 随机打散 可设置为种子每次相同打散方法</p><h3 id="索引与切片">索引与切片</h3><p>与<code>python</code>一样</p><p>间隔切片</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.Tensor(4,3,28,28)</span><br><span class="line">&gt;&gt;&gt; a[:,:,0:28:2,0:28:2].shape</span><br><span class="line">torch.Size([4, 3, 14, 14])</span><br></pre></td></tr></table></figure><p><strong>第二个<code>:</code>后为步长</strong></p><p><code>index_select(维度，torch.tensor[所选index])</code>：<mark>第二个参数必须是tensor </mark></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.Tensor(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.index_select(<span class="number">0</span>,torch.tensor([<span class="number">0</span>,<span class="number">2</span>])).shape</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure><p><code>...</code>:所有的维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>,...].shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[...,:<span class="number">2</span>].shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><p>masked_select():</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0.2088</span>, -<span class="number">0.1852</span>,  <span class="number">0.6233</span>,  <span class="number">0.5107</span>],</span><br><span class="line">        [ <span class="number">1.6500</span>,  <span class="number">0.3151</span>,  <span class="number">1.1227</span>,  <span class="number">1.7956</span>],</span><br><span class="line">        [-<span class="number">1.1915</span>,  <span class="number">0.8243</span>, -<span class="number">0.0114</span>,  <span class="number">0.7303</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask=a.ge(<span class="number">0.5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask</span><br><span class="line">tensor([[<span class="literal">False</span>, <span class="literal">False</span>,  <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>, <span class="literal">False</span>,  <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>,  <span class="literal">True</span>, <span class="literal">False</span>,  <span class="literal">True</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.masked_select(mask)</span><br><span class="line">tensor([<span class="number">0.6233</span>, <span class="number">0.5107</span>, <span class="number">1.6500</span>, <span class="number">1.1227</span>, <span class="number">1.7956</span>, <span class="number">0.8243</span>, <span class="number">0.7303</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.masked_select(mask).shape</span><br><span class="line">torch.Size([<span class="number">7</span>])</span><br></pre></td></tr></table></figure><h3 id="维度变换">维度变换</h3><h4 id="reshape：">reshape：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.rand(<span class="number">4</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">a.reshape(<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">784</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">784</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">4</span>*<span class="number">28</span>,<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">112</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">4</span>*<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure><h4 id="squeeze-unsqueeze">squeeze/unsqueeze:</h4><p>unsqueeze():参数取值范围 <strong>[-dim-1,dim+1)  正索引</strong> 是在当前索引 <strong>之后</strong> 插入，<strong>负索引</strong>是在当前索引 <strong>之前</strong> 插入</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.unsqueeze(3).shape</span><br><span class="line">torch.Size([4, 1, 28, 1, 28])</span><br><span class="line">&gt;&gt;&gt; a.unsqueeze(-3).shape</span><br><span class="line">torch.Size([4, 1, 1, 28, 28])</span><br></pre></td></tr></table></figure><p>squee(idx): 删除当前维度</p><h4 id="transpose-t-permute：">transpose/t/permute：</h4><p>transpose：维度交换</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.transpose(1,3).shape</span><br><span class="line">torch.Size([4, 28, 28, 1])</span><br></pre></td></tr></table></figure><p>t：只能用于矩阵 二维</p><p>permute：重构 输入参数为维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><h4 id="expand-repeat：">expand/repeat：</h4><p>expand: 需要时才复制数据，输入参数为扩张后的大小，只有为1的才能扩张，-1表示该维度保持不限</p><p>repeat：一开始就复制数据,输入参数为复制次数</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.expand(-1,4,28,28).shape</span><br><span class="line">torch.Size([4, 4, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.repeat(4,1,1,1).shape</span><br><span class="line">torch.Size([16, 1, 28, 28])</span><br></pre></td></tr></table></figure><h3 id="合并与分割">合并与分割</h3><h4 id="cat">cat</h4><p><strong>除了需要合并的dim以外，其他的dim大小应该相同</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.rand(<span class="number">4</span>,<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b=torch.rand(<span class="number">5</span>,<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat([a,b],dim=<span class="number">0</span>).shape</span><br><span class="line">torch.Size([<span class="number">9</span>, <span class="number">32</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><h4 id="stack">stack</h4><p><strong>重新在合并的dim维度之前添加一个维度，该维度不同取值显示合并前不同内容</strong></p><p><strong>要求所有dim的大小一样</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack([a,b],dim=<span class="number">1</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">2</span>, <span class="number">32</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><h4 id="split">split</h4><p><strong>通过长度拆分</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([3, 4, 32])</span><br><span class="line">&gt;&gt;&gt; b.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; c.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; d.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; a1,a2=a.split([2,1],dim=0)</span><br><span class="line">&gt;&gt;&gt; a1.shape</span><br><span class="line">torch.Size([2, 4, 32])</span><br><span class="line">&gt;&gt;&gt; a2.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; a1,a2=a.split(2,dim=0)</span><br><span class="line">&gt;&gt;&gt; a1.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br><span class="line">&gt;&gt;&gt; a2.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br></pre></td></tr></table></figure><h4 id="chunk">chunk</h4><p><strong>通过数量拆分</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a1,a2=a.chunk(2,dim=0)</span><br><span class="line">&gt;&gt;&gt; a1.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br><span class="line">&gt;&gt;&gt; a2.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br></pre></td></tr></table></figure><h3 id="Broadcasting">Broadcasting</h3><p><mark>大维度缺失可自动添加，且每个维度可以自动扩张</mark></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.rand(4,4,32,32)</span><br><span class="line">&gt;&gt;&gt; b=torch.rand(4,1,1)</span><br><span class="line">&gt;&gt;&gt; c=a+b</span><br><span class="line">&gt;&gt;&gt; c.shape</span><br><span class="line">torch.Size([4, 4, 32, 32])</span><br></pre></td></tr></table></figure><p>b:[4,1,1]→[1,4,1,1]→[4,4,32,32]</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.rand(1,3)</span><br><span class="line">&gt;&gt;&gt; b=torch.rand(3,1)</span><br><span class="line">&gt;&gt;&gt; c=a+b</span><br><span class="line">&gt;&gt;&gt; c.shape</span><br><span class="line">torch.Size([3, 3])</span><br></pre></td></tr></table></figure><p>a:[1,3]→[3,3]</p><p>b:[3,1]→[3,3]</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> Pytorch </category>
          
          <category> 热身基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mathjax基本语法</title>
      <link href="/2022/04/12/mathjax%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"/>
      <url>/2022/04/12/mathjax%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1>mathjax基本语法</h1><h2 id="1-基本语法">1.基本语法</h2><h3 id="1-1显示公式">1.1显示公式</h3><p><font size =4>行内公式：<code>$公式$</code></p><p>文内公式：单独一行</p><p><code>$$公式$$</code></p><h3 id="1-2-特殊字符">1.2 特殊字符</h3><h4 id="1-2-1-希腊字符">1.2.1 希腊字符</h4><table><thead><tr><th>显示</th><th>命令</th><th>显示</th><th>命令</th><th>显示</th><th>命令</th></tr></thead><tbody><tr><td>α</td><td>\alpha</td><td>β</td><td>\beta</td><td>υ</td><td>\upsilon</td></tr><tr><td>γ</td><td>\gamma</td><td>δ</td><td>\delta</td><td>ϕ</td><td>\phi</td></tr><tr><td>ϵ</td><td>\epsilon</td><td>ζ</td><td>\zeta</td><td>χ</td><td>\chi</td></tr><tr><td>ι</td><td>\iota</td><td>θ</td><td>\theta</td><td>ψ</td><td>\psi</td></tr><tr><td>η</td><td>\eta</td><td>κ</td><td>\kappa</td><td>ω</td><td>\omega</td></tr><tr><td>λ</td><td>\lambda</td><td>μ</td><td>\mu</td><td></td><td></td></tr><tr><td>ν</td><td>\nu</td><td>ξ</td><td>\xi</td><td></td><td></td></tr><tr><td>π</td><td>\pi</td><td>ρ</td><td>\rho</td><td></td><td></td></tr><tr><td>σ</td><td>\sigma</td><td>τ</td><td>\tau</td><td></td><td></td></tr></tbody></table><p><img src="https://appwk.baidu.com/naapi/doc/view?ih=621&amp;o=jpg_6_0_______&amp;iw=636&amp;ix=0&amp;iy=0&amp;aimw=636&amp;rn=1&amp;doc_id=84a3254a67ec102de3bd897b&amp;pn=1&amp;sign=00ee614654f2c18484257f890bd40217&amp;type=1&amp;app_ver=2.9.8.2&amp;ua=bd_800_800_IncredibleS_2.9.8.2_2.3.7&amp;bid=1&amp;app_ua=IncredibleS&amp;uid=&amp;cuid=&amp;fr=3&amp;Bdi_bear=WIFI&amp;from=3_10000&amp;bduss=&amp;pid=1&amp;screen=800_800&amp;sys_ver=2.3.7" alt="希腊字母大小写对照表"></p><p><mark>需要大写字母则将第一个字母大写</mark></p><p><code>\Gamma</code> :$\Gamma$</p><p><mark>需要斜体大写字母则在前加var</mark></p><p><code>\varGamma</code>: $\varGamma$</p><h4 id="1-2-2-特殊格式">1.2.2 特殊格式</h4><ul><li><p>上下标</p><p>上标：<code>^</code> $x^2$</p><p>下标：<code>_</code>$x_2$</p></li><li><p>向量</p><p>短箭头：<code>\vec a</code> $\vec a$ <code>\vec &#123;ab&#125;</code> $\vec {ab}$</p><p>长箭头：<code>\overrightarrow a</code> $\overrightarrow a$ <code>\overrightarrow &#123;ab&#125;</code> $\overrightarrow {ab}$</p></li><li><p>bar</p><p>上箭头：<code>\hat a</code> $\hat a$</p><p>横线：<code>\overline a</code> $\overline a$</p><p>下划线 <code>\underline a</code> $\underline a$</p></li><li><p>字体</p><p>空心字：<code>\mathbb &#123;a&#125;</code> $\mathbb {ABCDEFG}$</p></li><li><p><mark>空格</mark></p><p>空格需要转义字符\ ：<code>a\ b</code> $a\ b$</p></li></ul><h4 id="1-2-3括号与分组">1.2.3括号与分组</h4><ul><li><p>同一级用{}处理：<code>x_i^2</code> $x_i^2$ <code>x_&#123;i^2&#125;</code> $x_{i^2}$</p></li><li><p>小括号中括号可直接使用，大括号需要专业字符: <code>\&#123;...\&#125;</code></p></li><li><p>尖括号 <code>\langle...\rangle</code> $\langle ab\rangle$</p></li><li><p>绝对值 <code>\vert ... \vert</code> $\vert ab \vert$</p></li><li><p>双竖线 <code>\Vert ...\Vert</code> $\Vert ab \Vert$</p></li><li><p>使用<code>\left</code>和<code>\right</code>)使符号大小与邻近的公式相适应,该语句适用于所有括号类型</p></li><li><p><code>\left\&#123;\frac&#123;(x+y)&#125;&#123;[\alpha+\beta]&#125;\right\&#125;</code>显示为$\left\{\frac{(x+y)}{[\alpha+\beta]}\right\}$</p><p><font size=6><mark>hexo 解析时<code>\left\&#123;</code>和<code>\right\&#125;</code>会分别解析为<code>\left&#123;</code>与<code>\right&#125;</code>,想要正确解析需要更改为<code>\\&#123;  \\&#125;</code></mark></font></p></li></ul><h4 id="1-2-4-运算符">1.2.4 运算符</h4><table><thead><tr><th>运算符</th><th>演示</th><th>运算符</th><th>演示</th></tr></thead><tbody><tr><td>\times</td><td>$x \times y$</td><td>\cdot</td><td>$x \cdot y$</td></tr><tr><td>\ast</td><td>$x \ast y$</td><td>\div</td><td>$x \div y$</td></tr><tr><td>\pm</td><td>$x \pm y$</td><td>\mp</td><td>$x \mp y$</td></tr><tr><td>\leq</td><td>$x \leq y$</td><td>\geq</td><td>$x \geq y$</td></tr><tr><td>\approx</td><td>$x \approx y$</td><td>\equiv</td><td>$x \equiv y$</td></tr><tr><td>\bigodot</td><td>$x \bigodot y$</td><td>\bigtimes</td><td>$x \bigtimes y$</td></tr></tbody></table><table><thead><tr><th>运算符</th><th>演示</th><th>运算符</th><th>演示</th></tr></thead><tbody><tr><td>\in</td><td>$x \in y$</td><td>\subset</td><td>$x \subset y$</td></tr><tr><td>\subseteq</td><td>$x \subseteq y$</td><td>\varnothing</td><td>$\varnothing$</td></tr><tr><td>\cup</td><td>$x \cup y$</td><td>\cap</td><td>$x \cap y$</td></tr></tbody></table><h4 id="1-2-5-特殊符号">1.2.5 特殊符号</h4><table><thead><tr><th>代码</th><th>演示</th><th>命令</th></tr></thead><tbody><tr><td>\overbrace</td><td>$\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}$</td><td>\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}</td></tr><tr><td>\underbrace</td><td>$\underbrace{b+c}_{1.0}$</td><td>\underbrace{b+c}_{1.0}</td></tr><tr><td>\partial</td><td>$\frac{\partial z}{\partial x}$</td><td>\frac{\partial z}{\partial x}</td></tr><tr><td>\idots</td><td>$1,2,\ldots,n$</td><td>1,2,\ldots,n</td></tr><tr><td>\cdots</td><td>$1,2,\cdots,n$</td><td>1,2,\cdots,n$</td></tr><tr><td>\infty</td><td>$\infty$</td><td>–</td></tr><tr><td>\nabla</td><td>$\nabla$</td><td>–</td></tr><tr><td>\forall</td><td>$\forall$</td><td>–</td></tr><tr><td>\exists</td><td>$\exists$</td><td>–</td></tr><tr><td>\triangle</td><td>$\triangle$</td><td>–</td></tr><tr><td>\lnot</td><td>$\lnot$</td><td></td></tr></tbody></table><table><thead><tr><th>\uparrow</th><th>$\uparrow$</th><th>\Uparrow</th><th>$\Uparrow$</th></tr></thead><tbody><tr><td>\downarrow</td><td>$\downarrow$</td><td>\Downarrow</td><td>$\Downarrow$</td></tr><tr><td>\leftarrow</td><td>$\leftarrow$</td><td>\Leftarrow</td><td>$\Leftarrow$</td></tr><tr><td>\rightarrow</td><td>$\rightarrow$</td><td>\Rightarrow</td><td>$\Rightarrow$</td></tr></tbody></table><ul><li><p>求和 符号与积分</p><table><thead><tr><th>\sum</th><th>$\sum$</th><th>\sum_{i=0}^n</th><th>$\sum_{i=0}^n$</th><th>\displaystyle\sum_{i=0}^n</th><th>$\displaystyle\sum_{i=0}^n$</th></tr></thead><tbody><tr><td>\lim</td><td>$\lim$</td><td>\lim_{x\to\infty}</td><td>$\lim_{x\to\infty}$</td><td>\displaystyle\lim_{x\to\infty}</td><td>$\displaystyle\lim_{x\to\infty}$</td></tr><tr><td>\int</td><td>$\int$</td><td>\iint</td><td>$\iint$</td><td>\iiint</td><td>$\iiint$</td></tr><tr><td>\oint</td><td>$\oint$</td><td>\int_0^\infty{fxdx}</td><td>$\int_0^\infty{fxdx}$</td><td>\prod</td><td>$\prod$</td></tr></tbody></table></li></ul><h3 id="1-3-分式与根式">1.3 分式与根式</h3><ul><li>分式 <code>\frac&#123;分子&#125;&#123;分母&#125;</code> $\frac{x}{y}$</li><li>根式 <code>\sqrt[x]&#123;y&#125;</code>$\sqrt[x]{y}$</li></ul><h3 id="1-4-特殊函数">1.4 特殊函数</h3><p><code>\函数名</code> $\sin{x}$ $\log_2{x}$</p><h2 id="2-矩阵">2.矩阵</h2><h3 id="2-1矩阵生成">2.1矩阵生成</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;matrix&#125; </span><br><span class="line">1&amp;0&amp;1\\</span><br><span class="line">0&amp;1&amp;0\\</span><br><span class="line">1&amp;1&amp;0\\</span><br><span class="line">\end&#123;matrix&#125;</span><br></pre></td></tr></table></figure><ul><li>开始和结束需要输入 <code>\begin&#123;matrix&#125; \end&#123;matrix&#125;</code></li><li>同一行元素之间用 <strong>&amp;</strong> 符号连接</li><li><font size=6>换行 <strong>\\\\</strong> <mark>在hexo中时，<strong>\\\\<strong>会解析为一个</strong>\</strong>,故换行时需要输入 <strong>\\\\\\\\</strong> 且该方法只有在 <strong>矩阵、列表、方程组</strong> 等有<code>\beng&#123;&#125;\end&#123;&#125;</code>包围的公式中有效，若想写两行普通公式，还请插入 <strong>两个</strong> 公式块 </mark></font></li></ul><p>$$\begin{matrix}<br>1&amp;0&amp;1\\<br>0&amp;1&amp;0\\<br>1&amp;1&amp;0\\<br>\end{matrix} $$</p><h3 id="2-2矩阵边框">2.2矩阵边框</h3><p>在起始、结束位置替换matrix</p><table><thead><tr><th>pmatrix</th><th>$\begin{pmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{pmatrix}$</th><th>bmatrix</th><th>$\begin{bmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{bmatrix}$</th></tr></thead><tbody><tr><td>Bmatrix</td><td>$\begin{Bmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{Bmatrix}$</td><td>vmatrix</td><td>$\begin{vmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{vmatrix}$</td></tr><tr><td>Vmatrix</td><td>$\begin{Vmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{Vmatrix}$</td><td></td><td></td></tr></tbody></table><h3 id="2-3-高维矩阵的表示">2.3 高维矩阵的表示</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;bmatrix&#125;</span><br><span class="line">&#123;a_&#123;11&#125;&#125;&amp;&#123;a_&#123;12&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;1n&#125;&#125;\\</span><br><span class="line">&#123;a_&#123;21&#125;&#125;&amp;&#123;a_&#123;22&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;2n&#125;&#125;\\</span><br><span class="line">&#123;\vdots&#125;&amp;&#123;\vdots&#125;&amp;&#123;\ddots&#125;&amp;&#123;\vdots&#125;\\</span><br><span class="line">&#123;a_&#123;n1&#125;&#125;&amp;&#123;a_&#123;n2&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;nn&#125;&#125;\\</span><br><span class="line">\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure><p>$$\begin{bmatrix}<br>{a_{11}}&amp;{a_{12}}&amp;{\cdots}&amp;{a_{1n}}\\<br>{a_{21}}&amp;{a_{22}}&amp;{\cdots}&amp;{a_{2n}}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{a_{n1}}&amp;{a_{n2}}&amp;{\cdots}&amp;{a_{nn}}\\<br>\end{bmatrix}$$</p><ul><li><p>横省略号：<code>\cdots</code></p></li><li><p>竖省略号： <code>\vdots</code></p></li><li><p>斜省略号：<code>\ddots</code></p><h2 id="3-列表">3.列表</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;array&#125;&#123;c|lll&#125;</span><br><span class="line">&#123;\downarrow&#125;&amp;&#123;name&#125;&amp;&#123;age&#125;&amp;&#123;ID&#125;\\</span><br><span class="line">\hine</span><br><span class="line">&#123;num1&#125;&amp;&#123;yy&#125;&amp;&#123;22&#125;&amp;&#123;1320&#125;\\</span><br><span class="line">&#123;num2&#125;&amp;&#123;lw&#125;&amp;&#123;22&#125;&amp;&#123;1111&#125;\\</span><br><span class="line">\end&#123;array&#125;</span><br></pre></td></tr></table></figure><p>$$\begin{array}{c|lll}<br>{\downarrow}&amp;{name}&amp;{age}&amp;{ID}\\<br>\hline<br>{num1}&amp;{yy}&amp;{22}&amp;{1320}\\<br>{num2}&amp;{lw}&amp;{22}&amp;{1111}\\<br>\end{array}$$</p></li><li><p>起始、结束处以 <strong>{array}</strong> 声明</p></li><li><p>对齐方式:在{array}后以{}逐列统一声明</p></li><li><p>左对齐:l；居中：c；右对齐：r</p></li><li><p>竖直线:在声明对齐方式时，插入 <strong>|</strong> 建立竖直线</p></li><li><p>插入水平线:<strong>\hline</strong></p></li></ul><h2 id="3-方程组">3.方程组</h2><ul><li><p>起始结束为 <strong>{cases}</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;cases&#125;</span><br><span class="line">a_1x+b_1y+c_1z=d_1\\</span><br><span class="line">a_2x+b_2y+c_2z=d_2\\</span><br><span class="line">a_3x+b_3y+c_3z=d_3\\</span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure><p>$$\begin{cases}<br>a_1x+b_1y+c_1z=d_1\\<br>a_2x+b_2y+c_2z=d_2\\<br>a_3x+b_3y+c_3z=d_3\\<br>\end{cases}$$</p></li></ul><h2 id="长公式换行">长公式换行</h2><p>用<code>&amp;</code>表示对齐位置，<code>\\</code>换行</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line">\frac&#123;\partial w^l&#125;&#123;\partial m_&#123;F_i^l&#125;&#125;&amp;=\frac&#123;\partial&#125;&#123;\partial m_&#123;F_i^l&#125;&#125;\prod_&#123;j=1&#125;^pexp\left\&#123;-\frac&#123;1&#125;&#123;2&#125;[(x_j^&#123;(t)&#125;-m_&#123;F_j^l&#125;)/\sigma_&#123;F_j^l&#125;]^2 \right\&#125;\\&amp;=\frac&#123;\partial &#125;&#123;\partial m_&#123;F_i^l&#125;&#125;exp\left\&#123;  -\frac&#123;1&#125;&#123;2&#125;[(x_i^&#123;(t)&#125;-m_&#123;F_i^l&#125;)/\sigma_&#123;F_i^l&#125;]^2 \right\&#125;\times \prod_&#123;j=1\\j\neq i&#125;^pexp\left\&#123;  -\frac&#123;1&#125;&#123;2&#125;[(x_j^&#123;(t)&#125;-m_&#123;F_j^l&#125;)/\sigma_&#123;F_j^l&#125;]^2\right\&#125;\\&amp;=\prod_&#123;j=1&#125;^pexp\left\&#123;  -\frac&#123;1&#125;&#123;2&#125;[(x_j^&#123;(t)&#125;-m_&#123;F_j^l&#125;)/\sigma_&#123;F_j^l&#125;]^2 \right\&#125;\times \frac&#123;x_i^&#123;(i)&#125;-m_&#123;F_i^l&#125;&#125;&#123;\sigma_&#123;F_i^l&#125;^2&#125;\\&amp;=\frac&#123;x_i^&#123;(i)&#125;-m_&#123;F_i^l&#125;&#125;&#123;\sigma_&#123;F_i^l&#125;^2&#125;\times w^l</span><br><span class="line">\end&#123;split&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>\begin{split}{}<br>\frac{\partial w^l}{\partial m_{F_i^l}}&amp;=\frac{\partial}{\partial m_{F_i^l}}\prod_{j=1}^pexp\left{-\frac{1}{2}[(x_j^{(t)}-m_{F_j^l})/\sigma_{F_j^l}]^2 \right}\<br>&amp;=\frac{\partial }{\partial m_{F_i^l}}exp\left{  -\frac{1}{2}[(x_i^{(t)}-m_{F_i^l})/\sigma_{F_i^l}]^2 \right}\times \prod_{j=1\j\neq i}^pexp\left{  -\frac{1}{2}[(x_j^{(t)}-m_{F_j^l})/\sigma_{F_j^l}]^2\right}\<br>&amp;=\prod_{j=1}^pexp\left{  -\frac{1}{2}[(x_j^{(t)}-m_{F_j^l})/\sigma_{F_j^l}]^2 \right}\times \frac{x_i^{(i)}-m_{F_i^l}}{\sigma_{F_i^l}^2}\<br>&amp;=\frac{x_i^{(i)}-m_{F_i^l}}{\sigma_{F_i^l}^2}\times w^l<br>\end{split}<br>$$</p><blockquote><p>参考</p><p><a href="https://blog.csdn.net/ajacker/article/details/80301378?spm=1001.2014.3001.5502">Mathjax语法总结_ajacker的博客-CSDN博客_mathjax语法</a></p><p><a href="https://blog.csdn.net/ethmery/article/details/50670297">基本数学公式语法(of MathJax)_PUMC芋圆四号的博客-CSDN博客_mathjax语法</a></font></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 杂文 </category>
          
          <category> mathjax </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathjax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown与Typora</title>
      <link href="/2022/04/11/Markdown%E4%B8%8ETypora/"/>
      <url>/2022/04/11/Markdown%E4%B8%8ETypora/</url>
      
        <content type="html"><![CDATA[<h1>markdown基本语法与Typora</h1><p><font size=5>本文既是对markdown语法及Typora快捷键的记录 也是练习</font></p><p><mark>注意在行内插入加粗、斜体等最好在*符号与左右文本之间最好加上一个半角空格，不然hexo可能解析错误</mark></p><p>比如应<code>你好 **yy** 很高兴</code>而不是<code>你好**yy**很高兴</code></p><ol><li><p>加粗 <code>ctrl+B</code></p><p><code> **文字**</code><br><strong>文字</strong></p></li><li><p>倾斜 <code>Ctrl+I</code></p><p><code>*斜体字*</code><br><em>斜体字</em></p></li><li><p>下划线 <code>ctrl+U</code></p><p><code>&lt;u&gt; 下划线&lt;/u&gt;</code><br><u>下划线</u></p></li><li><p>多级标题 <code>Ctrl+1~6</code></p><p><code># 一级标题</code></p><h1>一级标题</h1><p><code>## 二级标题</code></p><h2 id="二级标题">二级标题</h2><p>以此类推</p></li><li><p>有序列表 <code>Ctrl+Shift+[</code></p><p><code>1. 文字</code></p><ol><li><p>一</p></li><li><p>二</p></li></ol></li><li><p>无序列表 <code>Ctrl+Shift+]</code></p><p><code>- 无序列表</code></p><ul><li>无序列表</li></ul></li><li><p>降级 <code>Tab</code></p></li><li><p>升级 <code>Shift+Tab</code></p></li><li><p>插入链接 <code>Ctrl+K</code></p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">文字链接:：[链接名称]（http://链接网址） </span><br><span class="line">网址链接：&lt;http://...&gt;</span><br></pre></td></tr></table></figure><p><code>&lt;网址&gt;</code> <a href="http://baidu.com">http://baidu.com</a></p><p><code>[百度](http://)</code> <a href="http://www.baidu.com">百度</a></p></li><li><p>插入公式 <code>Ctrl+Shift+M</code><br>使用时需要在front-matter中加上mathjax: true</p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">数学公式</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$\lim_{x\to\infty}\exp(-x)=0$$</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">内联公式 $\lim_&#123;x\to\infty&#125;\exp(-x)=0$</span><br></pre></td></tr></table></figure><p>内联公式: $\lim_{x\to\infty}\exp(-x)=0$<br><font size=5>注意使用内联公式时可能与非内联公式样式不同，比如上示lim下标  </font></p></li><li><p>行内代码 <code>Ctrl+shift+k</code></p><p>````代码` ```</p></li><li><p>插入图片 <code>Ctrl+Shift+I</code></p><p><code>![图片名称](http://)</code></p><p><img src="https://s1.328888.xyz/2022/04/11/fXIeF.png" alt="图片1"></p></li><li><p>创建表格 <code>Ctrl+T</code></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| 姓名 | 性别 |</span><br><span class="line">| :--- | ---：|</span><br><span class="line">| 张三 | 男 |</span><br><span class="line">| 李四 | 女 |</span><br></pre></td></tr></table></figure><table><thead><tr><th>姓名</th><th>性别</th></tr></thead><tbody><tr><td>张三</td><td>男</td></tr><tr><td>李四</td><td>女</td></tr></tbody></table></li><li><p>删除线 <code>ALT+Shift+5</code></p><p><code>~~删除线~~</code></p><p><s>删除线</s></p></li><li><p>引用 <code>Ctrl+Shift+Q</code></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; 这是一个引用</span><br><span class="line">&gt;&gt;这是一个嵌套引用</span><br></pre></td></tr></table></figure><blockquote><p>这是一个引用</p><blockquote><p>这是一个引用嵌套</p></blockquote></blockquote></li><li><p>上标</p><p><code> X&lt;sup&gt;2&lt;sup&gt;</code></p><p>X<sup>2</sup></p></li><li><p>下标</p><p><code>H&lt;sub&gt;2&lt;/suB&gt;O</code></p><p>H<sub>2</sub>O</p></li><li><p>分割线 <code>*** 或者 ___</code></p><hr><hr></li><li><p>自动产生目录 <code>[TOC]+Enter</code></p><p><code>[TOC]</code></p></li><li><p>改变字体大小</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span>=<span class="string">1</span>&gt;</span>字体大小size=1<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span>=<span class="string">3</span>&gt;</span>字体大小size=3<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span>=<span class="string">5</span>&gt;</span>字体大小size=5<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font size=1>字体大小size=1</font></p><p><font size=3>字体大小size=3</font></p><p><font size=5>字体大小size=5</font></p></li><li><p>改变字体颜色</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">red</span>&gt;</span>红色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">&quot;blue&quot;</span>&gt;</span>蓝色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">Yellow</span>&gt;</span>黄色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">YellowGreen</span>&gt;</span>黄绿色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font color=red>红色</font><br><font color="blue">蓝色</font><br><font color=Yellow>黄色</font><br><font color=YellowGreen>黄绿色</font></p></li><li><p>改变字体类型</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;黑体&quot;</span>&gt;</span>黑体<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;宋体&quot;</span>&gt;</span>宋体<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;仿宋&quot;</span>&gt;</span>仿宋<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;幼圆&quot;</span>&gt;</span>幼圆<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;楷书&quot;</span>&gt;</span>楷书<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文行楷&quot;</span>&gt;</span>华文行楷<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文隶书&quot;</span>&gt;</span>华文隶书<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文新魏&quot;</span>&gt;</span>华文新魏<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文彩云&quot;</span>&gt;</span>华文彩云<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文琥珀&quot;</span>&gt;</span>华文琥珀<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font face="黑体">黑体</font><br><font face="宋体">宋体</font><br><font face="仿宋">仿宋</font><br><font face="幼圆">幼圆</font><br><font face="楷书">楷书</font><br><font face="华文行楷">华文行楷</font><br><font face="华文隶书">华文隶书</font><br><font face="华文新魏">华文新魏</font><br><font face="华文彩云">华文彩云</font><br><font face="华文琥珀">华文琥珀</font></p></li><li><p>文本高亮</p><p><code>&lt;mark&gt;highlight 2&lt;/mark&gt;</code></p><p><mark>highlight 2</mark></p></li></ol><p>​</p><p>​</p>]]></content>
      
      
      <categories>
          
          <category> 杂文 </category>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> Typora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>序言</title>
      <link href="/2022/04/11/%E5%BA%8F%E8%A8%80/"/>
      <url>/2022/04/11/%E5%BA%8F%E8%A8%80/</url>
      
        <content type="html"><![CDATA[<h1>剑谱</h1><h2 id="序言">序言</h2><p><img src="https://s1.328888.xyz/2022/04/11/fXIeF.png" alt="fXIeF.png"></p><p><font size=4>   小生于壬寅年元月痛失所爱，数月以来郁郁寡欢，再不得昨日之愉，然偶见各路大神奉为顶上珍宝的格言：</p><blockquote><p>剑谱第一页 无爱既是神</p></blockquote><p>  日思夜想之下竟真悟出了几番道理，称其为道理确些许有失偏颇，然实有些许感悟，故在此立此blog，欲决心 <strong><s>发奋学习</s></strong> 练剑，将些许 <strong><s>学习笔记</s></strong> 心得写于此剑谱之中，望暮年之日回首往事，仍有迹可循。</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 序言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 骚话 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
