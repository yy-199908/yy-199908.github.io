<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Tensor基本运算</title>
      <link href="/2022/04/13/%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/"/>
      <url>/2022/04/13/%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<h2 id="Tensor基本运算">Tensor基本运算</h2><h3 id="矩阵相乘">矩阵相乘</h3><p><a href="http://torch.mm">torch.mm</a>：只适合矩阵 dim=2情形</p><p>torch.matmul：适用任何形式</p><p>@：简便写法</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a=torch.rand(<span class="number">4</span>,<span class="number">784</span>)</span><br><span class="line">&gt;&gt;&gt;x=torch.rand(<span class="number">512</span>,<span class="number">784</span>)</span><br><span class="line">&gt;&gt;&gt;(a@x.t()).shape</span><br><span class="line"><span class="comment">#torch.Size([4, 512])</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a=torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">64</span>)</span><br><span class="line">&gt;&gt;&gt;b=torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">&gt;&gt;&gt;torch.matmul(a,b).shape</span><br><span class="line"><span class="comment">#torch.Size([4, 3, 28, 32])</span></span><br></pre></td></tr></table></figure><h3 id="乘方">乘方</h3><p>power</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a=torch.full([<span class="number">2</span>,<span class="number">2</span>],<span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt;a.<span class="built_in">pow</span>(<span class="number">2</span>)</span><br><span class="line"><span class="comment">#tensor([[9, 9],</span></span><br><span class="line"><span class="comment">#        [9, 9]])</span></span><br></pre></td></tr></table></figure><h3 id="取整">取整</h3><p>.floor()：向下取整</p><p>.ceil()：向上取整</p><p>.trunc()：取小数</p><p>.frac()：取整数</p><p>.round()：四舍五入</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.tensor(<span class="number">3.14</span>)</span><br><span class="line">a.floor(),a.ceil(),a.trunc(),a.frac(),a.<span class="built_in">round</span>()</span><br><span class="line"><span class="comment">#(tensor(3.), tensor(4.), tensor(3.), tensor(0.1400), tensor(3.))</span></span><br></pre></td></tr></table></figure><h3 id="裁剪">裁剪</h3><p>.clamp()：输入参数<code>min</code> ：将小于min的数都置为min</p><p>​ 输入参数<code>(min,max)</code>：将小于min的数都置为min，大于max的数都置为max</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">3</span>)*<span class="number">15</span></span><br><span class="line">a.clamp(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line"><span class="comment">#tensor([[10.0000, 10.0000,  2.5097],</span></span><br><span class="line"><span class="comment">#         [10.0000,  1.2573,  8.4877]])</span></span><br></pre></td></tr></table></figure><h2 id="Tensor统计属性">Tensor统计属性</h2><h3 id="范数">范数</h3><p><img src="https://s1.328888.xyz/2022/04/13/fNR54.png" alt="范数"></p><p>.norm§：求矩阵的 <strong>p</strong> 范数</p><p>.norm(p,dim=x):在 <strong>x</strong> 维度上做p范数，输出shape为除了原维度去掉x维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">b = a.reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">b.norm(<span class="number">1</span>,dim=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#tensor([0.3336, 0.0033, 0.5679, 0.7974, 0.1241, 0.4108, 0.2766, 0.8038])</span></span><br><span class="line"><span class="comment">#tensor([[[0.3336, 0.0033],</span></span><br><span class="line"><span class="comment">#         [0.5679, 0.7974]],</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#        [[0.1241, 0.4108],</span></span><br><span class="line"><span class="comment">#         [0.2766, 0.8038]]])</span></span><br><span class="line"><span class="comment">#tensor([[0.4577, 0.4141],</span></span><br><span class="line"><span class="comment">#        [0.8445, 1.6012]])</span></span><br></pre></td></tr></table></figure><h3 id="统计属性">统计属性</h3><p>.prod()：累乘</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.prod()</span><br><span class="line"><span class="comment">#tensor(0.0008)</span></span><br></pre></td></tr></table></figure><p>.argmax（）：返回最大元素的索引，该索引是tensor打平为1维的索引</p><p>.argmin（）：返回最小元素的索引，该索引是tensor打平为1维的索引</p><p>.argmax（dim=x）：返回最大元素的索引，该索引是 <strong>x维度上</strong> 的索引</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.argmax()</span><br><span class="line"><span class="comment"># tensor([[[0.2517, 0.9526, 0.5908],</span></span><br><span class="line"><span class="comment">#          [0.1431, 0.3951, 0.5676]],</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [[0.7481, 0.8191, 0.4051],</span></span><br><span class="line"><span class="comment">#          [0.7140, 0.4541, 0.5540]]])</span></span><br><span class="line"><span class="comment"># tensor(1)</span></span><br><span class="line">a = torch.rand([<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.argmax(dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># tensor([[[0.0630, 0.4025, 0.8124],</span></span><br><span class="line"><span class="comment">#          [0.2175, 0.4514, 0.5231]],</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [[0.8366, 0.4124, 0.6334],</span></span><br><span class="line"><span class="comment">#          [0.3470, 0.0701, 0.2093]]])</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 0],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0]])</span></span><br></pre></td></tr></table></figure><p>keepdim=True :返回的tensor与原tensor维度一样</p><h3 id="TOPK与K-TH">TOPK与K-TH</h3><p>.topk(k,dim=x,largest=true): largest=true返回x维度上最大的k个值，largest=false返回x维度上最小的k个值，输出第一个参数为其值，第二个参数维其索引</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.topk(<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.2393, 0.7239, 0.3985, 0.5578],</span></span><br><span class="line"><span class="comment">#         [0.8645, 0.0815, 0.7446, 0.3979],</span></span><br><span class="line"><span class="comment">#         [0.6933, 0.7192, 0.4393, 0.2296],</span></span><br><span class="line"><span class="comment">#         [0.1022, 0.7430, 0.6715, 0.9983]])</span></span><br><span class="line"><span class="comment"># torch.return_types.topk(</span></span><br><span class="line"><span class="comment"># values=tensor([[0.7239, 0.5578, 0.3985],</span></span><br><span class="line"><span class="comment">#         [0.8645, 0.7446, 0.3979],</span></span><br><span class="line"><span class="comment">#         [0.7192, 0.6933, 0.4393],</span></span><br><span class="line"><span class="comment">#         [0.9983, 0.7430, 0.6715]]),</span></span><br><span class="line"><span class="comment"># indices=tensor([[1, 3, 2],</span></span><br><span class="line"><span class="comment">#         [0, 2, 3],</span></span><br><span class="line"><span class="comment">#         [1, 0, 2],</span></span><br><span class="line"><span class="comment">#         [3, 1, 2]]))</span></span><br></pre></td></tr></table></figure><p>.kthvalue(k,dim=x)：返回由小到大第k个值及其索引</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a.kthvalue(<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.4287, 0.7747, 0.8699, 0.7784],</span></span><br><span class="line"><span class="comment">#         [0.1043, 0.4982, 0.5863, 0.3341],</span></span><br><span class="line"><span class="comment">#         [0.1408, 0.0510, 0.4056, 0.9592],</span></span><br><span class="line"><span class="comment">#         [0.3366, 0.1080, 0.8596, 0.3885]])</span></span><br><span class="line"><span class="comment"># torch.return_types.kthvalue(</span></span><br><span class="line"><span class="comment"># values=tensor([0.7784, 0.4982, 0.4056, 0.3885]),</span></span><br><span class="line"><span class="comment"># indices=tensor([3, 1, 2, 3]))</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="高阶操作">高阶操作</h2><h3 id="torch-where">torch.where</h3><p>torch.where(condition,x,y)→Tensor</p><p>$$<br>out_i=\begin{cases}x_i\ \ if\ condition_i\\y_i\ \ otherwise\end{cases}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.zeros([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">b=torch.ones([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">condition=torch.rand([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(condition)</span><br><span class="line">torch.where(condition&gt;<span class="number">0.5</span>,a,b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.5633, 0.7544, 0.6521, 0.6338],</span></span><br><span class="line"><span class="comment">#         [0.5439, 0.5644, 0.6126, 0.1168],</span></span><br><span class="line"><span class="comment">#         [0.6247, 0.4382, 0.4246, 0.2221],</span></span><br><span class="line"><span class="comment">#         [0.0017, 0.7347, 0.6782, 0.9357]])</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0., 1.],</span></span><br><span class="line"><span class="comment">#         [0., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 0., 0., 0.]])</span></span><br></pre></td></tr></table></figure><h3 id="torch-gather">torch.gather</h3><p>torch.gather(input,dim,index)→Tensor：根据将index的第dim维作为索引查取input中对应元素并生成Tensor输出<br>$$<br>input=\begin{bmatrix}cat\\dog\\fish\end{bmatrix}\ \ dim=0\ \ index=\begin{bmatrix}1\\2\\0\end{bmatrix}\Rightarrow\ \ out=\begin{bmatrix}dog\\fish\\cat\end{bmatrix}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.rand(<span class="number">4</span>,<span class="number">10</span>)</span><br><span class="line">a1=a.topk(<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line">i=a1[<span class="number">1</span>]</span><br><span class="line">b=torch.arange(<span class="number">10</span>)+<span class="number">100</span></span><br><span class="line">torch.gather(b.expand(<span class="number">4</span>,<span class="number">10</span>),dim=<span class="number">1</span>,index=i)</span><br><span class="line"><span class="comment"># tensor([[100, 105, 101],</span></span><br><span class="line"><span class="comment">#         [101, 105, 108],</span></span><br><span class="line"><span class="comment">#         [107, 104, 100],</span></span><br><span class="line"><span class="comment">#         [101, 107, 106]])</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> 深度学习 </category>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensor数据类型</title>
      <link href="/2022/04/12/Tensor%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>/2022/04/12/Tensor%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1>Pytorch</h1><p><font size=4>本学习笔记基于<a href="https://www.bilibili.com/video/BV1J44y1i734?spm_id_from=333.337.search-card.all.click">【深度学习Pytorch入门】5天从Pytorch入门到实战！PyTorch深度学习快速入门教程 150全集 绝对通俗易懂（深度学习框架/神经网络）_哔哩哔哩_bilibili</a></font></p><p>Tensorflow：静态图优先</p><p>Pytorch：动态图优先</p><h2 id="Tensor数据类型">Tensor数据类型</h2><h3 id="维度DIM">维度DIM</h3><p>Tensor：张量，可以理解为任意维度的矩阵</p><p><img src="https://s1.328888.xyz/2022/04/12/fREG1.png" alt="fREG1.png"></p><p><mark>Pytorch 没有string类型，其句子用编码one-bot or enbeding 向量表示</mark></p><p>Dim0(标量)：<code>torch.tensor(1.3)</code> 即生成了一个值为1.3的变量 注意 ：<strong>1.3为0维标量 [1.3]为1维矢量</strong></p><p>​通常应用于loss计算</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.tensor(<span class="number">1.2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([])<span class="comment">#空的矩阵，即0维矢量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(a.shape)</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.size()</span><br><span class="line">torch.Size([])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure><p>Dim1(向量)：通常应用于节点输入bias 或者是Linear Input</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.1</span>])</span><br><span class="line">tensor([<span class="number">1.1000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.1</span>,<span class="number">2.1</span>])</span><br><span class="line">tensor([<span class="number">1.1000</span>, <span class="number">2.1000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.FloatTensor(<span class="number">1</span>)</span><br><span class="line">tensor([-<span class="number">1.0842e-19</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.FloatTensor(<span class="number">2</span>)</span><br><span class="line">tensor([<span class="number">0.0000</span>, <span class="number">0.0078</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones(<span class="number">2</span>)</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure><p>Dim2：通常用于多张图片的 <strong>Linear Input Batch</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.randn(2,3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[-0.4689, -1.2038, -1.6282],</span><br><span class="line">        [-0.8379, -1.1376, -1.9624]])</span><br><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([2, 3])</span><br><span class="line">&gt;&gt;&gt; a.size(0)</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; a.size(1)</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; a.shape[0]</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; a.shape[1]</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>Dim3: 用于 <strong>RNN Input Batch</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.rand(1,2,3)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[[0.4257, 0.1625, 0.1817],</span><br><span class="line">         [0.3695, 0.8208, 0.5442]]])</span><br><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([1, 2, 3])</span><br><span class="line">&gt;&gt;&gt; a[0]</span><br><span class="line">tensor([[0.4257, 0.1625, 0.1817],</span><br><span class="line">        [0.3695, 0.8208, 0.5442]])</span><br><span class="line">&gt;&gt;&gt; a[0][1]</span><br><span class="line">tensor([0.3695, 0.8208, 0.5442])</span><br><span class="line">&gt;&gt;&gt; a[0][1][1]</span><br><span class="line">tensor(0.8208)</span><br><span class="line">&gt;&gt;&gt; </span><br></pre></td></tr></table></figure><p>$$<br>\begin{bmatrix}\begin{bmatrix}0.4257&amp;0.1625&amp;0.1817\end{bmatrix}\\\begin{bmatrix}0.3695&amp;0.8208&amp;0.5442\end{bmatrix}\end{bmatrix}<br>$$<br>Dim4：适用于 <strong>图片</strong> [batch,channel,height,width]</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.numel()</span><br><span class="line"><span class="number">4704</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.dim()</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure><h3 id="创建Tensor">创建Tensor</h3><p>从np.array创建</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=np.array([<span class="number">2</span>,<span class="number">3.3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.from_numpy(a)</span><br><span class="line">tensor([<span class="number">2.0000</span>, <span class="number">3.3000</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=np.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.from_numpy(a)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>从list创建：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.tensor([[1,2,3,4,5],[3,4,5,6,7]])</span><br><span class="line">tensor([[1, 2, 3, 4, 5],</span><br><span class="line">        [3, 4, 5, 6, 7]])</span><br></pre></td></tr></table></figure><p><font size =5><mark>注意：tensor()输入参数为初始化数据 Tensor()输入参数为shape或list</mark></font></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.Tensor(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor([[<span class="number">2.3063e-31</span>, <span class="number">8.6740e-43</span>, <span class="number">8.4078e-45</span>],</span><br><span class="line">        [<span class="number">0.0000e+00</span>, <span class="number">2.3073e-31</span>, <span class="number">8.6740e-43</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.Tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tensor([<span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">2</span>,<span class="number">3</span>) </span><br><span class="line">报错</span><br></pre></td></tr></table></figure><p>不初始化：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.FloatTensor(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor([[[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.IntTensor(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]]], dtype=torch.int32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.empty(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">tensor([[[ <span class="number">6.4011e+23</span>,  <span class="number">1.7866e+25</span>],</span><br><span class="line">         [-<span class="number">2.2864e-31</span>,  <span class="number">7.7961e+34</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.1093e+27</span>,  <span class="number">4.1709e-08</span>],</span><br><span class="line">         [ <span class="number">3.7392e-38</span>, -<span class="number">1.2803e-26</span>]]])</span><br></pre></td></tr></table></figure><p>随机初始化：<code>rand</code>:[0,1]间均匀分布</p><p><code>rand_like(a)</code>相当于<code>rand(a.shape)</code></p><p><code>rand_int(min,max,shape)</code></p><p><code>randn(shape)</code>(0,1)正态分布</p><p><code>normal(mean=torch.full([shape],mean),std=torch.full([shape],std))</code>:自定义正态分布，均值mean 方差 std</p><p>torch.full：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.full([2,3],1)</span><br><span class="line">tensor([[1, 1, 1],</span><br><span class="line">        [1, 1, 1]])</span><br></pre></td></tr></table></figure><p>torch.arrange:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.arange(1,10,2)</span><br><span class="line">tensor([1, 3, 5, 7, 9])</span><br></pre></td></tr></table></figure><p>torch.linspace:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">4</span>)</span><br><span class="line">tensor([ <span class="number">0.0000</span>,  <span class="number">3.3333</span>,  <span class="number">6.6667</span>, <span class="number">10.0000</span>])</span><br><span class="line">等分为<span class="number">4</span>个数据</span><br></pre></td></tr></table></figure><p>torch.eye：单位矩阵</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.eye(3,3)</span><br><span class="line">tensor([[1., 0., 0.],</span><br><span class="line">        [0., 1., 0.],</span><br><span class="line">        [0., 0., 1.]])</span><br><span class="line">&gt;&gt;&gt; torch.eye(3,5)</span><br><span class="line">tensor([[1., 0., 0., 0., 0.],</span><br><span class="line">        [0., 1., 0., 0., 0.],</span><br><span class="line">        [0., 0., 1., 0., 0.]])</span><br></pre></td></tr></table></figure><p>randperm: 随机打散 可设置为种子每次相同打散方法</p><h3 id="索引与切片">索引与切片</h3><p>与<code>python</code>一样</p><p>间隔切片</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.Tensor(4,3,28,28)</span><br><span class="line">&gt;&gt;&gt; a[:,:,0:28:2,0:28:2].shape</span><br><span class="line">torch.Size([4, 3, 14, 14])</span><br></pre></td></tr></table></figure><p><strong>第二个<code>:</code>后为步长</strong></p><p><code>index_select(维度，torch.tensor[所选index])</code>：<mark>第二个参数必须是tensor </mark></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.Tensor(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.index_select(<span class="number">0</span>,torch.tensor([<span class="number">0</span>,<span class="number">2</span>])).shape</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure><p><code>...</code>:所有的维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>,...].shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[...,:<span class="number">2</span>].shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><p>masked_select():</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0.2088</span>, -<span class="number">0.1852</span>,  <span class="number">0.6233</span>,  <span class="number">0.5107</span>],</span><br><span class="line">        [ <span class="number">1.6500</span>,  <span class="number">0.3151</span>,  <span class="number">1.1227</span>,  <span class="number">1.7956</span>],</span><br><span class="line">        [-<span class="number">1.1915</span>,  <span class="number">0.8243</span>, -<span class="number">0.0114</span>,  <span class="number">0.7303</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask=a.ge(<span class="number">0.5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask</span><br><span class="line">tensor([[<span class="literal">False</span>, <span class="literal">False</span>,  <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>, <span class="literal">False</span>,  <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>,  <span class="literal">True</span>, <span class="literal">False</span>,  <span class="literal">True</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.masked_select(mask)</span><br><span class="line">tensor([<span class="number">0.6233</span>, <span class="number">0.5107</span>, <span class="number">1.6500</span>, <span class="number">1.1227</span>, <span class="number">1.7956</span>, <span class="number">0.8243</span>, <span class="number">0.7303</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.masked_select(mask).shape</span><br><span class="line">torch.Size([<span class="number">7</span>])</span><br></pre></td></tr></table></figure><h3 id="维度变换">维度变换</h3><h4 id="reshape：">reshape：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.rand(<span class="number">4</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">a.reshape(<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">784</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">784</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">4</span>*<span class="number">28</span>,<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">112</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">4</span>*<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure><h4 id="squeeze-unsqueeze">squeeze/unsqueeze:</h4><p>unsqueeze():参数取值范围 <strong>[-dim-1,dim+1)  正索引</strong> 是在当前索引 <strong>之后</strong> 插入，<strong>负索引</strong>是在当前索引 <strong>之前</strong> 插入</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.unsqueeze(3).shape</span><br><span class="line">torch.Size([4, 1, 28, 1, 28])</span><br><span class="line">&gt;&gt;&gt; a.unsqueeze(-3).shape</span><br><span class="line">torch.Size([4, 1, 1, 28, 28])</span><br></pre></td></tr></table></figure><p>squee(idx): 删除当前维度</p><h4 id="transpose-t-permute：">transpose/t/permute：</h4><p>transpose：维度交换</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.transpose(1,3).shape</span><br><span class="line">torch.Size([4, 28, 28, 1])</span><br></pre></td></tr></table></figure><p>t：只能用于矩阵 二维</p><p>permute：重构 输入参数为维度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><h4 id="expand-repeat：">expand/repeat：</h4><p>expand: 需要时才复制数据，输入参数为扩张后的大小，只有为1的才能扩张，-1表示该维度保持不限</p><p>repeat：一开始就复制数据,输入参数为复制次数</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.expand(-1,4,28,28).shape</span><br><span class="line">torch.Size([4, 4, 28, 28])</span><br><span class="line">&gt;&gt;&gt; a.repeat(4,1,1,1).shape</span><br><span class="line">torch.Size([16, 1, 28, 28])</span><br></pre></td></tr></table></figure><h3 id="合并与分割">合并与分割</h3><h4 id="cat">cat</h4><p><strong>除了需要合并的dim以外，其他的dim大小应该相同</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.rand(<span class="number">4</span>,<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b=torch.rand(<span class="number">5</span>,<span class="number">32</span>,<span class="number">8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat([a,b],dim=<span class="number">0</span>).shape</span><br><span class="line">torch.Size([<span class="number">9</span>, <span class="number">32</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><h4 id="stack">stack</h4><p><strong>重新在合并的dim维度之前添加一个维度，该维度不同取值显示合并前不同内容</strong></p><p><strong>要求所有dim的大小一样</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack([a,b],dim=<span class="number">1</span>).shape</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">2</span>, <span class="number">32</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><h4 id="split">split</h4><p><strong>通过长度拆分</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a.shape</span><br><span class="line">torch.Size([3, 4, 32])</span><br><span class="line">&gt;&gt;&gt; b.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; c.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; d.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; a1,a2=a.split([2,1],dim=0)</span><br><span class="line">&gt;&gt;&gt; a1.shape</span><br><span class="line">torch.Size([2, 4, 32])</span><br><span class="line">&gt;&gt;&gt; a2.shape</span><br><span class="line">torch.Size([1, 4, 32])</span><br><span class="line">&gt;&gt;&gt; a1,a2=a.split(2,dim=0)</span><br><span class="line">&gt;&gt;&gt; a1.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br><span class="line">&gt;&gt;&gt; a2.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br></pre></td></tr></table></figure><h4 id="chunk">chunk</h4><p><strong>通过数量拆分</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a1,a2=a.chunk(2,dim=0)</span><br><span class="line">&gt;&gt;&gt; a1.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br><span class="line">&gt;&gt;&gt; a2.shape</span><br><span class="line">torch.Size([2, 32, 5])</span><br></pre></td></tr></table></figure><h3 id="Broadcasting">Broadcasting</h3><p><mark>大维度缺失可自动添加，且每个维度可以自动扩张</mark></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.rand(4,4,32,32)</span><br><span class="line">&gt;&gt;&gt; b=torch.rand(4,1,1)</span><br><span class="line">&gt;&gt;&gt; c=a+b</span><br><span class="line">&gt;&gt;&gt; c.shape</span><br><span class="line">torch.Size([4, 4, 32, 32])</span><br></pre></td></tr></table></figure><p>b:[4,1,1]→[1,4,1,1]→[4,4,32,32]</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.rand(1,3)</span><br><span class="line">&gt;&gt;&gt; b=torch.rand(3,1)</span><br><span class="line">&gt;&gt;&gt; c=a+b</span><br><span class="line">&gt;&gt;&gt; c.shape</span><br><span class="line">torch.Size([3, 3])</span><br></pre></td></tr></table></figure><p>a:[1,3]→[3,3]</p><p>b:[3,1]→[3,3]</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 天下第一剑:机器学习 </category>
          
          <category> 深度学习 </category>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mathjax基本语法</title>
      <link href="/2022/04/12/mathjax%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"/>
      <url>/2022/04/12/mathjax%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1>mathjax基本语法</h1><h2 id="1-基本语法">1.基本语法</h2><h3 id="1-1显示公式">1.1显示公式</h3><p><font size =4>行内公式：<code>$公式$</code></p><p>文内公式：单独一行</p><p><code>$$公式$$</code></p><h3 id="1-2-特殊字符">1.2 特殊字符</h3><h4 id="1-2-1-希腊字符">1.2.1 希腊字符</h4><table><thead><tr><th>显示</th><th>命令</th><th>显示</th><th>命令</th><th>显示</th><th>命令</th></tr></thead><tbody><tr><td>α</td><td>\alpha</td><td>β</td><td>\beta</td><td>υ</td><td>\upsilon</td></tr><tr><td>γ</td><td>\gamma</td><td>δ</td><td>\delta</td><td>ϕ</td><td>\phi</td></tr><tr><td>ϵ</td><td>\epsilon</td><td>ζ</td><td>\zeta</td><td>χ</td><td>\chi</td></tr><tr><td>ι</td><td>\iota</td><td>θ</td><td>\theta</td><td>ψ</td><td>\psi</td></tr><tr><td>η</td><td>\eta</td><td>κ</td><td>\kappa</td><td>ω</td><td>\omega</td></tr><tr><td>λ</td><td>\lambda</td><td>μ</td><td>\mu</td><td></td><td></td></tr><tr><td>ν</td><td>\nu</td><td>ξ</td><td>\xi</td><td></td><td></td></tr><tr><td>π</td><td>\pi</td><td>ρ</td><td>\rho</td><td></td><td></td></tr><tr><td>σ</td><td>\sigma</td><td>τ</td><td>\tau</td><td></td><td></td></tr></tbody></table><p><img src="https://appwk.baidu.com/naapi/doc/view?ih=621&amp;o=jpg_6_0_______&amp;iw=636&amp;ix=0&amp;iy=0&amp;aimw=636&amp;rn=1&amp;doc_id=84a3254a67ec102de3bd897b&amp;pn=1&amp;sign=00ee614654f2c18484257f890bd40217&amp;type=1&amp;app_ver=2.9.8.2&amp;ua=bd_800_800_IncredibleS_2.9.8.2_2.3.7&amp;bid=1&amp;app_ua=IncredibleS&amp;uid=&amp;cuid=&amp;fr=3&amp;Bdi_bear=WIFI&amp;from=3_10000&amp;bduss=&amp;pid=1&amp;screen=800_800&amp;sys_ver=2.3.7" alt="希腊字母大小写对照表"></p><p><mark>需要大写字母则将第一个字母大写</mark></p><p><code>\Gamma</code> :$\Gamma$</p><p><mark>需要斜体大写字母则在前加var</mark></p><p><code>\varGamma</code>: $\varGamma$</p><h4 id="1-2-2-特殊格式">1.2.2 特殊格式</h4><ul><li><p>上下标</p><p>上标：<code>^</code> $x^2$</p><p>下标：<code>_</code>$x_2$</p></li><li><p>向量</p><p>短箭头：<code>\vec a</code> $\vec a$ <code>\vec &#123;ab&#125;</code> $\vec {ab}$</p><p>长箭头：<code>\overrightarrow a</code> $\overrightarrow a$ <code>\overrightarrow &#123;ab&#125;</code> $\overrightarrow {ab}$</p></li><li><p>bar</p><p>上箭头：<code>\hat a</code> $\hat a$</p><p>横线：<code>\overline a</code> $\overline a$</p><p>下划线 <code>\underline a</code> $\underline a$</p></li><li><p>字体</p><p>空心字：<code>\mathbb &#123;a&#125;</code> $\mathbb {ABCDEFG}$</p></li><li><p><mark>空格</mark></p><p>空格需要转义字符\ ：<code>a\ b</code> $a\ b$</p></li></ul><h4 id="1-2-3括号与分组">1.2.3括号与分组</h4><ul><li><p>同一级用{}处理：<code>x_i^2</code> $x_i^2$ <code>x_&#123;i^2&#125;</code> $x_{i^2}$</p></li><li><p>小括号中括号可直接使用，大括号需要专业字符: <code>\&#123;...\&#125;</code></p></li><li><p>尖括号 <code>\langle...\rangle</code> $\langle ab\rangle$</p></li><li><p>绝对值 <code>\vert ... \vert</code> $\vert ab \vert$</p></li><li><p>双竖线 <code>\Vert ...\Vert</code> $\Vert ab \Vert$</p></li><li><p>使用<code>\left</code>和<code>\right</code>)使符号大小与邻近的公式相适应,该语句适用于所有括号类型</p></li><li><p><code>\left\&#123;\frac&#123;(x+y)&#125;&#123;[\alpha+\beta]&#125;\right\&#125;</code>显示为$\left\{\frac{(x+y)}{[\alpha+\beta]}\right\}$</p><p><mark>hexo 解析时<code>\left\&#123;</code>和<code>\right\&#125;</code>会分别解析问<code>\left&#123;</code>与<code>\right&#125;</code>,想要正确解析需要更改为<code>\\&#123;  \\&#125;</code></mark></p></li></ul><h4 id="1-2-4-运算符">1.2.4 运算符</h4><table><thead><tr><th>运算符</th><th>演示</th><th>运算符</th><th>演示</th></tr></thead><tbody><tr><td>\times</td><td>$x \times y$</td><td>\cdot</td><td>$x \cdot y$</td></tr><tr><td>\ast</td><td>$x \ast y$</td><td>\div</td><td>$x \div y$</td></tr><tr><td>\pm</td><td>$x \pm y$</td><td>\mp</td><td>$x \mp y$</td></tr><tr><td>\leq</td><td>$x \leq y$</td><td>\geq</td><td>$x \geq y$</td></tr><tr><td>\approx</td><td>$x \approx y$</td><td>\equiv</td><td>$x \equiv y$</td></tr><tr><td>\bigodot</td><td>$x \bigodot y$</td><td>\bigtimes</td><td>$x \bigtimes y$</td></tr></tbody></table><table><thead><tr><th>运算符</th><th>演示</th><th>运算符</th><th>演示</th></tr></thead><tbody><tr><td>\in</td><td>$x \in y$</td><td>\subset</td><td>$x \subset y$</td></tr><tr><td>\subseteq</td><td>$x \subseteq y$</td><td>\varnothing</td><td>$\varnothing$</td></tr><tr><td>\cup</td><td>$x \cup y$</td><td>\cap</td><td>$x \cap y$</td></tr></tbody></table><h4 id="1-2-5-特殊符号">1.2.5 特殊符号</h4><table><thead><tr><th>代码</th><th>演示</th><th>命令</th></tr></thead><tbody><tr><td>\overbrace</td><td>$\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}$</td><td>\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}</td></tr><tr><td>\underbrace</td><td>$\underbrace{b+c}_{1.0}$</td><td>\underbrace{b+c}_{1.0}</td></tr><tr><td>\partial</td><td>$\frac{\partial z}{\partial x}$</td><td>\frac{\partial z}{\partial x}</td></tr><tr><td>\idots</td><td>$1,2,\ldots,n$</td><td>1,2,\ldots,n</td></tr><tr><td>\cdots</td><td>$1,2,\cdots,n$</td><td>1,2,\cdots,n$</td></tr><tr><td>\infty</td><td>$\infty$</td><td>–</td></tr><tr><td>\nabla</td><td>$\nabla$</td><td>–</td></tr><tr><td>\forall</td><td>$\forall$</td><td>–</td></tr><tr><td>\exists</td><td>$\exists$</td><td>–</td></tr><tr><td>\triangle</td><td>$\triangle$</td><td>–</td></tr><tr><td>\lnot</td><td>$\lnot$</td><td></td></tr></tbody></table><table><thead><tr><th>\uparrow</th><th>$\uparrow$</th><th>\Uparrow</th><th>$\Uparrow$</th></tr></thead><tbody><tr><td>\downarrow</td><td>$\downarrow$</td><td>\Downarrow</td><td>$\Downarrow$</td></tr><tr><td>\leftarrow</td><td>$\leftarrow$</td><td>\Leftarrow</td><td>$\Leftarrow$</td></tr><tr><td>\rightarrow</td><td>$\rightarrow$</td><td>\Rightarrow</td><td>$\Rightarrow$</td></tr></tbody></table><ul><li><p>求和 符号与积分</p><table><thead><tr><th>\sum</th><th>$\sum$</th><th>\sum_{i=0}^n</th><th>$\sum_{i=0}^n$</th><th>\displaystyle\sum_{i=0}^n</th><th>$\displaystyle\sum_{i=0}^n$</th></tr></thead><tbody><tr><td>\lim</td><td>$\lim$</td><td>\lim_{x\to\infty}</td><td>$\lim_{x\to\infty}$</td><td>\displaystyle\lim_{x\to\infty}</td><td>$\displaystyle\lim_{x\to\infty}$</td></tr><tr><td>\int</td><td>$\int$</td><td>\iint</td><td>$\iint$</td><td>\iiint</td><td>$\iiint$</td></tr><tr><td>\oint</td><td>$\oint$</td><td>\int_0^\infty{fxdx}</td><td>$\int_0^\infty{fxdx}$</td><td></td><td></td></tr></tbody></table></li></ul><h3 id="1-3-分式与根式">1.3 分式与根式</h3><ul><li>分式 <code>\frac&#123;分子&#125;&#123;分母&#125;</code> $\frac{x}{y}$</li><li>根式 <code>\sqrt[x]&#123;y&#125;</code>$\sqrt[x]{y}$</li></ul><h3 id="1-4-特殊函数">1.4 特殊函数</h3><p><code>\函数名</code> $\sin{x}$ $\log_2{x}$</p><h2 id="2-矩阵">2.矩阵</h2><h3 id="2-1矩阵生成">2.1矩阵生成</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;matrix&#125; </span><br><span class="line">1&amp;0&amp;1\\</span><br><span class="line">0&amp;1&amp;0\\</span><br><span class="line">1&amp;1&amp;0\\</span><br><span class="line">\end&#123;matrix&#125;</span><br></pre></td></tr></table></figure><ul><li>开始和结束需要输入 <code>\begin&#123;matrix&#125; \end&#123;matrix&#125;</code></li><li>同一行元素之间用 <strong>&amp;</strong> 符号连接</li><li>换行 <strong>\\\\</strong> <mark>在hexo中时，<strong>\\\\<strong>会解析为一个</strong>\</strong>,故换行时需要输入 <strong>\\\\\\\\</strong> </mark></li></ul><p>$$\begin{matrix}<br>1&amp;0&amp;1\\<br>0&amp;1&amp;0\\<br>1&amp;1&amp;0\\<br>\end{matrix} $$</p><h3 id="2-2矩阵边框">2.2矩阵边框</h3><p>在起始、结束位置替换matrix</p><table><thead><tr><th>pmatrix</th><th>$\begin{pmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{pmatrix}$</th><th>bmatrix</th><th>$\begin{bmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{bmatrix}$</th></tr></thead><tbody><tr><td>Bmatrix</td><td>$\begin{Bmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{Bmatrix}$</td><td>vmatrix</td><td>$\begin{vmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{vmatrix}$</td></tr><tr><td>Vmatrix</td><td>$\begin{Vmatrix} 1&amp;2&amp;3\\ 4&amp;5&amp;6\\ 7&amp;8&amp;9\\ \end{Vmatrix}$</td><td></td><td></td></tr></tbody></table><h3 id="2-3-高维矩阵的表示">2.3 高维矩阵的表示</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;bmatrix&#125;</span><br><span class="line">&#123;a_&#123;11&#125;&#125;&amp;&#123;a_&#123;12&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;1n&#125;&#125;\\</span><br><span class="line">&#123;a_&#123;21&#125;&#125;&amp;&#123;a_&#123;22&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;2n&#125;&#125;\\</span><br><span class="line">&#123;\vdots&#125;&amp;&#123;\vdots&#125;&amp;&#123;\ddots&#125;&amp;&#123;\vdots&#125;\\</span><br><span class="line">&#123;a_&#123;n1&#125;&#125;&amp;&#123;a_&#123;n2&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;nn&#125;&#125;\\</span><br><span class="line">\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure><p>$$\begin{bmatrix}<br>{a_{11}}&amp;{a_{12}}&amp;{\cdots}&amp;{a_{1n}}\\<br>{a_{21}}&amp;{a_{22}}&amp;{\cdots}&amp;{a_{2n}}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{a_{n1}}&amp;{a_{n2}}&amp;{\cdots}&amp;{a_{nn}}\\<br>\end{bmatrix}$$</p><ul><li><p>横省略号：<code>\cdots</code></p></li><li><p>竖省略号： <code>\vdots</code></p></li><li><p>斜省略号：<code>\ddots</code></p><h2 id="3-列表">3.列表</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;array&#125;&#123;c|lll&#125;</span><br><span class="line">&#123;\downarrow&#125;&amp;&#123;name&#125;&amp;&#123;age&#125;&amp;&#123;ID&#125;\\</span><br><span class="line">\hine</span><br><span class="line">&#123;num1&#125;&amp;&#123;yy&#125;&amp;&#123;22&#125;&amp;&#123;1320&#125;\\</span><br><span class="line">&#123;num2&#125;&amp;&#123;lw&#125;&amp;&#123;22&#125;&amp;&#123;1111&#125;\\</span><br><span class="line">\end&#123;array&#125;</span><br></pre></td></tr></table></figure><p>$$\begin{array}{c|lll}<br>{\downarrow}&amp;{name}&amp;{age}&amp;{ID}\\<br>\hline<br>{num1}&amp;{yy}&amp;{22}&amp;{1320}\\<br>{num2}&amp;{lw}&amp;{22}&amp;{1111}\\<br>\end{array}$$</p></li><li><p>起始、结束处以 <strong>{array}</strong> 声明</p></li><li><p>对齐方式:在{array}后以{}逐列统一声明</p></li><li><p>左对齐:l；居中：c；右对齐：r</p></li><li><p>竖直线:在声明对齐方式时，插入 <strong>|</strong> 建立竖直线</p></li><li><p>插入水平线:<strong>\hline</strong></p></li></ul><h2 id="3-方程组">3.方程组</h2><ul><li><p>起始结束为 <strong>{cases}</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;cases&#125;</span><br><span class="line">a_1x+b_1y+c_1z=d_1\\</span><br><span class="line">a_2x+b_2y+c_2z=d_2\\</span><br><span class="line">a_3x+b_3y+c_3z=d_3\\</span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure><p>$$\begin{cases}<br>a_1x+b_1y+c_1z=d_1\\<br>a_2x+b_2y+c_2z=d_2\\<br>a_3x+b_3y+c_3z=d_3\\<br>\end{cases}$$</p></li></ul><blockquote><p>参考</p><p><a href="https://blog.csdn.net/ajacker/article/details/80301378?spm=1001.2014.3001.5502">Mathjax语法总结_ajacker的博客-CSDN博客_mathjax语法</a></p><p><a href="https://blog.csdn.net/ethmery/article/details/50670297">基本数学公式语法(of MathJax)_PUMC芋圆四号的博客-CSDN博客_mathjax语法</a></font></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 杂文 </category>
          
          <category> mathjax </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathjax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown与Typora</title>
      <link href="/2022/04/11/Markdown%E4%B8%8ETypora/"/>
      <url>/2022/04/11/Markdown%E4%B8%8ETypora/</url>
      
        <content type="html"><![CDATA[<h1>markdown基本语法与Typora</h1><p><font size=5>本文既是对markdown语法及Typora快捷键的记录 也是练习</font></p><p><mark>注意在行内插入加粗、斜体等最好在*符号与左右文本之间最好加上一个半角空格，不然hexo可能解析错误</mark></p><p>比如应<code>你好 **yy** 很高兴</code>而不是<code>你好**yy**很高兴</code></p><ol><li><p>加粗 <code>ctrl+B</code></p><p><code> **文字**</code><br><strong>文字</strong></p></li><li><p>倾斜 <code>Ctrl+I</code></p><p><code>*斜体字*</code><br><em>斜体字</em></p></li><li><p>下划线 <code>ctrl+U</code></p><p><code>&lt;u&gt; 下划线&lt;/u&gt;</code><br><u>下划线</u></p></li><li><p>多级标题 <code>Ctrl+1~6</code></p><p><code># 一级标题</code></p><h1>一级标题</h1><p><code>## 二级标题</code></p><h2 id="二级标题">二级标题</h2><p>以此类推</p></li><li><p>有序列表 <code>Ctrl+Shift+[</code></p><p><code>1. 文字</code></p><ol><li><p>一</p></li><li><p>二</p></li></ol></li><li><p>无序列表 <code>Ctrl+Shift+]</code></p><p><code>- 无序列表</code></p><ul><li>无序列表</li></ul></li><li><p>降级 <code>Tab</code></p></li><li><p>升级 <code>Shift+Tab</code></p></li><li><p>插入链接 <code>Ctrl+K</code></p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">文字链接:：[链接名称]（http://链接网址） </span><br><span class="line">网址链接：&lt;http://...&gt;</span><br></pre></td></tr></table></figure><p><code>&lt;网址&gt;</code> <a href="http://baidu.com">http://baidu.com</a></p><p><code>[百度](http://)</code> <a href="http://www.baidu.com">百度</a></p></li><li><p>插入公式 <code>Ctrl+Shift+M</code><br>使用时需要在front-matter中加上mathjax: true</p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">数学公式</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$\lim_{x\to\infty}\exp(-x)=0$$</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">内联公式 $\lim_&#123;x\to\infty&#125;\exp(-x)=0$</span><br></pre></td></tr></table></figure><p>内联公式: $\lim_{x\to\infty}\exp(-x)=0$<br><font size=5>注意使用内联公式时可能与非内联公式样式不同，比如上示lim下标  </font></p></li><li><p>行内代码 <code>Ctrl+shift+k</code></p><p>````代码` ```</p></li><li><p>插入图片 <code>Ctrl+Shift+I</code></p><p><code>![图片名称](http://)</code></p><p><img src="https://s1.328888.xyz/2022/04/11/fXIeF.png" alt="图片1"></p></li><li><p>创建表格 <code>Ctrl+T</code></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| 姓名 | 性别 |</span><br><span class="line">| :--- | ---：|</span><br><span class="line">| 张三 | 男 |</span><br><span class="line">| 李四 | 女 |</span><br></pre></td></tr></table></figure><table><thead><tr><th>姓名</th><th>性别</th></tr></thead><tbody><tr><td>张三</td><td>男</td></tr><tr><td>李四</td><td>女</td></tr></tbody></table></li><li><p>删除线 <code>ALT+Shift+5</code></p><p><code>~~删除线~~</code></p><p><s>删除线</s></p></li><li><p>引用 <code>Ctrl+Shift+Q</code></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; 这是一个引用</span><br><span class="line">&gt;&gt;这是一个嵌套引用</span><br></pre></td></tr></table></figure><blockquote><p>这是一个引用</p><blockquote><p>这是一个引用嵌套</p></blockquote></blockquote></li><li><p>上标</p><p><code> X&lt;sup&gt;2&lt;sup&gt;</code></p><p>X<sup>2</sup></p></li><li><p>下标</p><p><code>H&lt;sub&gt;2&lt;/suB&gt;O</code></p><p>H<sub>2</sub>O</p></li><li><p>分割线 <code>*** 或者 ___</code></p><hr><hr></li><li><p>自动产生目录 <code>[TOC]+Enter</code></p><p><code>[TOC]</code></p></li><li><p>改变字体大小</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span>=<span class="string">1</span>&gt;</span>字体大小size=1<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span>=<span class="string">3</span>&gt;</span>字体大小size=3<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span>=<span class="string">5</span>&gt;</span>字体大小size=5<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font size=1>字体大小size=1</font></p><p><font size=3>字体大小size=3</font></p><p><font size=5>字体大小size=5</font></p></li><li><p>改变字体颜色</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">red</span>&gt;</span>红色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">&quot;blue&quot;</span>&gt;</span>蓝色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">Yellow</span>&gt;</span>黄色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">YellowGreen</span>&gt;</span>黄绿色<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font color=red>红色</font><br><font color="blue">蓝色</font><br><font color=Yellow>黄色</font><br><font color=YellowGreen>黄绿色</font></p></li><li><p>改变字体类型</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;黑体&quot;</span>&gt;</span>黑体<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;宋体&quot;</span>&gt;</span>宋体<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;仿宋&quot;</span>&gt;</span>仿宋<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;幼圆&quot;</span>&gt;</span>幼圆<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;楷书&quot;</span>&gt;</span>楷书<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文行楷&quot;</span>&gt;</span>华文行楷<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文隶书&quot;</span>&gt;</span>华文隶书<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文新魏&quot;</span>&gt;</span>华文新魏<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文彩云&quot;</span>&gt;</span>华文彩云<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;华文琥珀&quot;</span>&gt;</span>华文琥珀<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font face="黑体">黑体</font><br><font face="宋体">宋体</font><br><font face="仿宋">仿宋</font><br><font face="幼圆">幼圆</font><br><font face="楷书">楷书</font><br><font face="华文行楷">华文行楷</font><br><font face="华文隶书">华文隶书</font><br><font face="华文新魏">华文新魏</font><br><font face="华文彩云">华文彩云</font><br><font face="华文琥珀">华文琥珀</font></p></li><li><p>文本高亮</p><p><code>&lt;mark&gt;highlight 2&lt;/mark&gt;</code></p><p><mark>highlight 2</mark></p></li></ol><p>​</p><p>​</p>]]></content>
      
      
      <categories>
          
          <category> 杂文 </category>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> Typora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>序言</title>
      <link href="/2022/04/11/%E5%BA%8F%E8%A8%80/"/>
      <url>/2022/04/11/%E5%BA%8F%E8%A8%80/</url>
      
        <content type="html"><![CDATA[<h1>剑谱</h1><h2 id="序言">序言</h2><p><img src="https://s1.328888.xyz/2022/04/11/fXIeF.png" alt="fXIeF.png"></p><p><font size=4>   小生于壬寅年元月痛失所爱，数月以来郁郁寡欢，再不得昨日之愉，然偶见各路大神奉为顶上珍宝的格言：</p><blockquote><p>剑谱第一页 无爱既是神</p></blockquote><p>  日思夜想之下竟真悟出了几番道理，称其为道理确些许有失偏颇，然实有些许感悟，故在此立此blog，欲决心 <strong><s>发奋学习</s></strong> 练剑，将些许 <strong><s>学习笔记</s></strong> 心得写于此剑谱之中，望暮年之日回首往事，仍有迹可循。</p>]]></content>
      
      
      <categories>
          
          <category> 剑谱 </category>
          
          <category> 序言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 骚话 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
